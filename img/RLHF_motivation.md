
Congratulations on making it through the full generative AI project life cycle. Hopefully, you feel like you've developed some intuition about the important issues you have to consider when building applications using LLMs. This week, you saw how to align your models with human preferences, such as helpfulness, harmlessness, and honesty by fine-tuning using a technique called reinforcement learning with human feedback, or RLHF for short. Given the popularity of RLHF, there are many existing RL reward models and human alignment datasets available, enabling you to quickly start aligning your models. In practice, RLHF is a very effective mechanism that you can use to improve the alignment of your models, reduce the toxicity of their responses, and let you use your models more safely in production. You also saw important techniques to optimize your model for inference by reducing the size of the model through distillation, quantization, or pruning. This minimizes the amount of hardware resources needed to serve your LLMs in production. 