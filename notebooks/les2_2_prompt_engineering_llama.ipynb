{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VeerleDepestele/HOGENT_TrendsInAI/blob/work/prompt_engineering_principles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uyFmq7aDZSt"
   },
   "source": [
    "# Prompt engineering Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze notebook is gebaseerd op de \"short course\" \"prompt-engineering-with-llama-2\" van deeplearning.ai (\"https://learn.deeplearning.ai/\") \\\n",
    "\n",
    "Hiervoor werd een account aangemaakt voor \"https://www.together.ai/\" en een API key aangemaakt.\n",
    "\n",
    "Verschillende pogingen om een Llama-2 model te gebruiken via Huggingface leiden tot foutboodschappen, zoals hieronder aangetoond.\n",
    "(ref. Poging 1 en 2).\n",
    "\n",
    "Pogingen 1, 2, 3 en 4 om op een gratis manier een Llama model te gebruiken via Huggingface zijn ter illustratie.\n",
    "\n",
    "Tot nu toe probeer ik gratis oplossingen te vinden voor de lessen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "3L54FFwWExzT",
    "outputId": "943cefc3-ed00-4e63-d0db-93f54ac2a1db"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Laad de HF_TOKEN.\n",
    "from google.colab import userdata\n",
    "userdata.get('HF_TOKEN')[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "CeJCf81sFZgE",
    "outputId": "0b283e7d-d542-40ce-9ed5-75bc91bec90e"
   },
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": " (Request ID: D4BliBshjOFc0GnoYTEzZ)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/NousResearch/Hermes-2-Pro-Llama-3-8B/v1/chat/completions.\nIf you are trying to create or update content, make sure you have a token with the `write` role.\nThe model NousResearch/Hermes-2-Pro-Llama-3-8B is too large to be loaded automatically (16GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/NousResearch/Hermes-2-Pro-Llama-3-8B/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2106856b05e0>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m for message in client.chat_completion(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"What is the capital of France?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_url\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"tgi\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         data = self.post(\n\u001b[0m\u001b[1;32m    838\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m             json=dict(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m\"make sure you have a token with the `write` role.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m:  (Request ID: D4BliBshjOFc0GnoYTEzZ)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/NousResearch/Hermes-2-Pro-Llama-3-8B/v1/chat/completions.\nIf you are trying to create or update content, make sure you have a token with the `write` role.\nThe model NousResearch/Hermes-2-Pro-Llama-3-8B is too large to be loaded automatically (16GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "# Poging 1\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"NousResearch/Hermes-2-Pro-Llama-3-8B\",\n",
    "    # token=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "uUTEtCn3Etmr",
    "outputId": "77719fa0-56d9-4031-f23a-7d385176b7fe"
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: 5fRoWWjHuupmnnH3hOvFS)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b-chat-hf/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d599284bcb66>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m for message in client.chat_completion(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"What is the capital of France?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_url\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"tgi\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         data = self.post(\n\u001b[0m\u001b[1;32m    838\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m             json=dict(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"\\n\\nBad request for {endpoint_name} endpoint:\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mendpoint_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\\n\\nBad request:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             )\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadRequestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadRequestError\u001b[0m:  (Request ID: 5fRoWWjHuupmnnH3hOvFS)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
     ]
    }
   ],
   "source": [
    "# Poging 2\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    # oken=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "BAdWYgwDGRfr",
    "outputId": "e01f591d-061b-4390-a0d1-08e2ab5c67d3"
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: p2QP3GV9pfs4vGy83e7Rd)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.1-8B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-87dc47506344>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m for message in client.chat_completion(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"What is the capital of France?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_url\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"tgi\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         data = self.post(\n\u001b[0m\u001b[1;32m    838\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m             json=dict(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"\\n\\nBad request for {endpoint_name} endpoint:\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mendpoint_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\\n\\nBad request:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             )\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadRequestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadRequestError\u001b[0m:  (Request ID: p2QP3GV9pfs4vGy83e7Rd)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
     ]
    }
   ],
   "source": [
    "# Poging 3: gebruik een Llama 3.1 model.\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # token=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7tOLxJwGjE8",
    "outputId": "091cbbdf-419e-41a5-d2d8-4bab66bb3681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "# Poging 4: gebruik een Llama 3.1 model via NousResearch ipv via Meta.\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"NousResearch/Hermes-3-Llama-3.1-8B\",\n",
    "    # token=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y44JB2vOHo0c"
   },
   "source": [
    "#### Vanaf hier wordt gebruik gemaakt van together.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "DuqA-NcxDZ9z",
    "outputId": "0df68c2d-dc34-4324-f45d-f049076ed96f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'dc'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "userdata.get('TOGETHER_API_KEY')[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lz9yjJKvgboX"
   },
   "outputs": [],
   "source": [
    "# Aangezien we in de les met Google Colab werken, kunnen we de key laden via\n",
    "# \"userdata.get('TOGETHER_API_KEY')\"\n",
    "# Vandaar dat alles wat met het laden van de key uit de '.env' file in commentaar staat.\n",
    "\n",
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "#from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "# Initailize global variables\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "# warnings.filterwarnings('ignore')\n",
    "url = f\"{os.getenv('DLAI_TOGETHER_API_BASE', 'https://api.together.xyz')}/inference\"\n",
    "headers = {\n",
    "        \"Authorization\": f\"Bearer {userdata.get('TOGETHER_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "def llama(prompt,\n",
    "          add_inst=True,\n",
    "          model=\"togethercomputer/llama-2-7b-chat\",\n",
    "          temperature=0.0,\n",
    "          max_tokens=1024,\n",
    "          verbose=False,\n",
    "          url=url,\n",
    "          headers=headers,\n",
    "          base=2, # number of seconds to wait\n",
    "          max_tries=3):\n",
    "\n",
    "    if add_inst:\n",
    "        prompt = f\"[INST]{prompt}[/INST]\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Prompt:\\n{prompt}\\n\")\n",
    "        print(f\"model: {model}\")\n",
    "\n",
    "    data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "\n",
    "    # Allow multiple attempts to call the API incase of downtime.\n",
    "    # Return provided response to user after 3 failed attempts.\n",
    "    wait_seconds = [base**i for i in range(max_tries)]\n",
    "\n",
    "    for num_tries in range(max_tries):\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            return response.json()['output']['choices'][0]['text']\n",
    "        except Exception as e:\n",
    "            if response.status_code != 500:\n",
    "                return response.json()\n",
    "\n",
    "            print(f\"error message: {e}\")\n",
    "            print(f\"response object: {response}\")\n",
    "            print(f\"num_tries {num_tries}\")\n",
    "            print(f\"Waiting {wait_seconds[num_tries]} seconds before automatically trying again.\")\n",
    "            time.sleep(wait_seconds[num_tries])\n",
    "\n",
    "    print(f\"Tried {max_tries} times to make API call to get a valid response object\")\n",
    "    print(\"Returning provided response\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def llama_chat(prompts,\n",
    "               responses,\n",
    "               model=\"togethercomputer/llama-2-7b-chat\",\n",
    "               temperature=0.0,\n",
    "               max_tokens=1024,\n",
    "               verbose=False,\n",
    "               url=url,\n",
    "               headers=headers,\n",
    "               base=2,\n",
    "               max_tries=3\n",
    "              ):\n",
    "\n",
    "    prompt = get_prompt_chat(prompts,responses)\n",
    "\n",
    "    # Allow multiple attempts to call the API incase of downtime.\n",
    "    # Return provided response to user after 3 failed attempts.\n",
    "    wait_seconds = [base**i for i in range(max_tries)]\n",
    "\n",
    "    for num_tries in range(max_tries):\n",
    "        try:\n",
    "            response = llama(prompt=prompt,\n",
    "                             add_inst=False,\n",
    "                             model=model,\n",
    "                             temperature=temperature,\n",
    "                             max_tokens=max_tokens,\n",
    "                             verbose=verbose,\n",
    "                             url=url,\n",
    "                             headers=headers\n",
    "                            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            if response.status_code != 500:\n",
    "                return response.json()\n",
    "\n",
    "            print(f\"error message: {e}\")\n",
    "            print(f\"response object: {response}\")\n",
    "            print(f\"num_tries {num_tries}\")\n",
    "            print(f\"Waiting {wait_seconds[num_tries]} seconds before automatically trying again.\")\n",
    "            time.sleep(wait_seconds[num_tries])\n",
    "\n",
    "    print(f\"Tried {max_tries} times to make API call to get a valid response object\")\n",
    "    print(\"Returning provided response\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_prompt_chat(prompts, responses):\n",
    "  prompt_chat = f\"<s>[INST] {prompts[0]} [/INST]\"\n",
    "  for n, response in enumerate(responses):\n",
    "    prompt = prompts[n + 1]\n",
    "    prompt_chat += f\"\\n{response}\\n </s><s>[INST] \\n{ prompt }\\n [/INST]\"\n",
    "\n",
    "  return prompt_chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtTDhTs4hii-"
   },
   "source": [
    "## LLMs are stateless\n",
    "LLM's don't remember your previous interactions with them by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BM_K98INhpwH",
    "outputId": "0b23f599-178f-4364-d441-373922151985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  There are many fun activities you can do this weekend, depending on your interests and preferences. Here are some ideas:\n",
      "\n",
      "1. Outdoor Adventures: Go for a hike, have a picnic, or go camping in a nearby park or nature reserve.\n",
      "2. Cultural Events: Attend a concert, play, or festival in your area. Many cities have a vibrant cultural scene with plenty of events to choose from.\n",
      "3. Sports and Fitness: Try a new sport or activity, such as rock climbing, kayaking, or cycling. Many gyms and recreation centers offer classes and equipment rentals for these activities.\n",
      "4. Food and Drink: Take a food tour of your city, visit a local brewery or winery, or try a new restaurant or cuisine.\n",
      "5. DIY Projects: Get creative and work on a DIY project, such as painting, woodworking, or knitting.\n",
      "6. Game Night: Host a game night with friends and family, with board games, card games, or video games.\n",
      "7. Movie Night: Have a movie marathon with a theme, such as a favorite actor or director, or a specific genre.\n",
      "8. Volunteer: Look for volunteer opportunities in your community, such as helping at a local animal shelter or participating in a park clean-up.\n",
      "9. Spa Day: Treat yourself to a spa day, with a massage, facial, or other pampering treatments.\n",
      "10. Road Trip: Take a road trip to a nearby city or attraction, and explore new places and sights.\n",
      "11. Photography: Take your camera and go on a photography walk, capturing the beauty of your surroundings.\n",
      "12. Yoga or Meditation: Practice yoga or meditation, either at home or at a local studio, to relax and recharge.\n",
      "13. Book Club: Join a book club or start one with friends, and discuss your favorite books and authors.\n",
      "14. Wine Tasting: Visit a local vineyard or winery for a wine tasting, and learn about the process of wine-making.\n",
      "15. Karaoke: Sing your heart out at a local karaoke bar or event, and have fun with friends and family.\n",
      "\n",
      "Remember to always prioritize safety and comfort when planning activities, and make sure to have fun!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    What are fun activities I can do this weekend?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxJpVnY1htKU",
    "outputId": "47555c60-1dae-45be-a774-c04694e20345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  As a responsible AI language model, I must advise you that both options can be harmful to your health if consumed excessively or without proper precautions.\n",
      "\n",
      "Caffeine is a stimulant that can help increase alertness and energy, but it can also lead to negative side effects such as jitteriness, insomnia, and an increased heart rate if consumed in excess. Moderate caffeine consumption, defined as up to 400 milligrams per day (about the amount found in three cups of brewed coffee), is generally considered safe for most adults. However, it's important to be aware of your individual caffeine sensitivity and to limit your intake accordingly.\n",
      "\n",
      "Alcohol, on the other hand, can also have negative effects on your health when consumed in excess. Excessive alcohol consumption can lead to liver damage, heart problems, and an increased risk of certain cancers. It's important to drink alcohol in moderation, which is defined as up to one drink per day for women and up to two drinks per day for men.\n",
      "\n",
      "In summary, both caffeine and alcohol can be harmful to your health if consumed excessively or without proper precautions. It's important to be mindful of your individual tolerance and to consume these substances in moderation. If you have any concerns about your health or the impact of these substances on your well-being, I recommend consulting with a healthcare professional.\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Which of these would be good for my health?\n",
    "\"\"\"\n",
    "response_2 = llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6wCUnfKh84A"
   },
   "source": [
    "## Constructing multi-turn prompts\n",
    "You need to provide prior prompts and responses as part of the context of each new turn in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywYKj2zMh75G"
   },
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "    What are fun activities I can do this weekend?\n",
    "\"\"\"\n",
    "response_1 = llama(prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajDp5hG8iLs3"
   },
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Which of these would be good for my health?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHc3Q6E7iQM_",
    "outputId": "43361c29-8152-4887-b4c2-900990f52958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>[INST] \n",
      "    What are fun activities I can do this weekend?\n",
      " [/INST]\n",
      "  There are many fun activities you can do this weekend, depending on your interests and preferences. Here are some ideas:\n",
      "\n",
      "1. Outdoor Adventures: Go for a hike, have a picnic, or go camping in a nearby park or nature reserve.\n",
      "2. Cultural Events: Attend a concert, play, or festival in your area. Many cities have a vibrant cultural scene with plenty of events to choose from.\n",
      "3. Sports and Fitness: Try a new sport or activity, such as rock climbing, kayaking, or cycling. Many gyms and recreation centers offer classes and equipment rentals for these activities.\n",
      "4. Food and Drink: Take a food tour of your city, visit a local brewery or winery, or try a new restaurant or cuisine.\n",
      "5. DIY Projects: Get creative and work on a DIY project, such as painting, woodworking, or knitting.\n",
      "6. Game Night: Host a game night with friends and family, with board games, card games, or video games.\n",
      "7. Movie Night: Have a movie marathon with a theme, such as a favorite actor or director, or a specific genre.\n",
      "8. Volunteer: Look for volunteer opportunities in your community, such as helping at a local animal shelter or participating in a park clean-up.\n",
      "9. Spa Day: Treat yourself to a spa day, with a massage, facial, or other pampering treatments.\n",
      "10. Road Trip: Take a road trip to a nearby city or attraction, and explore new places and sights.\n",
      "11. Photography: Take your camera and go on a photography walk, capturing the beauty of your surroundings.\n",
      "12. Yoga or Meditation: Practice yoga or meditation, either at home or at a local studio, to relax and recharge.\n",
      "13. Book Club: Join a book club or start one with friends, and discuss your favorite books and authors.\n",
      "14. Wine Tasting: Visit a local vineyard or winery for a wine tasting, and learn about the process of wine-making.\n",
      "15. Karaoke: Sing your heart out at a local karaoke bar or event, and have fun with friends and family.\n",
      "\n",
      "Remember to always prioritize safety and comfort when planning activities, and make sure to have fun!\n",
      "</s>\n",
      "<s>[INST] \n",
      "Which of these would be good for my health?\n",
      " [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = f\"\"\"\n",
    "<s>[INST] {prompt_1} [/INST]\n",
    "{response_1}\n",
    "</s>\n",
    "<s>[INST] {prompt_2} [/INST]\n",
    "\"\"\"\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdA7_yFwiXy4",
    "outputId": "88a3eadb-780a-48d2-ebf0-6499b362c03b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "\n",
      "<s>[INST] \n",
      "    What are fun activities I can do this weekend?\n",
      " [/INST]\n",
      "  There are many fun activities you can do this weekend, depending on your interests and preferences. Here are some ideas:\n",
      "\n",
      "1. Outdoor Adventures: Go for a hike, have a picnic, or go camping in a nearby park or nature reserve.\n",
      "2. Cultural Events: Attend a concert, play, or festival in your area. Many cities have a vibrant cultural scene with plenty of events to choose from.\n",
      "3. Sports and Fitness: Try a new sport or activity, such as rock climbing, kayaking, or cycling. Many gyms and recreation centers offer classes and equipment rentals for these activities.\n",
      "4. Food and Drink: Take a food tour of your city, visit a local brewery or winery, or try a new restaurant or cuisine.\n",
      "5. DIY Projects: Get creative and work on a DIY project, such as painting, woodworking, or knitting.\n",
      "6. Game Night: Host a game night with friends and family, with board games, card games, or video games.\n",
      "7. Movie Night: Have a movie marathon with a theme, such as a favorite actor or director, or a specific genre.\n",
      "8. Volunteer: Look for volunteer opportunities in your community, such as helping at a local animal shelter or participating in a park clean-up.\n",
      "9. Spa Day: Treat yourself to a spa day, with a massage, facial, or other pampering treatments.\n",
      "10. Road Trip: Take a road trip to a nearby city or attraction, and explore new places and sights.\n",
      "11. Photography: Take your camera and go on a photography walk, capturing the beauty of your surroundings.\n",
      "12. Yoga or Meditation: Practice yoga or meditation, either at home or at a local studio, to relax and recharge.\n",
      "13. Book Club: Join a book club or start one with friends, and discuss your favorite books and authors.\n",
      "14. Wine Tasting: Visit a local vineyard or winery for a wine tasting, and learn about the process of wine-making.\n",
      "15. Karaoke: Sing your heart out at a local karaoke bar or event, and have fun with friends and family.\n",
      "\n",
      "Remember to always prioritize safety and comfort when planning activities, and make sure to have fun!\n",
      "</s>\n",
      "<s>[INST] \n",
      "Which of these would be good for my health?\n",
      " [/INST]\n",
      "\n",
      "\n",
      "model: togethercomputer/llama-2-7b-chat\n"
     ]
    }
   ],
   "source": [
    "response_2 = llama(chat_prompt,\n",
    "                 add_inst=False,\n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHsN2Ci3icpV",
    "outputId": "0aa27155-50d5-45a9-e217-a7909974b62a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  It's great that you're thinking about your health! All of the activities I mentioned can be beneficial for your health in different ways. Here are some specific benefits of each activity:\n",
      "\n",
      "1. Outdoor Adventures: Being in nature has been shown to have numerous health benefits, including reducing stress levels, improving mood, and boosting the immune system. Being active outdoors can also help improve cardiovascular health and overall fitness.\n",
      "2. Cultural Events: Attending cultural events can be a great way to reduce stress and improve mental health. It can also provide opportunities for socializing and connecting with others, which is important for overall well-being.\n",
      "3. Sports and Fitness: Engaging in sports and fitness activities can help improve cardiovascular health, increase strength and flexibility, and reduce the risk of chronic diseases like heart disease and diabetes.\n",
      "4. Food and Drink: Eating a variety of healthy foods and drinks can provide essential nutrients and help maintain a healthy weight. It can also be a fun and social activity, as many people enjoy trying new foods and drinks with friends and family.\n",
      "5. DIY Projects: Engaging in DIY projects can be a great way to reduce stress and improve mental health. It can also provide opportunities for creative expression and self-expression.\n",
      "6. Game Night: Playing games with friends and family can be a fun and social way to reduce stress and improve mental health. It can also provide opportunities for socializing and connecting with others.\n",
      "7. Movie Night: Watching movies can be a relaxing and enjoyable way to unwind and reduce stress. It can also provide opportunities for socializing and connecting with others.\n",
      "8. Volunteer: Volunteering can be a great way to improve mental health and overall well-being. It can also provide opportunities for socializing and connecting with others, as well as a sense of purpose and fulfillment.\n",
      "9. Spa Day: Treating yourself to a spa day can be a great way to reduce stress and improve mental health. It can also provide opportunities for relaxation and self-care.\n",
      "10. Yoga or Meditation: Practicing yoga or meditation can be a great way to reduce stress and improve mental health. It can also provide opportunities for physical exercise and relaxation.\n",
      "\n",
      "Ultimately, the best activity for your health will depend on your individual needs and preferences. It's important to find activities that you enjoy and that make you feel good, both physically and mentally.\n"
     ]
    }
   ],
   "source": [
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGXSHfXrlm8j"
   },
   "source": [
    "from: https://huggingface.co/meta-llama/Llama-2-7b#model-details\n",
    "\n",
    "### Intended Use\n",
    "Intended Use Cases Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n",
    "\n",
    "To get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the INST and <<SYS>> tags, BOS and EOS tokens, and the whitespaces and breaklines in between (we recommend calling strip() on inputs to avoid double-spaces). See our reference code in github for details: chat_completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFRoQ4vzot8t"
   },
   "source": [
    "### Use llama chat helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQo54UPvmjmO"
   },
   "source": [
    "https://github.com/meta-llama/llama/blob/main/llama/generation.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2IQX0Szle9k"
   },
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"\n",
    "    What are fun activities I can do this weekend?\n",
    "\"\"\"\n",
    "response_1 = llama(prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shEskQbFoyV9"
   },
   "outputs": [],
   "source": [
    "prompt_2 = \"\"\"\n",
    "Which of these would be good for my health?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJ6duSrYo1Ae"
   },
   "outputs": [],
   "source": [
    "prompts = [prompt_1,prompt_2]\n",
    "responses = [response_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVfc2_yZo3ov",
    "outputId": "25257bd5-80d2-4760-a72b-0774bb0abf5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<s>[INST] \n",
      "    What are fun activities I can do this weekend?\n",
      " [/INST]\n",
      "  There are many fun activities you can do this weekend, depending on your interests and preferences. Here are some ideas:\n",
      "\n",
      "1. Outdoor Adventures: Go for a hike, have a picnic, or go camping in a nearby park or nature reserve.\n",
      "2. Cultural Events: Attend a concert, play, or festival in your area. Many cities have a vibrant cultural scene with plenty of events to choose from.\n",
      "3. Sports and Fitness: Try a new sport or activity, such as rock climbing, kayaking, or cycling. Many gyms and recreation centers offer classes and equipment rentals for these activities.\n",
      "4. Food and Drink: Take a food tour of your city, visit a local brewery or winery, or try a new restaurant or cuisine.\n",
      "5. DIY Projects: Get creative and work on a DIY project, such as painting, woodworking, or knitting.\n",
      "6. Game Night: Host a game night with friends and family, with board games, card games, or video games.\n",
      "7. Movie Night: Have a movie marathon with a theme, such as a favorite actor or director, or a specific genre.\n",
      "8. Volunteer: Look for volunteer opportunities in your community, such as helping at a local animal shelter or participating in a park clean-up.\n",
      "9. Spa Day: Treat yourself to a spa day, with a massage, facial, or other pampering treatments.\n",
      "10. Road Trip: Take a road trip to a nearby city or attraction, and explore new places and sights.\n",
      "11. Photography: Take your camera and go on a photography walk, capturing the beauty of your surroundings.\n",
      "12. Yoga or Meditation: Practice yoga or meditation, either at home or at a local studio, to relax and recharge.\n",
      "13. Book Club: Join a book club or start one with friends, and discuss your favorite books and authors.\n",
      "14. Wine Tasting: Visit a local vineyard or winery for a wine tasting, and learn about the process of wine-making.\n",
      "15. Karaoke: Sing your heart out at a local karaoke bar or event, and have fun with friends and family.\n",
      "\n",
      "Remember to always prioritize safety and comfort when planning activities, and make sure to have fun!\n",
      " </s><s>[INST] \n",
      "\n",
      "Which of these would be good for my health?\n",
      "\n",
      " [/INST]\n",
      "\n",
      "model: togethercomputer/llama-2-7b-chat\n"
     ]
    }
   ],
   "source": [
    "# Pass prompts and responses to llama_chat function\n",
    "response_2 = llama_chat(prompts,responses,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJKlrc0Zo5Z5",
    "outputId": "5f9d3235-3348-441b-9004-1f630127fee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  It's great that you're thinking about your health! All of the activities I mentioned can be beneficial for your health in different ways. Here are some specific health benefits associated with each activity:\n",
      "\n",
      "1. Outdoor Adventures: Spending time in nature has been shown to have numerous health benefits, including reducing stress levels, improving mood, and boosting the immune system. Being physically active outdoors can also improve cardiovascular health and overall fitness.\n",
      "2. Cultural Events: Attending cultural events can be a great way to reduce stress and improve mental health. It can also provide opportunities for socializing and connecting with others, which is important for overall well-being.\n",
      "3. Sports and Fitness: Engaging in sports and fitness activities can improve cardiovascular health, increase strength and flexibility, and reduce the risk of chronic diseases like heart disease and diabetes.\n",
      "4. Food and Drink: Eating a variety of nutritious foods and drinks can provide essential vitamins, minerals, and other nutrients that support overall health. Limiting processed and sugary foods and drinks can also help reduce the risk of chronic diseases.\n",
      "5. DIY Projects: Engaging in DIY projects can provide opportunities for creative expression and problem-solving, which can be beneficial for mental health. It can also provide a sense of accomplishment and self-esteem.\n",
      "6. Game Night: Playing games with friends and family can provide opportunities for socializing and connecting with others, which is important for overall well-being. It can also be a fun and relaxing way to spend time together.\n",
      "7. Movie Night: Watching movies can be a great way to relax and unwind, and it can also provide opportunities for socializing and connecting with others.\n",
      "8. Volunteer: Volunteering can provide opportunities for socializing and connecting with others, as well as a sense of purpose and fulfillment. It can also improve mental health and overall well-being.\n",
      "9. Spa Day: Treating yourself to a spa day can provide opportunities for relaxation and stress relief, as well as a sense of pampering and self-care.\n",
      "10. Road Trip: Taking a road trip can provide opportunities for adventure and exploration, as well as a chance to spend time with friends and family. It can also be a great way to reduce stress and improve mental health.\n",
      "11. Photography: Engaging in photography can provide opportunities for creative expression and problem-solving, which can be beneficial for mental health. It can also provide a sense of accomplishment and self-esteem.\n",
      "12. Yoga or Meditation: Practicing yoga or meditation can provide numerous health benefits, including reducing stress levels, improving mood, and boosting overall well-being.\n",
      "13. Book Club: Joining a book club can provide opportunities for socializing and connecting with others, as well as a chance to discuss and learn about new books and authors.\n",
      "14. Wine Tasting: Participating in wine tasting can provide opportunities for socializing and connecting with others, as well as a chance to learn about different types of wine and the process of winemaking.\n",
      "15. Karaoke: Singing and performing with others can provide opportunities for socializing and connecting with others, as well as a chance to express oneself creatively.\n",
      "\n",
      "Ultimately, the best activity for your health will depend on your personal preferences and interests. It's important to find activities that you enjoy and that provide opportunities for socializing, creative expression, and relaxation.\n"
     ]
    }
   ],
   "source": [
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgoqVy5_o-3V"
   },
   "source": [
    "### Try it yourself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFZ0VTi-pCGO"
   },
   "outputs": [],
   "source": [
    "# replace prompt_3 with your own question!\n",
    "prompt_3 = \"Which of these activites would be fun with friends?\"\n",
    "prompts = [prompt_1, prompt_2, prompt_3]\n",
    "responses = [response_1, response_2]\n",
    "\n",
    "response_3 = llama_chat(prompts, responses, verbose=True)\n",
    "\n",
    "print(response_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmLBmHuupW8y"
   },
   "source": [
    "### In-context learning\n",
    "#### Standard prompt with instruction\n",
    "- So far, you have been stating the instruction explicitly in the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hd9UA2k6pkZ1",
    "outputId": "f972cadd-74ac-4b2c-bf06-04f2edc2e04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The sentiment of the message \"Hi Amit, thanks for the thoughtful birthday card!\" is positive. The use of the word \"thoughtful\" implies that the sender appreciated the effort put into the card, and the tone is friendly and sincere.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is the sentiment of:\n",
    "Hi Amit, thanks for the thoughtful birthday card!\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCoDGIVg73CJ"
   },
   "source": [
    "- We can also ask for an output in the following way.  \n",
    "The content of the message is: \"Hi Amit, ...\". Then we ask for the sentiment, just by stating: \"Sentiment: ?\". \\\n",
    "Here you provide the structure of the desired output to the model. \\\n",
    "This way of formulating the prompt is called \"zero shot prompting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NjPHnVppoHA",
    "outputId": "1f71c10c-4496-402d-fc62-fcdafb7cd0c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The sentiment of the message is \"Thank you\".\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gb49syEtp6y2"
   },
   "source": [
    "#### Few-shot Prompting\n",
    "Here is an example of few-shot prompting.\n",
    "In few-shot prompting, you not only provide the structure to the model, but also two or more examples.\n",
    "You are prompting the model to see if it can infer the task from the structure, as well as the examples in your prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEZx6UtJ9jwY"
   },
   "source": [
    "Voor de duidelijkheid, in het Nederlands: \\\n",
    "Hier is een voorbeeld van few-shot prompting. Bij few-shot prompting geef je niet alleen de structuur aan het model, maar ook twee of meer voorbeelden. Je stimuleert het model om te zien of het de taak kan afleiden uit zowel de structuur als de voorbeelden in je prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1TqgSjNp-9M",
    "outputId": "d6a50130-7e2b-48d2-8a1e-2a1b8b5afbe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, here are the sentiments for each message:\n",
      "\n",
      "1. Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "2. Message: Can't wait to order pizza for dinner tonight\n",
      "Sentiment: Positive\n",
      "3. Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SteWbsZQqP4o"
   },
   "source": [
    "### Specifying the Output Format\n",
    "You can also specify the format in which you want the model to respond.\n",
    "In the example below, you are asking to \"give a one word response\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7mfBGtUqQv2",
    "outputId": "8d5134fe-4233-4797-eb91-fc1cf979daf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are the one-word responses for each message:\n",
      "\n",
      "1. Negative: Disappointed\n",
      "2. Positive: Excited\n",
      "3. ? (Uncertain): Grateful\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xa3ifXB-qWK2"
   },
   "source": [
    "Note: For all the examples above, you used the 7 billion parameter model, llama-2-7b-chat. And as you saw in the last example, the 7B model was uncertain about the sentiment.\n",
    "\n",
    "You can use the larger (70 billion parameter) llama-2-70b-chat model to see if you get a better, certain response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mF9f8qHdqcHO",
    "outputId": "8060835c-30b4-4ed7-93c5-582e8d9f7a7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Unable to access non-serverless model togethercomputer/llama-2-70b-chat. Please visit https://api.together.ai/models/togethercomputer/llama-2-70b-chat to create and start a new dedicated endpoint for the model.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_available'}}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: ?\n",
    "\n",
    "Give a one word response.\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                model=\"togethercomputer/llama-2-70b-chat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_Af2qkdqlWW"
   },
   "source": [
    "### Add constraints / be specific about the desired output\n",
    "Now, use the smaller model again, but adjust your prompt in order to help the model to understand what is being expected from it.\n",
    "Restrict the model's output format to choose from positive, negative or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mI3zYcj1qvS3",
    "outputId": "174a4bda-4156-41c1-b7b3-98f4e11665ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, I'd be happy to help! Here are my responses:\n",
      "\n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "\n",
      "Message: Can't wait to order pizza for dinner tonight\n",
      "Sentiment: Positive\n",
      "\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment:\n",
    "\n",
    "Respond with either positive, negative, or neutral.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2Wxm4OlqI2v"
   },
   "source": [
    "### Role Prompting\n",
    "Roles give context to LLMs what type of answers are desired.\n",
    "Llama 2 often gives more consistent responses when provided with a role. \\\n",
    "First, try standard prompt and see the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQedqJtcq2BO",
    "outputId": "6bb9c779-4851-4a76-cdee-a5edfb4f8120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The question of the meaning of life is a complex and philosophical one that has been debated throughout human history. There are many different perspectives and interpretations on what the meaning of life is, and there is no one definitive answer. However, here are some possible ways to approach this question:\n",
      "\n",
      "1. Religious or spiritual perspective: Many people believe that the meaning of life is to fulfill a divine or spiritual purpose. According to this view, life has a higher purpose that is connected to a deity or a higher power. The meaning of life is to fulfill this purpose, which may involve following certain moral principles or practices, such as prayer, meditation, or service to others.\n",
      "2. Personal growth and fulfillment: From this perspective, the meaning of life is to grow and develop as an individual, and to find fulfillment and happiness in one's experiences. This may involve pursuing one's passions and interests, building meaningful relationships, and achieving personal goals.\n",
      "3. Social or cultural perspective: From this view, the meaning of life is tied to one's social or cultural context. For example, some cultures place a strong emphasis on family and community, while others prioritize individualism and personal freedom. The meaning of life may be found in contributing to the greater good of one's society or community.\n",
      "4. Existentialist perspective: Existentialists believe that life has no inherent meaning, and that it is up to each individual to create their own purpose and meaning. According to this view, life is a blank slate, and it is up to each person to fill it with meaning through their choices and actions.\n",
      "5. Hedonistic perspective: From this perspective, the meaning of life is to seek pleasure and avoid pain. According to this view, life is about maximizing one's enjoyment and minimizing one's suffering, and finding ways to do so in a way that is ethical and responsible.\n",
      "6. Epicurean perspective: Epicureans believe that the meaning of life is to seek happiness and tranquility through the cultivation of friendships, the pursuit of knowledge, and the avoidance of pain and fear. According to this view, life is about living in the present moment and finding joy in the simple things.\n",
      "7. Stoic perspective: Stoics believe that the meaning of life is to live in accordance with reason and virtue, and to cultivate inner strength and resilience. According to this view, life is about accepting the things that are outside of one's control, and focusing on what one can control, such as one's thoughts and actions.\n",
      "8. Humanistic perspective: From this perspective, the meaning of life is to fulfill one's potential as a human being, and to make the most of one's time on earth. According to this view, life is about developing one's talents and abilities, and using them to make a positive impact on the world.\n",
      "\n",
      "These are just a few of the many possible perspectives on the meaning of life. Ultimately, the answer to this question may be different for each person, and may depend on their individual experiences, beliefs, and values.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMfyMbdkq4Fr"
   },
   "source": [
    "Now, try it by giving the model a \"role\", and within the role, a \"tone\" using which it should respond with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aWwTLNMq5r2",
    "outputId": "3704c64b-3434-4faa-989e-9355ce40acfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shiver me timbers! Yer lookin' fer the meaning o' life, eh? Well, matey, that be a question that's been puzzlin' the greatest minds on the high seas fer centuries! *adjusts eye patch*\n",
      "\n",
      "Now, I ain't one to give ye a straight answer, but I'll share me thoughts with ye. The meaning o' life, me hearty, be different fer each and every one o' us. It be the sum o' all yer experiences, the memories ye make, the adventures ye have, and the treasure ye find along the way! *winks*\n",
      "\n",
      "Ye see, life be a great big ocean, and ye be a ship sailin' through it. Ye gotta chart yer own course, follow yer heart, and navigate through the storms and calm seas. The meaning o' life be findin' yer own treasure, me matey! *adjusts hat*\n",
      "\n",
      "So, don't be lookin' fer a definitive answer, or a treasure map that'll lead ye straight to the meaning o' life. It be a journey, a adventure, a treasure hunt, if ye will! *winks*\n",
      "\n",
      "Now, go forth and find yer own treasure, me hearty! And remember, the meaning o' life be whatever ye make it! *adjusts eye patch*\n"
     ]
    }
   ],
   "source": [
    "role = \"\"\"\n",
    "Your role is a life coach \\\n",
    "who gives advice to people about living a good life.\\\n",
    "You attempt to provide unbiased advice.\n",
    "You respond in the tone of an English pirate.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "{role}\n",
    "How can I answer this question from my friend:\n",
    "What is the meaning of life?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln3G02scq86t"
   },
   "source": [
    "####Summarization\n",
    "Summarizing a large text is another common use case for LLMs. Let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwli7B-vq_K1"
   },
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear Amit,\n",
    "\n",
    "An increasing variety of large language models (LLMs) are open source, or close to it. The proliferation of models with relatively permissive licenses gives developers more options for building applications.\n",
    "\n",
    "Here are some different ways to build applications based on LLMs, in increasing order of cost/complexity:\n",
    "\n",
    "Prompting. Giving a pretrained LLM instructions lets you build a prototype in minutes or hours without a training set. Earlier this year, I saw a lot of people start experimenting with prompting, and that momentum continues unabated. Several of our short courses teach best practices for this approach.\n",
    "One-shot or few-shot prompting. In addition to a prompt, giving the LLM a handful of examples of how to carry out a task — the input and the desired output — sometimes yields better results.\n",
    "Fine-tuning. An LLM that has been pretrained on a lot of text can be fine-tuned to your task by training it further on a small dataset of your own. The tools for fine-tuning are maturing, making it accessible to more developers.\n",
    "Pretraining. Pretraining your own LLM from scratch takes a lot of resources, so very few teams do it. In addition to general-purpose models pretrained on diverse topics, this approach has led to specialized models like BloombergGPT, which knows about finance, and Med-PaLM 2, which is focused on medicine.\n",
    "For most teams, I recommend starting with prompting, since that allows you to get an application working quickly. If you’re unsatisfied with the quality of the output, ease into the more complex techniques gradually. Start one-shot or few-shot prompting with a handful of examples. If that doesn’t work well enough, perhaps use RAG (retrieval augmented generation) to further improve prompts with key information the LLM needs to generate high-quality outputs. If that still doesn’t deliver the performance you want, then try fine-tuning — but this represents a significantly greater level of complexity and may require hundreds or thousands more examples. To gain an in-depth understanding of these options, I highly recommend the course Generative AI with Large Language Models, created by AWS and DeepLearning.AI.\n",
    "\n",
    "(Fun fact: A member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like me. I wonder if my job is at risk? 😜)\n",
    "\n",
    "Additional complexity arises if you want to move to fine-tuning after prompting a proprietary model, such as GPT-4, that’s not available for fine-tuning. Is fine-tuning a much smaller model likely to yield superior results than prompting a larger, more capable model? The answer often depends on your application. If your goal is to change the style of an LLM’s output, then fine-tuning a smaller model can work well. However, if your application has been prompting GPT-4 to perform complex reasoning — in which GPT-4 surpasses current open models — it can be difficult to fine-tune a smaller model to deliver superior results.\n",
    "\n",
    "Beyond choosing a development approach, it’s also necessary to choose a specific model. Smaller models require less processing power and work well for many applications, but larger models tend to have more knowledge about the world and better reasoning ability. I’ll talk about how to make this choice in a future letter.\n",
    "\n",
    "Keep learning!\n",
    "\n",
    "Andrew\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cry6DrxRrEiX",
    "outputId": "92034cd0-fc41-4b86-d95c-926b395ffb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The author of the email discusses the use of large language models (LLMs) and provides an overview of different ways to build applications based on these models. The key points extracted from the email are:\n",
      "\n",
      "1. Increasing number of LLMs are open source or close to it, giving developers more options for building applications.\n",
      "2. Different ways to build applications based on LLMs, in increasing order of cost/complexity: prompting, one-shot or few-shot prompting, fine-tuning, and pretraining.\n",
      "3. Prompting allows developers to build a prototype in minutes or hours without a training set.\n",
      "4. One-shot or few-shot prompting gives the LLM a handful of examples of how to carry out a task, which sometimes yields better results.\n",
      "5. Fine-tuning an LLM that has been pretrained on a lot of text can be done by training it further on a small dataset of your own.\n",
      "6. Pretraining a model from scratch takes a lot of resources, so it is not recommended for most teams.\n",
      "7. Specialized models like BloombergGPT and Med-PaLM 2 have been developed for specific domains.\n",
      "8. The choice of development approach and model depends on the application, with smaller models requiring less processing power and larger models having more knowledge about the world and better reasoning ability.\n",
      "9. The author recommends starting with prompting and gradually moving to more complex techniques if needed.\n",
      "10. The course \"Generative AI with Large Language Models\" created by AWS and DeepLearning.AI provides an in-depth understanding of these options.\n",
      "\n",
      "Regarding llama models, the author mentions that a member of the DeepLearning.AI team has been trying to fine-tune Llama-2-7B to sound like the author, but there is no mention of any specific insights or findings related to llama models.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize this email and extract some key points.\n",
    "What did the author say about llama models?:\n",
    "\n",
    "email: {email}\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqeupg8orFUW"
   },
   "source": [
    "Providing New Information in the Prompt\n",
    "A model's knowledge of the world ends at the moment of its training - so it won't know about more recent events.\n",
    "Llama 2 was released for research and commercial use on July 18, 2023, and its training ended some time before that date.\n",
    "Ask the model about an event, in this case, FIFA Women's World Cup 2023, which started on July 20, 2023, and see how the model responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUeU-P75rBE3"
   },
   "source": [
    "### The model only \"knows\" the content it has been trained on.\n",
    "A model only has information up to the point in time it was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GgXI8ZQHrH8Y",
    "outputId": "11d1eb7b-feab-4d54-b64e-b6d589ad187c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The 2023 Women's World Cup has not yet taken place, as it is scheduled to be held in 2023. The tournament is organized by FIFA (Fédération Internationale de Football Association) and is held every four years. The winner of the 2023 Women's World Cup will be determined by the team that wins the final match, which is expected to take place in July 2023.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Who won the 2023 Women's World Cup?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urbBP-fjrJaA"
   },
   "source": [
    "As you can see, the model still thinks that the tournament is yet to be played, even though you are now in 2024!\n",
    "Another thing to note is, July 18, 2023 was the date the model was released to public, and it was trained even before that, so it only has information upto that point. The response says, \"the final match is scheduled to take place in July 2023\", but the final match was played on August 20, 2023.\n",
    "You can provide the model with information about recent events, in this case text from Wikipedia about the 2023 Women's World Cup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFhpXkcDrMNH"
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "The 2023 FIFA Women's World Cup (Māori: Ipu Wahine o te Ao FIFA i 2023)[1] was the ninth edition of the FIFA Women's World Cup, the quadrennial international women's football championship contested by women's national teams and organised by FIFA. The tournament, which took place from 20 July to 20 August 2023, was jointly hosted by Australia and New Zealand.[2][3][4] It was the first FIFA Women's World Cup with more than one host nation, as well as the first World Cup to be held across multiple confederations, as Australia is in the Asian confederation, while New Zealand is in the Oceanian confederation. It was also the first Women's World Cup to be held in the Southern Hemisphere.[5]\n",
    "This tournament was the first to feature an expanded format of 32 teams from the previous 24, replicating the format used for the men's World Cup from 1998 to 2022.[2] The opening match was won by co-host New Zealand, beating Norway at Eden Park in Auckland on 20 July 2023 and achieving their first Women's World Cup victory.[6]\n",
    "Spain were crowned champions after defeating reigning European champions England 1–0 in the final. It was the first time a European nation had won the Women's World Cup since 2007 and Spain's first title, although their victory was marred by the Rubiales affair.[7][8][9] Spain became the second nation to win both the women's and men's World Cup since Germany in the 2003 edition.[10] In addition, they became the first nation to concurrently hold the FIFA women's U-17, U-20, and senior World Cups.[11] Sweden would claim their fourth bronze medal at the Women's World Cup while co-host Australia achieved their best placing yet, finishing fourth.[12] Japanese player Hinata Miyazawa won the Golden Boot scoring five goals throughout the tournament. Spanish player Aitana Bonmatí was voted the tournament's best player, winning the Golden Ball, whilst Bonmatí's teammate Salma Paralluelo was awarded the Young Player Award. England goalkeeper Mary Earps won the Golden Glove, awarded to the best-performing goalkeeper of the tournament.\n",
    "Of the eight teams making their first appearance, Morocco were the only one to advance to the round of 16 (where they lost to France; coincidentally, the result of this fixture was similar to the men's World Cup in Qatar, where France defeated Morocco in the semi-final). The United States were the two-time defending champions,[13] but were eliminated in the round of 16 by Sweden, the first time the team had not made the semi-finals at the tournament, and the first time the defending champions failed to progress to the quarter-finals.[14]\n",
    "Australia's team, nicknamed the Matildas, performed better than expected, and the event saw many Australians unite to support them.[15][16][17] The Matildas, who beat France to make the semi-finals for the first time, saw record numbers of fans watching their games, their 3–1 loss to England becoming the most watched television broadcast in Australian history, with an average viewership of 7.13 million and a peak viewership of 11.15 million viewers.[18]\n",
    "It was the most attended edition of the competition ever held.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySR4fOCBrOTg",
    "outputId": "b2b4868f-334f-4c7d-9792-3d3b9da09944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the information provided in the context, Spain won the 2023 Women's World Cup.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Given the following context, who won the 2023 Women's World cup?\n",
    "context: {context}\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMg_ZQeqrQYB"
   },
   "source": [
    "Try it Yourself!\n",
    "Try asking questions of your own! Modify the code below and include your own context to see how the model responds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGwbs9L5rR3A"
   },
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "<paste context in here>\n",
    "\"\"\"\n",
    "query = \"<your query here>\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Given the following context,\n",
    "{query}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "response = llama(prompt,\n",
    "                 verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBXWCnf-rTXW"
   },
   "source": [
    "### Chain-of-thought Prompting\n",
    "LLMs can perform better at reasoning and logic problems if you ask them to break the problem down into smaller steps. This is known as chain-of-thought prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-o78pinrVuP",
    "outputId": "15c27bba-1941-48c3-9af9-a51659435d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Yes, all 15 people can get to the restaurant by car or motorcycle.\n",
      "\n",
      "Here's how:\n",
      "\n",
      "* Two people with cars can fit 5 people each, so they can take 10 people in total.\n",
      "* Two people with motorcycles can fit 2 people each, so they can take 4 people in total.\n",
      "\n",
      "That means there are 10 people who can go by car and 4 people who can go by motorcycle, for a total of 14 people who can get to the restaurant.\n",
      "\n",
      "The remaining 1 person can either walk or find another mode of transportation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JutrW7VHrXEF"
   },
   "source": [
    "Modify the prompt to ask the model to \"think step by step\" about the math problem you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iI-WyObqrY9C",
    "outputId": "880dd2f8-dd01-49a9-d093-bbbb9fddcb45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure, let's break it down step by step to see if we can get everyone to the restaurant:\n",
      "\n",
      "1. Number of people who want to go to the restaurant: 15\n",
      "2. Number of people who have cars: 2\n",
      "3. Number of people who can fit in each car: 5\n",
      "4. Number of motorcycles available: 2\n",
      "5. Number of people who can fit in each motorcycle: 2\n",
      "\n",
      "Now, let's see if we can accommodate everyone:\n",
      "\n",
      "1. Car 1: 2 people who want to go to the restaurant (driver and passenger)\n",
      "2. Car 2: 5 people who want to go to the restaurant (driver and 4 passengers)\n",
      "3. Motorcycle 1: 2 people who want to go to the restaurant (driver and passenger)\n",
      "4. Motorcycle 2: 2 people who want to go to the restaurant (driver and passenger)\n",
      "\n",
      "Total number of people who can get to the restaurant: 12 (6 in Car 1, 5 in Car 2, and 2 in Motorcycle 1 and Motorcycle 2)\n",
      "\n",
      "Unfortunately, we have 3 more people who want to go to the restaurant than we have available transportation. So, no, we cannot all get to the restaurant by car or motorcycle.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6oGGrWrratu"
   },
   "source": [
    "Provide the model with additional instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSKj5L8nrcBk",
    "outputId": "a7cec5d5-50c8-476d-cd68-980c31323f0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Let's break down the problem step by step to determine if all 15 people can get to the restaurant by car or motorcycle:\n",
      "\n",
      "Step 1: Identify the number of people who can be accommodated by each car:\n",
      "\n",
      "* Two cars are available, and each car can seat 5 people.\n",
      "* Therefore, a total of 10 people can be accommodated by the two cars.\n",
      "\n",
      "Step 2: Identify the number of people who can be accommodated by each motorcycle:\n",
      "\n",
      "* Two motorcycles are available, and each motorcycle can fit 2 people.\n",
      "* Therefore, a total of 4 people can be accommodated by the two motorcycles.\n",
      "\n",
      "Step 3: Determine the total number of people who can be accommodated for the journey:\n",
      "\n",
      "* Add the number of people who can be accommodated by the cars (10) and the number of people who can be accommodated by the motorcycles (4):\n",
      "\n",
      "10 + 4 = 14\n",
      "\n",
      "Step 4: Check if the total number of people who can be accommodated is equal to the number of people who want to go to the restaurant:\n",
      "\n",
      "* 14 is less than 15 (the number of people who want to go to the restaurant)\n",
      "\n",
      "Therefore, it is not possible to accommodate all 15 people by car or motorcycle.\n",
      "\n",
      "Answer: No, it is not possible to get to the restaurant by car or motorcycle.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "\n",
    "Think step by step.\n",
    "Explain each intermediate step.\n",
    "Only when you are done with all your steps,\n",
    "provide the answer based on your intermediate steps.\n",
    "\"\"\"\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db01OVY0rdbW"
   },
   "source": [
    "The order of instructions matters!\n",
    "Ask the model to \"answer first\" and \"explain later\" to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Q4-qH93rfC2",
    "outputId": "348d980b-d947-4744-c305-76f5b4a2cda3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Yes, all 15 people can get to the restaurant by car or motorcycle. Here's how:\n",
      "\n",
      "Step 1:\n",
      "Two people with cars can fit 10 people (5 people per car x 2 cars).\n",
      "\n",
      "Step 2:\n",
      "Two people with motorcycles can fit 4 people (2 people per motorcycle x 2 motorcycles).\n",
      "\n",
      "Step 3:\n",
      "Combine the people who can be transported by car and motorcycle: 10 + 4 = 14 people.\n",
      "\n",
      "Therefore, yes, all 15 people can get to the restaurant by car or motorcycle.\n",
      "\n",
      "Here are the intermediate steps:\n",
      "\n",
      "1. Two people with cars can fit 10 people.\n",
      "2. Two people with motorcycles can fit 4 people.\n",
      "3. Combine the people who can be transported by car and motorcycle: 10 + 4 = 14 people.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQRpTd9orguH"
   },
   "source": [
    "Since LLMs predict their answer one token at a time, the best practice is to ask them to think step by step, and then only provide the answer after they have explained their reasoning."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPmhroypqrvlS2QUx6i0284",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
