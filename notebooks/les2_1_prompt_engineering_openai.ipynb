{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4c8c31-af4b-452f-960f-6f7bb2d0a4be",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f24502f-c719-42f0-9343-b633f278776c",
   "metadata": {},
   "source": [
    "## wat\n",
    "\n",
    "Prompt engineering is het sleutelen aan de vraag (de prompt) die je aan LLM's stelt zodat je een beter antwoord krijgt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f5f09",
   "metadata": {},
   "source": [
    "Deze notebook toont een aantal voorbeelden m.b.t. prompt engineering.\n",
    "Het gaat hem erom enkele prompt engineering technieken te demonstreren a.d.h.v. deze voorbeelden.\n",
    "\n",
    "De code dateert van 2024 en dient hoogst waarschijnlijk aangepast te worden om te functioneren. \n",
    "Voor het OpenAI gedeelte is er ook een (betalende) API-key nodig.\n",
    "\n",
    "Om de OpenAI API key te gebruiken, kan je starten met deze documentatie: \\\n",
    "https://platform.openai.com/docs/guides/text?api-mode=responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639c9fb-7ae1-4865-a574-7ce7903a878e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-sv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nchat_completion = client.chat.completions.create(\\n    messages=[\\n        {\\n            \"role\": \"user\",\\n            \"content\": \"Say this is a test\",\\n        }\\n    ],\\n    model=\"gpt-3.5-turbo\",\\n)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Met onderstaande code creëer je een openAI client en een chat completion.\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Indien je met colab werkt, gebruik dan:\n",
    "# from google.colab import userdata\n",
    "# client = OpenAI(\n",
    "#     api_key=userdata.get('OPENAI_API_KEY'),\n",
    "# )\n",
    "\n",
    "# Werk je met een .env file, gebruik dan:\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "_ = load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")  \n",
    "\n",
    "assert isinstance(api_key, str) and api_key.startswith(\"sk-\"), \"API key not a string\"\n",
    "\n",
    "client = OpenAI(api_key=api_key.strip())\n",
    "\n",
    "\n",
    "# Je kan hiermee testen of het werkt:\n",
    "\"\"\"\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35471c9-12cc-461f-b1b2-a63b681ad564",
   "metadata": {},
   "source": [
    "### OpenAI API\n",
    "\n",
    "Om de API te gebruiken dien je de [chat completions API](https://platform.openai.com/docs/guides/gpt/chat-completions-api) aan te spreken. Je geeft minimaal een (lijst van) boodschappen, het model dat je wenst te gebruiken en de api key als input, en krijgt het door het model gegenereerde antwoord terug.\n",
    "\n",
    "Hoewel *Chat*GPT eigenlijk ontworpen is om conversaties te houden, kan het toch nuttig zijn als je het in een \"single turn\" mode gebruikt ook.\n",
    "\n",
    "(In deze notebook gebruiken we altijd `role: user` gebruiken, verder gaan we dieper in op welke rollen er zijn en hoe die kunnen gebruikt worden)\n",
    "\n",
    "Er zijn verschillende openai modellen (ref. [openai models](https://platform.openai.com/docs/models)). Hier maken we gebruik van het model \"gpt-3.5-turbo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a3490-c007-4afd-ab32-54a92ec13280",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T08:52:18.597276Z",
     "start_time": "2023-10-03T08:52:07.581309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-ACpOHMJ6iUT8o7Ic7wqifCX9PhrNO\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1727620845,\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Prompt engineering is een concept dat voortkomt uit de softwareontwikkeling en is gericht op het cre\\u00ebren van een effici\\u00ebnte en gestructureerde ontwikkelomgeving. Het is een strategie waarbij ontwikkelaars worden aangemoedigd om snel te reageren op feedback en verzoeken van gebruikers. Prompt engineering omvat het analyseren, ontwerpen, implementeren en testen van software met een snelle doorlooptijd.\\n\\nEen belangrijk aspect van prompt engineering is het gebruik van agile ontwikkelmethoden, zoals Scrum en Kanban, waarbij teams in korte iteraties werken om regelmatig updates en verbeteringen uit te brengen. Door deze aanpak kunnen ontwikkelaars snel inspelen op veranderingen en feedback van gebruikers, waardoor de software sneller en beter kan worden aangepast aan de behoeften van de gebruikers.\\n\\nPrompt engineering omvat ook het gebruik van tools en technieken die de ontwikkeling van software versnellen, zoals geautomatiseerde tests, continue integratie en deployment, en cloud computing. Door deze technologie\\u00ebn te gebruiken, kunnen ontwikkelaars effici\\u00ebnter werken en sneller hoogwaardige software leveren.\\n\\nKortom, prompt engineering is een benadering van softwareontwikkeling die zich richt op snelheid, flexibiliteit en kwaliteit. Het helpt ontwikkelaars om snel te reageren op veranderingen en gebruikersfeedback, waardoor ze beter in staat zijn om software te leveren die aan de behoeften van de gebruikers voldoet.\",\n",
      "        \"refusal\": null\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 17,\n",
      "    \"completion_tokens\": 370,\n",
      "    \"total_tokens\": 387,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"system_fingerprint\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", \n",
    "                                        messages=[{\"role\": \"user\", \n",
    "                                                   \"content\": \"Ik zou graag meer leren over prompt engineering\"}])\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee06688-fd4b-4ac1-a20f-ab0b5f91a97f",
   "metadata": {},
   "source": [
    "Je krijgt steeds een JSON object als antwoord, naast een timestamp en de precieze versie van het model dat gebruikt werd interesseert ons hier vooral het object `choices[0].message[\"content\"]` dat het antwoord bevat.\n",
    "\n",
    "Daarnaast zal ook altijd het aantal tokens voor vraag/antwoord terug gestuurd worden, waarmee je ook de kost kan berekenen. Deze specifieke prompt kostte (in 2023) ~ €0,0012835.\n",
    "\n",
    "In de cursus van vorig jaar werd hier verwezen naar:\n",
    "ref. https://platform.openai.com/docs/guides/chat-completions, secties \"Overview\", \"Getting Started\" en \"Response format\".\n",
    "\n",
    "De link werkt nog steeds, maar de secties zijn veranderd, alsook het advies over hoe te prompten + de vorm van de output van de modellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e549b8-b45e-4c2f-9d44-3a4fa1dec11d",
   "metadata": {},
   "source": [
    "We schrijven een helper functie \"get_answer\". Hiermee wordt het mogelijk om snel te wisselen tussen het gebruik van een openai model of een Huggingface model. In de helper functie staat de parameter \"max_tokens\" standaard op 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2f3533-d6ae-4ca2-8cd2-c80c0059eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model options are for example: \"gpt-3.5-turbo\", \"gpt-4.o-mini\"\n",
    "def get_answer(prompt, model=\"gpt-3.5-turbo\", role=\"user\", max_tokens=100):\n",
    "    message = [{\"role\": role, \"content\": prompt}]\n",
    "    res = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=message,\n",
    "    max_tokens=max_tokens\n",
    "        )\n",
    "    return res.choices[0].message.content\n",
    "\n",
    "# model options are for example: \n",
    "#   \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "#   \"NousResearch/Hermes-3-Llama-3.1-8B\"\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "_ = load_dotenv()\n",
    "hf_api_key = os.getenv('HF_API_KEY')\n",
    "\n",
    "assert isinstance(hf_api_key, str) and len(hf_api_key) > 10, \"HF API key not a string\"\n",
    "\n",
    "def get_answer_hf(prompt, model=\"meta-llama/Meta-Llama-3-8B-Instruct\", role=\"user\", max_tokens=100, hf_api_key=hf_api_key):\n",
    "    client = InferenceClient(model, api_key=hf_api_key)\n",
    "    message = [{\"role\": role, \"content\": prompt}]\n",
    "    res = client.chat_completion(messages=message, max_tokens=max_tokens)\n",
    "    return res.choices[0].message.content, res.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecef5d8-ad3f-438c-a8a6-0da69897448b",
   "metadata": {},
   "source": [
    "#### Oefening\n",
    "Beschouw onderstaande alternatieve helper functie. I.p.v. enkel het textueel resultaat terug te geven, geven we een ruimer overzicht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77673631-7b99-48af-ac25-e82226e06344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model options are for example: \"gpt-3.5-turbo\", \"gpt-4.o-mini\"\n",
    "def get_answer_alternative(prompt, model=\"gpt-3.5-turbo\", role=\"user\", max_tokens=3):\n",
    "    message = [{\"role\": role, \"content\": prompt}]\n",
    "    res = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=message,\n",
    "    max_tokens=max_tokens\n",
    "        )\n",
    "    return res.choices[0].message.content, res.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50432811-43f0-43b2-8871-fba2d20e670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_completion: The capital of\n",
      "text_completion: {\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The capital of\",\n",
      "    \"refusal\": null\n",
      "  },\n",
      "  \"logprobs\": null,\n",
      "  \"finish_reason\": \"length\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test get_answer_alternative(\"What is the capital of France?\")\n",
    "text_completion, entire_completion = get_answer_alternative(\"What is the capital of France?\")\n",
    "print(f\"text_completion: {text_completion}\")\n",
    "print(f\"text_completion: {entire_completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f93848-ce05-4536-bc00-65089a489c51",
   "metadata": {},
   "source": [
    "- Welke finish_reason krijg je wanneer je de optie max_tokens = 10, en wanneer je de optie max_tokens = 3 gebruikt? \n",
    "- Wat betekent dit?\n",
    "- Wat zijn andere finish reasons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82996832-62c0-44de-b30c-90df29313855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Test your openAI access\n",
    "print(get_answer(\"What is the capital of France?\"))\n",
    "print(get_answer(\"What is the capital of France?\", model=\"gpt-4o-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af5976c1-e34c-41c6-9b47-18f6918eaf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The capital of France is Paris.', ChatCompletionOutputMessage(role='assistant', content='The capital of France is Paris.', tool_call_id=None, tool_calls=None))\n",
      "(\"The capital of France is Paris. Paris has been the capital of France since the 12th century and is the country's major cultural, commercial, and industrial center. It is also a global hub for fashion, art, and gastronomy. The city is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral. Paris is situated on the River Seine and is the most populous city in France, with an estimated population of over\", ChatCompletionOutputMessage(role='assistant', content=\"The capital of France is Paris. Paris has been the capital of France since the 12th century and is the country's major cultural, commercial, and industrial center. It is also a global hub for fashion, art, and gastronomy. The city is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral. Paris is situated on the River Seine and is the most populous city in France, with an estimated population of over\", tool_call_id=None, tool_calls=None))\n"
     ]
    }
   ],
   "source": [
    "# Setup and test your HuggingFace access\n",
    "from huggingface_hub import InferenceClient\n",
    "print(get_answer_hf(\"What is the capital of France?\"))\n",
    "print(get_answer_hf(\"What is the capital of France?\", model=\"NousResearch/Hermes-3-Llama-3.1-8B\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677b284-78bf-4cc0-861a-1d91c1945702",
   "metadata": {},
   "source": [
    "## prompt technieken - principes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12afe8-930d-4f42-942c-593e1335c68b",
   "metadata": {},
   "source": [
    "De voorbeelden (en prompts) zullen soms in het Engels zijn. Hoewel Nederlands vaak redelijk werkt, zijn de resultaten toch veel betrouwbaarder en nuttiger in het Engels. Zowel bij ChatGPT 3.5, maar vooral wanneer we bv. een model lokaal draaien, of wanneer we een kleiner model gebruiken.\n",
    "Ondertussen is er een duidelijke vooruitgang in de talenkennis met ChatGPT4o-mini (gebruik deze voor alledaagse taken) en ChatGPT4o (gebruik deze voor complexere taken).\n",
    "\n",
    "\n",
    "### categorie 1: input\n",
    "Input data is de data die beschrijft waar het over gaat bij de prompt, de tekst die moet samengevat worden, de bachelorproef die moet geëvalueerd worden, het stuk code waar uitleg over moet gegeven worden.\n",
    "#### input - gebruik scheidingstekens\n",
    "Gebruik scheidingstekens om bepaalde delen van de vraag duidelijk af te bakenen, scheidingstekens zijn bvb. ``` of \"\"\" of <><>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba1587-5d7c-4898-845a-c217cfa07362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:00:36.579098Z",
     "start_time": "2023-10-03T09:00:33.951211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is a new discipline that involves tailoring questions asked to language models like ChatGPT in order to improve the usability and reliability of the answers, and although the field is still evolving, there are already some principles established on how to achieve better results from these models.\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "Prompt engineering is tinkering with the question \\\n",
    "(the prompt) you ask LLMs so that you get a better answer. \n",
    "\n",
    "It soon became apparent that with ChatGPT etc., the usability \\\n",
    "and reliability of the answer can be greatly improved by asking \\\n",
    "the question in a certain way. A new discipline was thus born: \\\n",
    "prompt engineering. \n",
    "\n",
    "The interest in this is very new, and as the models mature more \\\n",
    "and more consensus of what \"best practices\" are will emerge here \\\n",
    "as well, but we can already establish some principles on how to \\\n",
    "get better results from these models, depending on what you are \\\n",
    "trying to achieve. \n",
    "The chances that this chapter will be the same \\\n",
    "next year (or even by the end of the semester) are pretty slim, \\\n",
    "this area of research is changing very quickly.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks into a single sentence \\\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e74ad-3aad-4737-aadb-07f7adac80b7",
   "metadata": {},
   "source": [
    "### categorie 2: instructies \n",
    "De instructies beschrijven wat het LLM moet doen.\n",
    "\"Geef 5 aanbevelingen voor nieuwe boeken\", \"Beschrijf alle schrijf- en grammaticale fouten, geef twee verbeterpunten aan waar de student mee aan de slag kan\", \"Leg uit wat dit stuk code doet\"\n",
    "#### instructies: geef duidelijke, concrete instructies\n",
    "Geef duidelijke, concrete instructies.\n",
    "\n",
    "- Een voorbeeld van een vage instructie: \\\n",
    "prompt = f\"\"\"You will be presented with a news article. Extract useful information in a structured format. \"\"\"\n",
    "\n",
    "- Een voorbeeld van een duidelijke, concrete instructie: \\\n",
    "prompt = f\"\"\"You will be presented with a news article. Your task it to identify any opininions expressed about the government, and their sentiment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402b9b1c-d874-4786-9554-d066359216b0",
   "metadata": {},
   "source": [
    "#### instructies: vraag gestructureerde output\n",
    "LLM's hebben een goede notie van hoe correcte JSON (of zelfs HTML) er uit ziet, door een skelet van de gevraagde output te voorzien kan je veel beter gestructureerde output krijgen (die je dan weer veel vlotter kan verwerken met volgende prompts of in andere code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7509d1b1-0422-4dfc-8ec4-03f2640f3c5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:02:47.555912Z",
     "start_time": "2023-10-03T09:02:44.635194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"transaction_id\": \"1\",\n",
      "    \"amount\": 50.00,\n",
      "    \"date\": \"2022-01-10\",\n",
      "    \"place\": \"Grocery Store\"\n",
      "  },\n",
      "  {\n",
      "    \"transaction_id\": \"2\",\n",
      "    \"amount\": 25.50,\n",
      "    \"date\": \"2022-01-12\",\n",
      "    \"place\": \"Gas Station\"\n",
      "  },\n",
      "  {\n",
      "    \"transaction_id\": \"3\",\n",
      "    \"amount\": 100.75,\n",
      "    \"date\": \"2022-01-15\",\n",
      "    \"place\": \"Restaurant\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Generate an array of three bank transactions, provide them in JSON format \\\n",
    "with the following keys: transaction_id, amount, date, place \\ \n",
    "Only output the json object\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "json_data = res\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7935e305-7fff-44a7-88e8-8bbbc7fe5ef9",
   "metadata": {},
   "source": [
    "#### instructies: split complexe taken op in eenvoudigere taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ba462b6c-fc59-4758-b64e-ecdac210cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article = \"'It is not for Rishi Sunak to decide that negociations are over before he has even stepped in the room', they added.\"\n",
    "article = \"This dispute will end only at the negotiating table. \\\n",
    "If the PM was hoping to demoralise and divide our profession with his actions, he will be disappointed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5b549584-3b40-499b-8d40-b85b02012cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are presented with a news article: {article}. \\\n",
    "Your task is to identify any opinions expressed about the government, and their sentiment. \\\n",
    "Approach this task step-by-step, take your time and do not skip steps. \\\n",
    "The steps to follow are the following:\n",
    "1. Read a paragraph of the news article\n",
    "2. Determine whether an opinion is expressed in this paragraph. If not, continue to the next paragraph.\n",
    "3. If there is an opinion, extract a JSON with keys as follows:\n",
    "- opinion: allowable values are \"positive\", \"negative\" or \"neutral\"\n",
    "- evidence: contains a list of strings evidencing the opinion\n",
    "- speaker: the person or government body who expressed the opinion.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "491648e2-001d-4edc-ae17-f18d8b69aa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"This dispute will end only at the negotiating table. If the PM was hoping to demoralise and divide our profession with his actions, he will be disappointed.\"\n",
      "\n",
      "2. Yes, an opinion is expressed in this paragraph.\n",
      "\n",
      "3. {\n",
      "    \"opinion\": \"negative\",\n",
      "    \"evidence\": [\n",
      "        \"If the PM was hoping to demoralise and divide our profession with his actions, he will be disappointed.\"\n",
      "    ],\n",
      "    \"speaker\": \"unknown\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40a6a2-95ff-47a2-8420-b81aefa8f413",
   "metadata": {},
   "source": [
    "#### instructies: laat het model eerst zelf een oplossing uitwerken, alvorens een conclusie te trekken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e48f87c-83a1-4852-b620-39f35418f628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Het antwoord van de student is correct.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Bepaal of het antwoord van de student correct is of niet.\n",
    "\n",
    "Vraag:\n",
    "Er wordt een opslagplaats gebouwd, de bouwkost wordt geraamd op €150 \\\n",
    "per vierkante meter.\n",
    "De grond zelf kost €200 per vierkante meter\n",
    "Het contract om het gebouw te bewaken is eenmalig €5000 en \\\n",
    "daarbovenop €10 per vierkante meter per jaar.\n",
    "\n",
    "Hoeveel kost het gebouw mij het eerste jaar?\n",
    "\n",
    "Oplossing van de student:\n",
    "Stel dat x het aantal vierkante meter is\n",
    "Kost van de grond: 200x\n",
    "Bouwkost: 150x\n",
    "Bewakingsfirma: 5000 + 100x\n",
    "Totale kost: 200x + 150x + 5000 + 100x = 450x + 5000\n",
    "\"\"\"\n",
    "response = get_answer(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b36cef-9a49-4120-b71d-25b06165523b",
   "metadata": {},
   "source": [
    "Merk op dat het antwoord van de student helemaal niet correct is, er is een (typ?)fout in de redenering, de bewakingsfirma kost 5000+10x, niet 5000+100x\n",
    "\n",
    "We kunnen hier betere resultaten bekomen door het model eerst zelf een volledige oplossing te laten uitwerken en dan het resultaat te vergelijken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "575160ad-8dc5-4fc4-988b-4904e2908abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uitwerking van de oplossing:\n",
      "\n",
      "Kost van de grond: 200 euro per vierkante meter\n",
      "Kost van de bouw: 150 euro per vierkante meter\n",
      "Kost van de bewakingsfirma: eenmalig 5000 euro + 10 euro per vierkante meter\n",
      "\n",
      "De totale kost voor het gebouw het eerste jaar is dus:\n",
      "Totaal = (200 + 150 + 10) * x + 5000\n",
      "Totaal = 360x + 5000\n",
      "\n",
      "Dus, de totale kost voor het gebouw het eerste jaar bedraagt 360x + 5000 euro.\n",
      "\n",
      "Het antwoord van de student is niet correct, aangezien de totale kost van het gebouw het eerste jaar 360x + 5000 euro is, terwijl de student 450x + 5000 euro heeft als totale kost. \n",
      "\n",
      "Antwoord: Nee\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Bepaal of het antwoord van de student correct is of niet.\n",
    "\n",
    "Om dit te bepalen moet je de volgende stappen ondernemen:\n",
    "- Werk eerst zelf de volledige oplossing uit\n",
    "- Vergelijk vervolgens jouw antwoord met dat van de student om \\\n",
    "te kijken of de student zijn antwoord correct is of niet\n",
    "Bepaal niet of het antwoord van de student correct is alvorens \\ \n",
    "je de oplossing volledig uitgewerkt hebt.\n",
    "\n",
    "Geef als uitvoer jouw uitgewerkte oplossing, gevolgd door \\\n",
    "het antwoord op de vraag: \"is de oplossing van de student dezelfde als die van jou\"\n",
    "\n",
    "Vraag:\n",
    "Er wordt een opslagplaats gebouwd, de bouwkost wordt geraamd op €150 \\\n",
    "per vierkante meter.\n",
    "De grond zelf kost €200 per vierkante meter\n",
    "Het contract om het gebouw te bewaken is eenmalig €5000 en \\\n",
    "daarbovenop €10 per vierkante meter per jaar.\n",
    "\n",
    "Hoeveel kost het gebouw mij het eerste jaar?\n",
    "\n",
    "Oplossing van de student:\n",
    "Stel dat x het aantal vierkante meter is\n",
    "Kost van de grond: 200x\n",
    "Bouwkost: 150x\n",
    "Bewakingsfirma: 5000 + 100x\n",
    "Totale kost: 200x + 150x + 5000 + 100x = 450x + 5000\n",
    "\"\"\"\n",
    "response = get_answer(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409b032-bc84-4370-88bc-19b97ed057a0",
   "metadata": {},
   "source": [
    "Nu is het antwoord wel juist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf9d97-e6bb-4779-9400-649edfc6c0bc",
   "metadata": {},
   "source": [
    "#### instructies: geef het model tijd om te denken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef97c55-5048-443b-97ec-74695515564a",
   "metadata": {},
   "source": [
    "Een voorbeeld van een prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec851a0-e670-4166-a992-254142a18860",
   "metadata": {},
   "source": [
    "prompt = f\"\"\"Approach this task step-by-step, take your time and do not skip steps.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a12799-084e-4d02-acf2-97f4ac00a7f6",
   "metadata": {},
   "source": [
    "### categorie 3: examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38182e8-4959-4d54-ac0e-1d562b6d4e5a",
   "metadata": {},
   "source": [
    "Een krachtige strategie om het model betere resultaten te laten produceren, is door voorbeelden van de taak die je wilt dat het model uitvoert, op te nemen in de prompt.\n",
    "\n",
    "Voorbeelden geven binnen het context window wordt 'in-context learning' genoemd. Met in-context learning kun je LLM's meer helpen leren over de gevraagde taak door voorbeelden of extra gegevens in de prompt op te nemen. \n",
    "\n",
    "Hier is een concreet voorbeeld.\n",
    "Binnen de getoonde prompt vraag je het model om het sentiment van een recensie te classificeren. \n",
    "De prompt bestaat uit:\n",
    "- een instructie: 'Classify this review',\n",
    "- de inhoud: 'I loved this movie!',\n",
    "- output format: 'Sentiment:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6be12624-8564-4b6d-9dfa-639895de75f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# ChatGPT4o-mini\n",
    "response = get_answer(\"Classify this review: I loved this movie! Sentiment:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4b2a5-a7e2-41a4-b5fa-f7e955170dae",
   "metadata": {},
   "source": [
    "Deze methode, inclusief je invoergegevens binnen de prompt, wordt **'zero-shot inference'** genoemd. De grootste LLM's zijn verrassend goed in dit opzicht, ze begrijpen de te voltooien taak en geven een goed antwoord. In dit voorbeeld identificeert het model het sentiment correct als positief. Kleinere modellen kunnen hier echter moeite mee hebben. Hieronder is een voorbeeld. Zoals je kunt zien, volgt het model de instructie niet. Hoewel het tekst genereert die enigszins gerelateerd is aan de prompt, kan het model de details van de taak niet achterhalen en identificeert het het sentiment niet. Dit is waar het geven van een voorbeeld binnen de prompt de prestaties kan verbeteren. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666dc29-8ccb-4dcc-9e57-1639772ba22c",
   "metadata": {},
   "source": [
    "![in_context_learning_zero_shot_inference](img/in_context_learning_zero_shot_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fdb73-158c-4db9-99ce-ef12b48a52ef",
   "metadata": {},
   "source": [
    "In de figuur hieronder is de prompttekst langer. De prompttekst begint met een voorbeeld dat de taken die uitgevoerd moeten worden aan het model demonstreert. Ik vond deze film geweldig, gevolgd door een voltooide sentimentanalyse. In dit geval is de recensie positief.\n",
    "\n",
    "Vervolgens geeft de prompt de instructie opnieuw en bevat de daadwerkelijke invoerrecensie die we willen dat het model analyseert. Je geeft deze nieuwe langere prompt door aan het kleinere model, dat nu een betere kans heeft om de taak die je specificeert en het formaat van de gewenste respons te begrijpen. De opname van één voorbeeld staat bekend als **'one-shot inference'**, in tegenstelling tot de zero-shot prompt die je eerder gaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01495ee-c528-45c6-b23e-f608120c9752",
   "metadata": {},
   "source": [
    "![in_context_learning_one_shot_inference](img/in_context_learning_one_shot_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f77bd-eed4-49b5-8f4e-a6e7eab14bef",
   "metadata": {},
   "source": [
    "Soms zal één voorbeeld niet voldoende zijn voor het model om te leren wat je wilt dat het doet. Je kan dan meerdere voorbeelden meegeven om het model nog beter te sturen. Het meegeven van meerdere voorbeelden heet **\"few shot inference\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0de30-7fea-41d4-8eaf-777d10cfa716",
   "metadata": {},
   "source": [
    "#### Samengevat:\n",
    "Je kunt je prompts ontwerpen om het model aan te moedigen te leren door voorbeelden. Terwijl de grootste modellen goed zijn in zero-shot inference zonder voorbeelden, kunnen kleinere modellen profiteren van one-shot of few-shot inference die voorbeelden van het gewenste gedrag bevatten. \n",
    "\n",
    "Je bent hierbij gelimiteerd door de 'context window',  de hoeveelheid in-context learning die je in het model kunt doorgeven is beperkt. Over het algemeen, als je merkt dat je model niet goed presteert wanneer je bijvoorbeeld vijf of zes voorbeelden opneemt, moet je proberen je model te finetunen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504daa3-f34f-4e92-8cba-ec550ad81fb6",
   "metadata": {},
   "source": [
    "### categorie 4: constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d8624-63fc-4f3e-bf09-32f5b109561b",
   "metadata": {},
   "source": [
    "De constraints dienen om af te lijnen waartoe het LLM zich moet beperken. Dit kan gaan om vormelijke beperkingen \"Genereer enkel een JSON file met key 'boek' en key 'author'\", \"Maximaal 100 woorden lang\", \"in de stijl van Shakespeare\".\n",
    "\n",
    "Maar er kunnen ook inhoudelijke beperkingen zijn: \"Wees vriendelijk\", \"Eindig met een positieve noot\", \"Zorg dat de boeken van hetzelfde genre zijn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb6a6b8c-4a46-4382-9575-822b363d50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_answer(\"Schrijf python code die een histogram maakt van volgende data: {jaar:[2022, 2023, 2024], frequency:[0.4, 0.6, 0.65]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84f9feb2-cdb4-4c43-a2b3-469002423385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natuurlijk! Hier is een voorbeeld van Python-code die een histogram genereert op basis van de gegeven data. We maken gebruik van de populaire bibliotheek `matplotlib` om het histogram te plotten.\n",
      "\n",
      "Zorg ervoor dat je `matplotlib` hebt geïnstalleerd. Je kunt het installeren met pip als dat nog niet gedaan is:\n",
      "\n",
      "```bash\n",
      "pip install matplotlib\n",
      "```\n",
      "\n",
      "Hier is de code:\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Gegeven data\n",
      "data = {\n",
      "    'jaar': [2022, 2023, 2024],\n",
      "    'frequency': [0.4, 0.6, 0.65]\n",
      "}\n",
      "\n",
      "# Plot het histogram\n",
      "plt.bar(data['jaar'], data['frequency'], color='skyblue')\n",
      "\n",
      "# Voeg titels en labels toe\n",
      "plt.title('Histogram van Frequenties per Jaar')\n",
      "plt.xlabel('Jaar')\n",
      "plt.ylabel('Frequentie')\n",
      "\n",
      "# Toon de grafiek\n",
      "plt.xticks(data['jaar'])  # Zorg ervoor dat de jaren goed op de x-as staan\n",
      "plt.ylim(0, 1)  # Zet de y-as limieten van 0 tot 1, omdat de frequenties tussen 0 en 1 liggen\n",
      "plt.grid(axis='y')\n",
      "\n",
      "# Laat de grafiek zien\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "Bij het uitvoeren van deze code krijg je een histogram met de opgegeven jaren op de x-as en de frequenties op de y-as.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f849070-6646-4026-876a-c38ced402d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_answer(\"Schrijf python code die een histogram maakt van volgende data: {jaar:[2022, 2023, 2024], \\\n",
    "frequency:[0.4, 0.6, 0.65]}. Beperk de output to het weergeven van de Python code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "071e9039-230a-4a0e-b4fc-4b45bf3d1ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "data = {\n",
      "    'jaar': [2022, 2023, 2024],\n",
      "    'frequency': [0.4, 0.6, 0.65]\n",
      "}\n",
      "\n",
      "plt.bar(data['jaar'], data['frequency'], color='blue')\n",
      "plt.xlabel('Jaar')\n",
      "plt.ylabel('Frequentie')\n",
      "plt.title('Histogram van Frequentie per Jaar')\n",
      "plt.xticks(data['jaar'])\n",
      "plt.ylim(0, 1)\n",
      "plt.show()\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b7445-3d3c-43c2-975a-b92e3e788d59",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### categorie 5: context or role\n",
    "\n",
    "De context van een prompt beschrijft meestal op welke manier het LLM zich dient te gedragen, \"Je bent een expert JavaScript developer\", \"Je bent een AI assistent die boeken aanraadt op basis van gebruikers voorkeuren\", \"Je bent een hogeschool lector die bachelorproefvoorstellen evalueert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d7035bf09d27d34f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:06:08.086781Z",
     "start_time": "2023-10-03T09:06:03.325300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const express = require('express');\n",
      "const app = express();\n",
      "\n",
      "app.get('/transactions', (req, res) => {\n",
      "  const transactions = [\n",
      "    {\n",
      "      \"transaction_id\": \"1\",\n",
      "      \"amount\": 50.00,\n",
      "      \"date\": \"2022-01-10\",\n",
      "      \"place\": \"Grocery Store\"\n",
      "    },\n",
      "    {\n",
      "      \"transaction_id\": \"2\",\n",
      "      \"amount\": 25.50,\n",
      "      \"date\": \"2022-01-12\",\n",
      "      \"place\": \"Gas Station\"\n",
      "    },\n",
      "    {\n",
      "      \"transaction_id\": \"3\",\n",
      "      \"amount\": 100.75,\n",
      "      \"date\": \"2022-01-15\",\n",
      "      \"place\": \"Restaurant\"\n",
      "    }\n",
      "  ];\n",
      "\n",
      "  res.json(transactions);\n",
      "});\n",
      "\n",
      "app.listen(3000, () => {\n",
      "  console.log('Server is running on port 3000');\n",
      "});\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You're an expert JavaScript Developer. Construct a GET request handler which returns a json in the following format ```{json_data}``` when requesting the path /transactions using NodeJs.\n",
    "only output the code without formatting\n",
    "\"\"\"\n",
    "\n",
    "response = get_answer(prompt)\n",
    "code = response;\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ff14d271c48f5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### oefening\n",
    "\n",
    "1. Creëer een prompt  die een databank schema genereert zodat het bovenstaande json formaat kan opgeslaan worden in een SQLite databank.\n",
    "2. Creëer vervolgens een prompt die als datalayer van een NodeJs die data ophaalt.\n",
    "3. En laat de LLM als laatste de door mij gegenereerde JavaScript code aanpassen zodat deze datalayer gebruikt wordt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d89b63cd173559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:25:46.533181Z",
     "start_time": "2023-10-03T09:25:38.123186Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Het bovenstaande codefragment is een Node.js HTTP-server die een JSON-array van transacties retourneert wanneer een GET-verzoek naar de URL \"/transactions\" wordt gemaakt.\n",
      "\n",
      "Om een SQLite-databaseschema te maken op basis van de JSON-gegevens, moeten we de structuur van de JSON begrijpen. Het lijkt erop dat elke transactie objecteigenschappen heeft zoals \"transaction_id\", \"amount\", \"date\" en \"place\". We kunnen deze eigenschappen gebruiken om de tabellen en kolommen van het SQLite-schema te maken.\n",
      "\n",
      "Hier is een voorbeeld van een SQLite-databaseschema op basis van de JSON-gegevens:\n",
      "\n",
      "```sql\n",
      "CREATE TABLE transactions (\n",
      "  transaction_id TEXT PRIMARY KEY,\n",
      "  amount INTEGER,\n",
      "  date TEXT,\n",
      "  place TEXT\n",
      ");\n",
      "```\n",
      "\n",
      "Het bovenstaande SQL-script maakt een tabel genaamd \"transactions\" met vier kolommen, namelijk \"transaction_id\", \"amount\", \"date\" en \"place\". De \"transaction_id\" kolom wordt gedefinieerd als een teksttype en ingesteld als de primaire sleutel. De \"amount\", \"date\" en \"place\" kolommen zijn ook gedefinieerd als teksttypes.\n",
      "\n",
      "U kunt dit SQL-script gebruiken om de SQLite-database en de \"transactions\" tabel te maken. Vervolgens kunt u de JSON-gegevens gebruiken om de tabel in te vullen met behulp van INSERT-opdrachten.\n",
      "\n",
      "Het is belangrijk om op te merken dat het bovenstaande SQLite-databaseschema een vereenvoudigde weergave is die alleen rekening houdt met de gegeven JSON-gegevens. In een echte toepassing kunnen er meer tabellen en kolommen zijn om aan de vereisten te voldoen.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Creeer een SQLite databank schema van onderstaande json data ```{code}```\"\"\"\n",
    "\n",
    "response = get_answer(prompt)\n",
    "print (response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66ed27cc9cf704",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:29:42.501977Z",
     "start_time": "2023-10-03T09:29:32.654119Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Om een datalaag te maken in Node.js die gegevens ophaalt uit een database, moet je een databasebibliotheek installeren, zoals `pg` voor PostgreSQL-database.\n",
      "\n",
      "Om te beginnen, installeer de `pg`-bibliotheek met npm:\n",
      "\n",
      "```\n",
      "npm install pg\n",
      "```\n",
      "\n",
      "Vervolgens kun je de volgende code gebruiken om de datalaag te maken:\n",
      "\n",
      "```javascript\n",
      "const http = require('http');\n",
      "const { Client } = require('pg');\n",
      "\n",
      "// Maak een nieuwe databaseclient\n",
      "const client = new Client({\n",
      "  user: 'database_user',\n",
      "  host: 'localhost',\n",
      "  database: 'database_name',\n",
      "  password: 'database_password',\n",
      "  port: 5432,\n",
      "});\n",
      "\n",
      "// Maak verbinding met de database\n",
      "client.connect();\n",
      "\n",
      "const server = http.createServer((req, res) => {\n",
      "  if (req.url === '/transactions' && req.method === 'GET') {\n",
      "    res.setHeader('Content-Type', 'application/json');\n",
      "\n",
      "    // Query om transacties op te halen\n",
      "    const query = 'SELECT * FROM transactions';\n",
      "\n",
      "    // Voer de query uit\n",
      "    client.query(query, (err, result) => {\n",
      "      if (err) {\n",
      "        console.error('Fout bij het uitvoeren van query', err);\n",
      "        res.statusCode = 500;\n",
      "        res.end();\n",
      "        return;\n",
      "      }\n",
      "\n",
      "      // Resultaat omzetten naar het gewenste formaat\n",
      "      const transactions = result.rows.map(row => ({\n",
      "        transaction_id: row.transaction_id,\n",
      "        amount: row.amount,\n",
      "        date: row.date,\n",
      "        place: row.place,\n",
      "      }));\n",
      "\n",
      "      // Stuur het resultaat terug naar de client\n",
      "      res.end(JSON.stringify(transactions));\n",
      "    });    \n",
      "  } else {\n",
      "    res.statusCode = 404;\n",
      "    res.end();\n",
      "  }\n",
      "});\n",
      "\n",
      "server.listen(3000, () => {\n",
      "  console.log('Server is running on port 3000');\n",
      "});\n",
      "```\n",
      "\n",
      "Zorg ervoor dat je de juiste waarden voor `user`, `host`, `database`, `password` en `port` opgeeft in de `Client`-constructor om verbinding te maken met je database.\n",
      "\n",
      "Deze code maakt een nieuwe client, maakt verbinding met de database en voert vervolgens een query uit om de transacties op te halen vanuit de database. Het resultaat wordt vervolgens omgezet naar het gewenste formaat en teruggestuurd naar de client.\n",
      "\n",
      "Opmerking: zorg ervoor dat je het juiste databaseverbindingsinformatie gebruikt en controleer de documentatie van de `pg`-bibliotheek voor meer informatie over hoe je verbinding kunt maken en query's kunt uitvoeren met verschillende databases.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Creëer een datalayer met NodeJs die data ophaalt uit een databank gecreeerd met het volgende schema ```CREATE TABLE transactions (\n",
    "  transaction_id TEXT PRIMARY KEY,\n",
    "  amount INTEGER,\n",
    "  date TEXT,\n",
    "  place TEXT\n",
    ");```. Verander de code tussen triple backticks ```{code}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_answer(prompt)\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6448010-23a4-40bf-8399-b2b9e86e4247",
   "metadata": {},
   "source": [
    "#### Role\n",
    "Wanneer je de openai api gebruikt, dien je in de 'message' zowel een rol als een context mee te geven. De mogelijke rollen zijn: system, user of assistant.\n",
    "\n",
    "De 'User' is de gebruiker. 'Assistant' is de chatbot (of ai assistant). Door beide rollen in een 'message' te gebruiken, kan je delen van eerdere conversaties terug als context meesturen bij een volgende vraag.\n",
    "\n",
    "De context van de 'system' rol legt vast hoe het systeem zich dient te gedragen.\n",
    "\n",
    "De voordelen van het specificeren van een rol zijn:\n",
    "\n",
    "grotere context: Chatbots hebben een beperkte context, naarmate een conversatie vordert, vergeten ze wat ze eerder zeiden. Door het antwoord van de 'assistent' op te nemen in de volgende context, wordt dit minder snel vergeten.\n",
    "standvastigheid: de rol die je vastlegt blijft makkelijker behouden, de chat messages met de gebruiker nadien gaan minder snel 'afleiden'.\n",
    "Een (theoretisch) voorbeeld:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1649ae1-3d3d-4351-ad7b-be9c4f732cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfe7ba3-5bc5-476e-8a1a-9e5899d31f4e",
   "metadata": {},
   "source": [
    "De huidige versie 'ChatGPT-4o' en zelfs de iets lichtere versie 'ChatGPT-4o-mini', zijn ondertussen wonderbaarlijk goed in het beantwoorden van opeenvolgende vragen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53bb21-ab5d-4e30-b564-1245715109a0",
   "metadata": {},
   "source": [
    "Hieronder volgen een aantal tips om goede prompts te schrijven, alsook enkele voorbeelden.\n",
    "\n",
    "### principe 1 : prompts iteratief verbeteren\n",
    "\n",
    "Als je prompts ontwerpt om dingen (programmatorisch) te bereiken met LLM's, zal je zelden van de eerste keer het resultaat bekomen dat je voor ogen had.\n",
    "Het loont om de prompts iteratief te verbeteren, preciezer te maken in wat je wenst door de instructies te verscherpen, tot je het gewenste resultaat, in het gewenste formaat bereikt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46709165-6b03-4d4d-8ac7-80be2623ec1c",
   "metadata": {},
   "source": [
    "#### te lange output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6435fbc-386a-4f54-b1f5-d92197945a04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:38:58.504925Z",
     "start_time": "2023-10-03T09:38:40.291458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Unlock the Future with 'Trends in AI' - Exploring the Revolutionary World of Artificial Intelligence\n",
      "\n",
      "Introduction:\n",
      "Welcome to the University College of Ghent, where innovation meets intellectual brilliance. Immerse yourself in the captivating realm of artificial intelligence (AI) through our cutting-edge course, 'Trends in AI.' Designed to equip students with foundational knowledge and practical skills, this course offers an unparalleled opportunity to explore and embrace the transformative power of AI.\n",
      "\n",
      "Course Overview:\n",
      "'Trends in AI' is a comprehensive exploration of the rapidly evolving field of artificial intelligence. From its fundamental principles to advanced applications, this course delves into the exciting world where human ingenuity intersects with machine intelligence. Combining theoretical knowledge with hands-on experiences, students develop the necessary skillset to leverage AI across various domains.\n",
      "\n",
      "Key Highlights:\n",
      "1. Introduction to AI: Gain a deep understanding of AI concepts, including machine learning, neural networks, and natural language processing. Unearth the underlying principles that drive AI algorithms and their real-world applications.\n",
      "2. Emerging Trends: Stay ahead of the curve as we unravel the latest trends in AI, such as deep learning, computer vision, and robotics. Discover how these innovations are reshaping industries and creating new possibilities.\n",
      "3. Ethical Considerations: Engage in thought-provoking discussions surrounding the ethical implications of AI. Examine issues like bias, privacy, and accountability, gaining the tools to navigate the potential pitfalls of AI implementation.\n",
      "4. Practical Applications: Put theory into practice through hands-on projects and case studies. Develop AI models, work with state-of-the-art tools and explore real-world datasets. Cultivate problem-solving skills necessary for AI implementation in diverse domains.\n",
      "5. Industry Insights: Benefit from guest lectures and workshops led by leading experts in the AI field. Gain valuable insights into industry best practices, real-world challenges, and career opportunities within the AI landscape.\n",
      "6. Capstone Project: Culminate your learning journey with a challenging capstone project. Apply your acquired knowledge in a practical scenario, showcasing your ability to think critically, solve complex problems, and drive innovation using AI.\n",
      "\n",
      "Why Take 'Trends in AI'?\n",
      "1. Relevance: AI is transforming industries worldwide, and 'Trends in AI' prepares you to thrive in this rapidly evolving landscape. Equip yourself with the skills demanded by future employers across a multitude of sectors.\n",
      "2. In-Demand Expertise: AI professionals are highly sought after, with careers in cutting-edge technologies, research, consulting, and even entrepreneurship. Gain a competitive edge in the job market by mastering the foundations of AI.\n",
      "3. Interdisciplinary Nature: 'Trends in AI' is designed for students from diverse academic backgrounds. Whether you are studying computer science, engineering, business, healthcare, or design, this course provides a cross-disciplinary exploration of AI's applications.\n",
      "4. Practical Skill Development: Navigate the realm of AI through hands-on projects, ensuring you are equipped with the practical knowledge to turn theory into impactful solutions. Build a strong portfolio showcasing real-world AI applications.\n",
      "5. Future-Proofing: The application of AI will continue to expand exponentially. By understanding the trends and implications, you position yourself as an adaptive and future-proof professional, ready to embrace AI's transformative potential.\n",
      "\n",
      "Conclusion:\n",
      "Don't miss the opportunity to join our 'Trends in AI' course and make your mark on the dynamic world of artificial intelligence. With a uniquely designed curriculum, hands-on experiences, and expert faculty, we invite you to uncover the limitless possibilities AI offers. Enroll today and unlock a future brimming with endless opportunities!\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You're tasked to help the university college \\\n",
    "of ghent write a marketing piece for their \\\n",
    "website to promote the new course 'Trends in AI'\n",
    "\n",
    "Describe the contents of the course and why it's \\\n",
    "really interesting for students to take this course\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece1b96-9518-4200-8185-4db501b25425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:40:15.061157Z",
     "start_time": "2023-10-03T09:40:13.552057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore cutting-edge applications of artificial intelligence and learn to analyze emerging trends. Gain a competitive edge in the digital era.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You're tasked to help the university college \\\n",
    "of ghent write a marketing piece for their \\\n",
    "website to promote the new course 'Trends in AI'\n",
    "\n",
    "Describe the contents of the course and why it's \\\n",
    "really interesting for students to take this course\n",
    "\n",
    "Use at most 25 words.\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20007df6-a52c-40c0-8ce5-c2b2ca476589",
   "metadata": {},
   "source": [
    "#### verkeerd doelpubliek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5402bf5b-8774-4aae-aa62-d58740c2a242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:41:38.937718Z",
     "start_time": "2023-10-03T09:41:37.669516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Discover the latest advancements in Artificial Intelligence in our new course. Gain insights, practical skills, and be at the forefront of innovation.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You're tasked to help the university college \\\n",
    "of ghent write a marketing piece for their \\\n",
    "website to promote the new course 'Trends in AI'\n",
    "\n",
    "Describe the contents of the course and why it's \\\n",
    "really interesting for students to take this course\n",
    "\n",
    "Use at most 25 words, the description is intended for future students\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d86bba7-be73-4130-8b09-af9a181b4363",
   "metadata": {},
   "source": [
    "#### verkeerd formaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d4f19-e0f1-4334-9603-a999db3fc942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:42:00.435737Z",
     "start_time": "2023-10-03T09:41:58.751435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discover the cutting-edge world of artificial intelligence at University College of Ghent! Explore emerging trends and unlock limitless career opportunities. #AI #future #education\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You're tasked to help the university college \\\n",
    "of ghent write a marketing piece for their \\\n",
    "website to promote the new course 'Trends in AI'\n",
    "\n",
    "Describe the contents of the course and why it's \\\n",
    "really interesting for students to take this course\n",
    "\n",
    "Use at most 25 words, the description is intended for future students, end with some relevant hashtags\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11653118-aae5-4eed-a5d1-e9e05d0d8409",
   "metadata": {},
   "source": [
    "### principe 2: samenvattingen\n",
    "\n",
    "LLM's zijn vrij goed in het samenvatten van grote(re) stukken tekst, maar je kan mits wat bijsturen de output vaak sterk verbeteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7cfd26-5a5d-4233-aa75-3263e6dca13a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:43:42.494140Z",
     "start_time": "2023-10-03T09:43:42.482826Z"
    }
   },
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "wat\n",
    "\n",
    "Prompt engineering is het sleutelen aan de vraag (de prompt) \\\n",
    "die je aan LLM's stelt zodat je een beter antwoord krijgt.\n",
    "\n",
    "Er bleek al snel dat bij ChatGPT e.d. de bruikbaarheid en \\\n",
    "betrouwbaarheid van het antwoord sterk kan verbeterd worden \\\n",
    "door de vraag op een bepaalde manier te stellen. Er onstond \\\n",
    "zo een nieuwe discipline: prompt engineering.\n",
    "\n",
    "De interesse hierin is zeer nieuw, en naarmate de modellen \\\n",
    "matuurder worden zal hier ook meer en meer consensus ontstaan \\\n",
    "van wat \"best practices\" zijn, maar we kunnen toch al een \\\n",
    "aantal principes opstellen over hoe je betere resultaten \\\n",
    "krijgt uit deze modellen, afhankelijk van wat je probeert \\\n",
    "te bereiken. De kans dat dit hoofdstuk volgend jaar (of zelfs \\\n",
    "tegen het einde van het semester) hetzelfde is, is vrij gering, \\\n",
    "dit onderzoeksdomein wijzigt zeer snel.\n",
    "\n",
    "setup\n",
    "\n",
    "We gaan met de OpenAI API connecteren, hiervoor heb je een OpenAI \\\n",
    "API key nodig, en een account bij OpenAI. De eerste €18 zijn gratis, \\\n",
    "maar je bent ook sowieso in tijd beperkt. (of je de €18 nu opgebruikt \\\n",
    "of niet, na drie maanden is de API niet langer gratis)\n",
    "\n",
    "Om dit een beetje te duiden, je betaalt bij ChatGPT per 'token'. ChatGPT \\\n",
    "(en andere LLM's) genereren hun teksten in stukjes die we tokens noemen. \\\n",
    "Het hangt van de taal af, maar bij Engels en Nederlands komen tokens min \\\n",
    "of meer overeen met het aantal lettergrepen. Je betaalt zowel voor tokens \\\n",
    "in de vraag als in het antwoord aan een €0.002 per 1000 tokens. Je €18 is \\\n",
    "dus goed voor 9.000.000 tokens. Om wat in de les (en thuis) manueel mee te \\\n",
    "prutsen zal dit ruimschoots voldoende zijn, maar als je dit 'echt' gebruikt \\\n",
    "in geautomatiseerde processen best je verbruik goed in het oog houden.\n",
    "\n",
    "Een alternatief is lokaal GPT4All draaien, die dezelfde API nabootst. De \\\n",
    "antwoorden zijn minder goed, maar het is wel altijd volledig gratis natuurlijk, \\\n",
    "dus zeker om alles op te zetten en te testen een aantrekkelijke optie. Ergens in \\\n",
    "juni werkte dit goed, begin september na een update kreeg ik het niet meer aan de \\\n",
    "praat op mijn macbook, hopelijk weer wel tegen dat jullie deze les krijgen. Maar \\\n",
    "los van de API draaien zijn er nog Python bindings, die wel werken, zie verder.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e394b629-cd2b-4bd9-b35a-7a3c89763348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:43:49.487589Z",
     "start_time": "2023-10-03T09:43:46.216386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering involves manipulating the question (prompt) asked to an LLM to obtain better answers. It is a new discipline with evolving best practices. Using the OpenAI API incurs costs per token, but there are alternatives like running GPT4All locally.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize the text below, delimited by triple backticks, use at most 30 words\n",
    "\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4d7d0-5a93-4cbe-afa8-3394332453f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:44:51.219068Z",
     "start_time": "2023-10-03T09:44:49.698999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering involves optimizing the question prompt to obtain better results. The cost to the student using ChatGPT is based on the number of tokens used, with the first €18 free but limited by time. An alternative is running GPT4All locally for free but with lower-quality results.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize the text below, delimited by triple backticks, use at most 30 words, focus on the cost to the student.\n",
    "\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6f111-573e-4f6e-9fd7-130fb2a25f25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:45:11.502954Z",
     "start_time": "2023-10-03T09:45:08.783871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses prompt engineering, a new discipline that focuses on improving the usefulness and reliability of answers generated by language models like ChatGPT through modifying the prompts. It also mentions the novelty and rapid evolution of the field. Additionally, it provides information on using the OpenAI API and the cost associated with using tokens. An alternative option is using a locally-run GPT4All, which is free but has slightly less accurate answers.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize the text below, delimited by triple backticks, use at most 30 words, focus on the novelty of the field.\n",
    "\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506d1f1-a056-4aa9-8cca-29de549c4bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T09:45:58.463191Z",
     "start_time": "2023-10-03T09:45:55.278419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Prompt engineering is the process of improving the usefulness and reliability of answers from language models.\n",
      "- The cost of using the OpenAI API for ChatGPT is based on the number of tokens used in both the question and answer.\n",
      "- The first €18 is free, but after three months, the API is no longer free.\n",
      "- The cost is €0.002 per 1000 tokens.\n",
      "- Using GPT4All locally is a free alternative with less accurate answers.\n",
      "- Python bindings are also available for use.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Summarize the text below, delimited by triple backticks, use at most 30 words, focus on the cost, use bullet points to make the cost clear.\n",
    "\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c1f01ebf94890",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### oefening\n",
    "\n",
    "1. Creëer een prompt die de uitleg over LLM's van vorige week samenvat. (Zie 'text' hieronder).\n",
    "2. Vervolgens willen we suggesties over hoe de uitleg kan verbeterd worden, en later ook uitgediept.\n",
    "3. Daarnaast een prompt die alle schrijffouten eruit haalt, we willen enkel de tekst zonder schrijffouten, geen blabla er rond.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0082b697-8cbb-4710-ac3b-262229003225",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "## model inference vs model training\n",
    " We spreken over:\n",
    "- \"model inference\", d.i. het gebruik van het model => draait op een computer\n",
    "- \"model training\", d.i. het leren van de parameters => dit is computationeel intensief.\n",
    "\n",
    "De machine learning-modellen die generatieve AI ondersteunen, hebben deze vaardigheden geleerd door statistische patronen te vinden in enorme datasets met content die oorspronkelijk door mensen is gegenereerd. Grote taalmodellen zijn getraind op triljoenen woorden gedurende vele weken en maanden, en met veel rekenkracht. Deze foundation modellen, zoals we ze noemen, met miljarden parameters, vertonen eigenschappen die verder gaan dan alleen taal. Onderzoekers ontdekken hun vermogen om complexe taken te ontleden en problemen op te lossen.\n",
    "\n",
    "Hier is een verzameling foundation modellen, soms ook basis modellen genoemd, en hun relatieve grootte in termen van parameters. Je zult later (eerder in het vak Deep Learning) in meer detail ingaan op deze parameters, maar je kan ze voorstellen als het geheugen van het model. Hoe meer parameters een model heeft, hoe meer geheugen, en zoals blijkt, hoe geavanceerder de taken die het kan uitvoeren. \n",
    "Stel dat we de LLMs voorstellen a.d.h.v. cirkels. Hier is een idee van het aantal parameters waarmee verschillende modellen getraind zijn:\n",
    "- GPT3: 175 miljard\n",
    "- BLOOM: 176 miljard\n",
    "- LLama2: 70 miljard\n",
    "\n",
    "![LLMs voorstelling](img/LLMs_voorstelling.png)\n",
    "\n",
    "### voorbeeld\n",
    "Hieronder is een voorbeeld om een gevoel te krijgen over wat het betekent in termen van resources om een model te trainen, dan wel een getraind model te gebruiken.\n",
    "\n",
    "Om een 70b-large language model te draaien, heb je in principe 2 files nodig: \n",
    "- Een 140GB file die de model parameters bevat,\n",
    "- Een file met +-500 lijnen code (for example C-code, or python code)\n",
    "\n",
    "Nota: 70b staat voor \"70 billion\" of 70 miljard parameters. Voor elke parameter heb je 2 bytes nodig - er wordt gebruik gemaakt van \"float16\" data - dus de file wordt 140 GB groot.\\\n",
    "De code file dient om het language model te draaien, gebruik makend van de parameters. Dus, om gebruik te maken vaneen model, heb je in principe geen internet connectie nodig. Het draait op een (krachtige) computer.\\\n",
    "Je beschouwt de code file, compileert het, er wordt een binary file gemaakt die je naar de parameters kan laten verwijzen en je kan met dit \"language model\" praten.\n",
    "Het \"language model\" gaat tekst genereren, het zal je instructies opvolgen.\n",
    "\n",
    "![llama2_70b_2](img/llama2_70b_2.png)\n",
    "\n",
    "Er is dus niet heel veel nodig om een model te gebruiken. De rekenkundige complexiteit komt erbij wanneer je die parameters wil bekomen, dus wanneer je het model traint. \\\n",
    "Wanneer je de parameters wilt leren, neem je \"een deel van het internet\" (om het concreet te maken, neem ongeveer 10 TB aan tekst). Om zo een hoeveelheid aan data te verwerken en de parameters van het neuraal netwerk te\n",
    "leren, heb je ongeveer 6000 GPU's nodig, gedurende 12 dagen. De kost hiervoor wordt geschat op 2 miljoen dollar.\n",
    "\n",
    "## Het model trainen\n",
    "Twee belangrijke fasen in het trainen van een LLM zijn:\n",
    "- het pre-trainen,\n",
    "- het finetunen.\n",
    "\n",
    "### LLM: trainingsfase 1 - het pre-trainen\n",
    "Erg gesimplificeerd voorgesteld is het objectief van een LLM om het volgende woord in een zin/tekst te voorspellen.\n",
    "\n",
    "Neem bijvoorbeeld de trainingszin “De lector sprak zijn _ aan”. Als in de oorspronkelijke tekst stond ‘student’, zal het model zijn gewichten zien versterken als hij student voorspelt.\n",
    "\n",
    "Maar natuurlijk, de eerste L van LLM’s, de “Large”, maakt dat we uit zoveel data trainen dat voor vele zinnen er meerdere mogelijkheden zijn. Er is niet langer één juist antwoord, in het voorbeeld zou naast “student” ook “collega”, “opleidingshoofd”, “vriendin”, … een mogelijkheid kunnen zijn.\n",
    "Dus bij het leren van de gewichten zullen we (conceptueel) een kans toewijzen aan elk van deze mogelijkheden, en dan zou ons model in een ideaal geval bijvoorbeeld 40% van de tijd student voorspellen, 20% van de tijd collega, 5% van de tijd vriendin enz.\n",
    "\n",
    "Om een LLM dan langere teksten te laten genereren wordt de output na het genereren van een woord gebruikt als input voor de volgende voorspelling.\n",
    "\n",
    "“De”  \n",
    "“De lector”  \n",
    "“De lector sprak”  \n",
    "\n",
    "Het eindresultaat is een LLM die verbazend vlotte teksten kan genereren.\n",
    "Dit pre-trained LLM noemen we een foundation model.\n",
    "\n",
    "Hieronder wordt de pre-training fase schematisch voorgesteld:\n",
    "![](/img/llm_pretraining_at_a_high_level_2.png)\n",
    "\n",
    "Er wordt heel veel ongestructureerde data verzameld (teksten), bv. van internet. De kwaliteit van deze data laat echter te wensen over. Er blijkt dat slechts 1 tot 3% van de origineel verzamelde data bruikbaar is om van te leren (dus om het model op te trainen). Toch blijft dit nog een enorme hoeveelheid data. Met deze data wordt de pre-training gedaan, en er wordt een foundation model gecreëerd. Dit foundation model beschikt dan over een heel pak kennis, in de vorm van een 'vocabulair'. Voor deze pre-trainingsfase is heel veel rekenkracht (GPU) en tijd nodig.\n",
    "\n",
    "In de pretrainingsfase spreken worden technieken als unsupervised learning en self-supervised learning gebruikt. Er komt geen menselijke feedback aan te pas.\n",
    "\n",
    "### LLM: trainingsfase 2 - het finetunen\n",
    "Maar, de teksten die het model genereert, zijn zeer afhankelijk van het soort tekst dat de trainingsdata vormde. Na het trainen van een foundation model, 'weet' dit model al veel, maar het is nog niet goed in het uitvoeren van specifieke taken, zoals bv. het samenvatten van tekst of het antwoorden op vragen. \n",
    "Als er veel vooroordelen in de oorspronkelijke tekst zaten, zullen die met een grote kans als ‘waarschijnlijk vervolg’ gekozen worden, en zal het large language model zonder veel moeite de meeste grove uitspraken maken.\n",
    "\n",
    "Dus, er is een extra stap nodig: het model moet gefinetuned worden. Het model wordt opnieuw getraind met extra informatie. Men maakt datasets waarin het gewenste gedrag gereflecteerd wordt, en gebruikt die om het model verder op te trainen. HIerbij wordt ook gebruik gemaakt van menselijke feedback.\n",
    "\n",
    "Deze extra informatie die gebruikt wordt voor finetuning is niet zo heel groot in volume (bv. 100000 vragen en bijhorende antwoorden), maar wel zeer kwalitatief voor de taak waarvoor men de LLM uiteindelijk wil gebruiken.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1344df3-4047-4101-af55-3940f825001d",
   "metadata": {},
   "source": [
    "### principe 3: intentie en gevoel\n",
    "\n",
    "Soms willen we (snel) weten of een stuk tekst (bvb. een review) positief of negatief is. Ik kan me best voorstellen dat social media managers snel willen kunnen inspelen op zeer negatieve comments als er iets begint te 'leven' in de commentaren van een facebook post.\n",
    "\n",
    "LLM's zijn hier ook vrij degelijk in, als voorbeeld heb ik een aantal reviews van een computermuis genomen van coolblue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd896d1b-82c2-4b62-8336-38a2592161a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:02:23.913891Z",
     "start_time": "2023-10-03T10:02:23.909784Z"
    }
   },
   "outputs": [],
   "source": [
    "review = f\"\"\"\n",
    "Ideal mouse for photo editing for me and daily use with macOS. \\\n",
    "Format takes a bit of getting used to and it's a pity that they \\\n",
    "don't include the USB connector BOLT with the macOS version. \\\n",
    "Bluetooth connection is excellent. Use this mouse in combination \\\n",
    "with a Mac Studio. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39b1de-3c61-49e1-9d0e-e0298b8c6489",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:03:04.117917Z",
     "start_time": "2023-10-03T10:03:03.268666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the product review is positive.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review delimited with triple backticks\n",
    "\n",
    "review: ```{review}```\n",
    "\"\"\"\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6279f40-2be2-4bb1-9904-b47410c11fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:03:24.750940Z",
     "start_time": "2023-10-03T10:03:24.273420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review delimited with triple backticks\n",
    "\n",
    "Give your answer with a single word, either \"positive\" of \"negative\"\n",
    "\n",
    "review: ```{review}```\n",
    "\"\"\"\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf4c255-f24a-412e-b077-6873961b5676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:04:10.507802Z",
     "start_time": "2023-10-03T10:04:08.274479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satisfied, disappointed, content\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify a list of emotions that the writer of the review delimited with triple backticks is expressing.\n",
    "\n",
    "Give no more than three emotions, as lower-case words separated by commas.\n",
    "\n",
    "review: ```{review}```\n",
    "\"\"\"\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02edf787-ef1e-45b6-b6d7-de8d9ac3265c",
   "metadata": {},
   "source": [
    "#### oefening\n",
    "\n",
    "Ga op zoek naar een lijst van reviews (vijf a tiental), maak een prompt die voor elke review een antwoord genereert als JSON object, met enerzijds een SENTIMENT key die 'positive' of 'negative' als antwoord geeft, en daarnaast EMOTIONS die een array bevat met de top drie emoties van de review.\n",
    "Geef daarnaast ook een samenvatting van alle reviews 'mostly positive / mostly negative' al naargelang wat meest van toepassing is. (gegenereerd door een prompt uiteraard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2821b2a219984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:17:33.201974Z",
     "start_time": "2023-10-03T10:17:26.783387Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"review1\": {\n",
      "    \"SENTIMENT\": \"negatief\",\n",
      "    \"EMOTIONS\": [\"frustratie\", \"tevredenheid\", \"voorzichtigheid\"]\n",
      "  },\n",
      "  \"review2\": {\n",
      "    \"SENTIMENT\": \"positief\",\n",
      "    \"EMOTIONS\": [\"tevredenheid\", \"enthousiasme\", \"aanbeveling\"]\n",
      "  },\n",
      "  \"review3\": {\n",
      "    \"SENTIMENT\": \"negatief\",\n",
      "    \"EMOTIONS\": [\"teleurstelling\", \"frustratie\", \"traagheid\"]\n",
      "  },\n",
      "  \"review4\": {\n",
      "    \"SENTIMENT\": \"positief\",\n",
      "    \"EMOTIONS\": [\"tevredenheid\", \"snelheid\", \"gemak\"]\n",
      "  },\n",
      "  \"review5\": {\n",
      "    \"SENTIMENT\": \"positief\",\n",
      "    \"EMOTIONS\": [\"blijdschap\", \"tevredenheid\", \"gemak\"]\n",
      "  }\n",
      "}\n",
      "\n",
      "Samenvatting: mostly positive\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Geef voor volgende reviews een JSON object met een SENTIMENT key (positief of negatief afhankelijk van de review) en EMOTIONS key dat een array bevat met 3 emoties die bij de review past. Geef hiernaast een samenvatting van alle reviews als: \"mostly positive / mostly negative\" afhankelijk van de reviews. \n",
    "\n",
    "review 1: 4/5 stars Ondanks dat de prestaties ERG goed zijn voor creatieve programmas zoals, photoshop/illustrator/aftereffects, is deze laptop NIET geschikt voor gaming. Laat je daarom ook NIET gek maken door de specificaties en gebruik hoogstens alleen LICHTE games op deze laptop. Op het oog is het design/ontwerp echt heel mooi, heel compact en ben ik er echt tevreden mee. Daarentegen is het scherm (ook door eerdere reviews al gezecht) HEEL kwetsbaar voor vingerafdrukken en stof. Haal het stof er gewoon af met een stoffen doekje en niet met je handen. Ook maakt het materiaal het niet best voor het scherm gedeelte. Het krijgt zeer makkelijk beschadigingen, met name bij het gedeelte waar het scherm rond het toetsenbord draait. Wanneer de laptop dicht is zit er een klein openingetje tussen het scherm en het toetsenbord (wat gewoon zo hoort), maar als je deze erg word belast, onder andere door in een te volle tas te doen, word het scherm op het toetsen word gedrukt en blijft deze zo staan en komen er Allerlei beschadigingen rond het draaigedeelte (foto). En leg er ZEKER niks zwaars op! Ook tasten deze beschadigingen het scherm zelf iets aan. Conclusie, Ik zou deze zekerrrrr aanbevelen voor creatieve programmas en uiterst lichte games, maar alsnog, ga er echt voorzichtig mee om, het beschadigd makkelijker dan je denkt. \n",
    "review 2: 5/5 stars Vorige week deze Vivobook van Asus gekocht. Naast dat de laptop een erg mooi design heeft, is hij ook super snel dankzij de i7 processor en 16gb werkgeheugen en de videokaart. Ik bewerk graag foto's en video's en zelfs dat verloopt soepeltjes. Zeker een aanrader wanneer je grafische taken wilt uitvoeren. \n",
    "review 3: 3/5 stars Valt dat ff tegen! Mijn 10 jaar oude computer, (wel met 2 ssd schijven) is binnen 20 seconden volledig opgestart. Daar kan deze niet aan tippen!! Ook niet na het uitschakelen van programma's die dat nog meer vertragen. Eenmaal opgestart is hij wel lekker snel maar ook het uit slaapstand komen is ook weer erg traag. \n",
    "review 4: 5/5 stars Het is een zeer goede laptop met een goede processor. Je kan er wat games op spelen die niet te zwaar zijn. De SSD en de i7 processor combinatie maakt de laptop erg snel. Vingerafdruk scanner is snel en is mooi in het TouchPad verwerkt. Toetsenbord geeft licht. Laptop start binnen 2-5 seconden op. \n",
    "review 5: 4/5 stars Na lang twijfelen heb ik er toch voor gekozen om deze laptop te kiezen en ik ben er zeer blij mee. De laptop heeft een mooi ontwerp en goede specificaties. De game kan ook bepaalde lichte games runnen zoals Minecraft. Voor multitasken en media bewerking is deze laptop ook zeer fijn. De enige echte minpunten die ik kan noemen zijn de hele slechte speakers (geloof me, het is slechter dan je denkt) en de helderheid van het scherm (die valt wel mee). Voor mij is dit verder niet echt een probleem en voor de audio gebruik ik toch altijd een headset. Al met al is het een mooi product.\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac163fc-cb4e-40d2-b079-688b1979f83e",
   "metadata": {},
   "source": [
    "#### principe 4: spelling en stijl\n",
    "\n",
    "LLM's blijken uitstekende spell checkers te zijn, maar zoals altijd, niet blindelings te vertrouwen. Daarnaast blijken LLM's ook goed in een bepaalde stijl te kunnen schrijven, wat erg handig kan zijn als je een tekst formeler wilt laten klinken (of net niet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c51f91f-cf8b-4f94-a3ef-150b4d74644f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:20:44.939281Z",
     "start_time": "2023-10-03T10:20:44.928093Z"
    }
   },
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "Dit onderzoek omvat een casus van Randstad/Tempo team waar er met een dataset \\\n",
    "die netwerkverkeer van alle kantoren bevat, een grondige en correcte analyse \\\n",
    "gaan gemaakd worden. Op basis van die analyse kunnen er te weten gekomen worden \\\n",
    "wat er in een kantoor gebeurt en kunnen er zo conclusies getrokken worden over de werkdruk. \\\n",
    "Het is voor het bedrijf om belangrijk te weten te komen of er al dan niet teveel werknemers \\\n",
    "op een bepaald kantoor werken en zo onnodig bronnen besteed worden waar het niet nodig is en \\\n",
    "dan die bronnen kunnen gebruiken waar wel het nodig is. De aanleiding kwam vanuit de CIO van \\\n",
    "het bedrijf zelf. Hij wou dit omdat men zoveel kantooren over heel het land heeft en dat dit \\\n",
    "niet manueel te controleren valt. Hij wil dit kunnen controleren met data afkomstig van het \\\n",
    "netwerkverkeer om zo een beeld te krijgen van wat er gepresteerd wordt op een kantoor. \\\n",
    "De data die we van het bedrijf krijgen, kunnen we aan de hand van filtering bruikbaar maken voor analyse. \\\n",
    "Uiteindelijk is het de bedoeling om een duidelijk overzicht krijgen met onder andere een proof of concept.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902a2fa-f78b-46aa-8ed6-fd231104e98b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:22:26.659832Z",
     "start_time": "2023-10-03T10:22:11.747161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelfouten:\n",
      "- aangepaste (gemaakd)\n",
      "- voor het bedrijf om belangrijk (voor het bedrijf is het belangrijk)\n",
      "- teveel werknemers (te veel werknemers)\n",
      "- het niet nodig is en dan die (en die)\n",
      "- die bronnen kunnen gebruiken waar wel het nodig is (die bronnen kunnen gebruiken waar het wel nodig is)\n",
      "- zoveel kantooren (zoveel kantoren)\n",
      "\n",
      "Grammaticale fouten:\n",
      "- waar er met een dataset die netwerkverkeer (waarbij een dataset wordt gebruikt die netwerkverkeer)\n",
      "- conclusies getrokken worden over de werkdruk (conclusies worden getrokken over de werkdruk)\n",
      "- te weten gekomen worden wat er in een kantoor gebeurt (te weten gekomen kan worden wat er in een kantoor gebeurt)\n",
      "- besteed worden waar het niet nodig is en dan die bronnen kunnen gebruiken waar wel het nodig is (besteed worden waar het niet nodig is en die bronnen kunnen gebruiken waar het wel nodig is)\n",
      "\n",
      "Fouten tegen vervoegingen:\n",
      "- gaan gemaakd worden (zullen gemaakt worden)\n",
      "- te weten gekomen worden (te weten gekomen worden)\n",
      "- conclusies kunnen getrokken worden (conclusies kunnen worden getrokken)\n",
      "- te controleren valt (te controleren is)\n",
      "- Hij wil dit kunnen controleren (Hij wil dit kunnen controleren)\n",
      "\n",
      "Gecorrigeerde tekst: ```\n",
      "Dit onderzoek omvat een casus van Randstad/Tempo team waarbij er met een dataset die netwerkverkeer van alle kantoren bevat, een grondige en correcte analyse zal worden gemaakt. Op basis van die analyse kan er te weten gekomen worden wat er in een kantoor gebeurt en kunnen er zo conclusies getrokken worden over de werkdruk. Voor het bedrijf is het belangrijk om te weten te komen of er al dan niet te veel werknemers op een bepaald kantoor werken en zo onnodige bronnen besteed worden waar het niet nodig is en die bronnen kunnen gebruiken waar het wel nodig is. De aanleiding kwam vanuit de CIO van het bedrijf zelf. Hij wil dit kunnen controleren met data afkomstig van het netwerkverkeer om zo een beeld te krijgen van wat er gepresteerd wordt op een kantoor. De data die we van het bedrijf krijgen, kunnen we aan de hand van filtering bruikbaar maken voor analyse. Uiteindelijk is het de bedoeling om een duidelijk overzicht te krijgen met onder andere een proof of concept.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identificeer alle schrijffouten in de volgende tekst, afgebakend met triple backticks. \\\n",
    "Lijst ze op per categorie (spelfouten / grammaticale fouten / fouten tegen vervoegingen) en geef ook de gecorrigeerde tekst.\n",
    "\n",
    "tekst: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33000201-bcbc-4b28-9cdc-34105971e5a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:24:50.163932Z",
     "start_time": "2023-10-03T10:24:42.948676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Het voorliggende onderzoek betreft een casestudie van Randstad/Tempo-team, waarbij een grondige en nauwkeurige analyse wordt uitgevoerd van een dataset die het netwerkverkeer van alle kantoren bevat. Op basis van deze analyse is het mogelijk om inzicht te verkrijgen in de activiteiten die plaatsvinden in een kantoor en om conclusies te trekken met betrekking tot de werkdruk. Het is van belang voor het bedrijf om vast te stellen of er mogelijk te veel werknemers werkzaam zijn op bepaalde kantoren, zodat onnodige middelen kunnen worden vermeden en deze middelen efficiënter kunnen worden ingezet op de juiste locaties. De impuls voor dit onderzoek is afkomstig van de CIO van het bedrijf zelf, die van mening is dat handmatige controle van alle kantoren verspreid over het hele land niet haalbaar is. Hij wenst de mogelijkheid te hebben om aan de hand van gegevens afkomstig van het netwerkverkeer een beeld te verkrijgen van de prestaties op een kantoor. Door middel van filtering kunnen we de ontvangen gegevens van het bedrijf geschikt maken voor analyse. Het uiteindelijke doel van dit onderzoek is om een duidelijk overzicht te verschaffen, inclusief een proof of concept.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"\n",
    "Je bent een bachelorstudent aan de hogeschool, herschrijf de hierna volgende introductie \\\n",
    "van een onderzoeksvoorstel, afgebakend door triple backticks. \\\n",
    "Herschrijf enkel de introductie in een meer formele onderzoeksstijl, zonder schrijffouten, \\\n",
    "en doe dit zin per zin zodat de oorspronkelijke structuur behouden blijft.\n",
    "\n",
    "voorstel: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt, False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3707977-042b-4e69-b4b4-862f4f568154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:25:35.958100Z",
     "start_time": "2023-10-03T10:25:26.396844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voorstel:\n",
      "\n",
      "```\n",
      "Dit onderzoek betreft een casestudy van Randstad/Tempo Team, waarbij een grondige en correcte analyse wordt uitgevoerd op een dataset die het netwerkverkeer van alle kantoren omvat. Op basis van deze analyse kan inzicht worden verkregen in de activiteiten op een kantoor, waardoor conclusies kunnen worden getrokken met betrekking tot de werkdruk. Het is voor het bedrijf van belang te weten te komen of er mogelijk te veel werknemers werkzaam zijn op bepaalde kantoren, waardoor onnodige middelen worden besteed op plaatsen waar dit niet noodzakelijk is. Deze middelen kunnen dan beter ingezet worden op de plekken waar dit wel noodzakelijk is. De aanleiding voor dit onderzoek kwam vanuit de Chief Information Officer (CIO) van het bedrijf zelf. Hij heeft behoefte aan een methode om dit te controleren omdat er verspreid over het hele land zoveel kantoren zijn, wat handmatige controle onmogelijk maakt. Hij wil in staat zijn om met behulp van data afkomstig van het netwerkverkeer een beeld te krijgen van de prestaties op een kantoor. De ontvangen data van het bedrijf kan middels filtering geschikt worden gemaakt voor analyse. Uiteindelijk is het doel om een duidelijk overzicht te krijgen, inclusief een proof of concept en andere relevante informatie.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"\n",
    "Je bent een bachelorstudent aan de hogeschool, herschrijf de hierna volgende introductie \\\n",
    "van een onderzoeksvoorstel, afgebakend door triple backticks. \\\n",
    "Herschrijf enkel de introductie in een meer formele onderzoeksstijl, zonder schrijffouten.\n",
    "\n",
    "voorstel: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt, False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ae703-ce7b-40f9-849f-7ff2f3dff5a6",
   "metadata": {},
   "source": [
    "Qua stijl kan je dus makkelijk formeel taalgebruik vragen, maar je kan hier ver in gaan, heel ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ae884-015b-4d3e-9fa5-5c034f9c0c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:26:41.710458Z",
     "start_time": "2023-10-03T10:26:27.651492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Scene: The offices of Randstad/Tempo Team. A group of silly researchers are gathered around a computer, huddled over a dataset of network traffic.\n",
      "\n",
      "Researcher 1: (excitedly) Good morning, everyone! Today, we embark on a tremendous adventure in the land of data analysis!\n",
      "\n",
      "Researcher 2: (curiously) What are we analyzing this time?\n",
      "\n",
      "Researcher 1: (grinning) Oh, you won't believe it! We have been entrusted with the task of unraveling the mysteries of the network traffic in all the offices of Randstad/Tempo Team!\n",
      "\n",
      "Researcher 3: (sarcastically) Oh joy, traffic analysis. How riveting!\n",
      "\n",
      "Researcher 1: (ignoring the sarcasm) But wait, my friends, this is not just any data analysis. With our sharp minds and impeccable algorithms, we shall uncover the secrets of what truly happens in these offices. And more importantly, we shall determine the workload!\n",
      "\n",
      "Researcher 4: (intrigued) The workload, you say? How on earth can we do that?\n",
      "\n",
      "Researcher 1: Ah, my dear colleague, fear not! Our trusty dataset shall guide us and reveal the truth. By analyzing the network traffic, we shall discern the number of employees present in each office and assess whether there are too many or, dare I say it, not enough.\n",
      "\n",
      "Researcher 5: (amused) So, you mean to tell me that we will be the heroes who save Randstad/Tempo Team from unnecessary resource wastage?\n",
      "\n",
      "Researcher 1: Precisely! The Chief Information Officer (CIO) himself had the epiphany that manual control of all these offices is an arduous task. Therefore, he yearns for the power of data, of network traffic, to grant him a glimpse into the productivity of each office.\n",
      "\n",
      "Researcher 6: (pondering) But how will we make sense of this chaotic data? Won't it be a mess?\n",
      "\n",
      "Researcher 1: (smirking) Your concerns are valid, my friend, but fret not! We shall filter and transform the data provided by the company, making it suitable for our analysis. And in the end, we shall present a clear and concise overview of our findings, including a dazzling proof of concept!\n",
      "\n",
      "Researcher 3: (mocking) Ah, a proof of concept! Because what else could be more thrilling than that?\n",
      "\n",
      "Researcher 1: (enthusiastically) Oh, my dear colleague, once we uncover the secrets of these offices, we shall be hailed as the heroes of optimization! We shall save resources where they are not needed and allocate them where they truly matter!\n",
      "\n",
      "(They all laugh manically and get to work, ready to delve into the world of network traffic analysis.)\n",
      "\n",
      "End scene.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Je bent een bachelorstudent aan de hogeschool, herschrijf de hierna volgende introductie \\\n",
    "van een onderzoeksvoorstel, afgebakend door triple backticks. \\\n",
    "Herschrijf enkel de introductie als monty python sketch.\n",
    "\n",
    "voorstel: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt, False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203b3d1-083d-48f6-879e-d8fc84efc3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:27:13.785999Z",
     "start_time": "2023-10-03T10:27:02.825830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voorstel: ```\n",
      "Dit onderzoek gaat over een smurfse casus van Randstad/Tempo team waar er smurfgehoopt wordt om met een smurftastische dataset die smurfig netwerkverkeer van alle smurfkantoren bevat, een grondige en smurfcorrecte analyse te smurfen. Op basis van die smurflistige analyse kunnen er smurfwijze conclusies worden getrokken over de smurfwerkdruk en wat er allemaal smurfgebeurt in een kantoor. Het is voor het smurfbedrijf heel belangrijk om te weten te smurfen of er al dan niet teveel smurfen op een bepaald kantoor werken, zodat ze geen smurfbelangrijke smurfmiddelen versmurfen waar het niet nodig is, maar juist smurfbestemmen waar het wel nodig is. De smurfaanleiding voor dit onderzoek komt vanuit de smurfCIO van het smurfbedrijf zelf. Hij wilde dit omdat er zoveel smurfkantoren verspreid zijn over het hele smurfland, en dat het smurfcontroleren hiervan met de hand onsmurfelijk is. Hij wil deze smurfcontrole kunnen uitvoeren met data die afkomstig is van het smurfnetwerkverkeer, zodat hij een smurfbeeld kan vormen van wat er smurfgerealiseerd wordt op een kantoor. De smurfdata die we van het bedrijf krijgen, kunnen we smurfgeschikt maken voor analyse door middel van smurffiltering. Uiteindelijk smurfdoel is om een smurfduidelijk overzicht te verkrijgen, inclusief een smurfproof of concept.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Je bent een bachelorstudent aan de hogeschool, herschrijf de hierna volgende introductie \\\n",
    "van een onderzoeksvoorstel, afgebakend door triple backticks. \\\n",
    "Herschrijf enkel de introductie in de smurfentaal.\n",
    "\n",
    "voorstel: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt, False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc103eaf-8a67-457b-b9e8-87981917ff11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:27:34.447737Z",
     "start_time": "2023-10-03T10:27:32.839597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Er was eens een onderzoek in Randstad,\n",
      "Met data van elke kantoortje gegrond.\n",
      "We willen weten,\n",
      "Hoe werkdruk te meten.\n",
      "En zo onnodige kosten besparen, blond.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Je bent een bachelorstudent aan de hogeschool, herschrijf de hierna volgende introductie \\\n",
    "van een onderzoeksvoorstel, afgebakend door triple backticks. \\\n",
    "Herschrijf enkel de introductie als limerick.\n",
    "\n",
    "voorstel: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt, False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cc34d6-1b97-4ce0-ba6f-75c3a2165d60",
   "metadata": {},
   "source": [
    "Afhankelijk van wie jullie promoter is, misschien een ideetje voor de bachelorproef?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc2ba9-bae2-49b6-a111-b7a70d1f424c",
   "metadata": {},
   "source": [
    "#### principe 5: uitwerken\n",
    "\n",
    "LLM's kunnen niet enkel samenvatten, ook uitweiden is een optie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8cdbe-4f9b-4f18-8e19-89c079948bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:28:29.045031Z",
     "start_time": "2023-10-03T10:28:29.031793Z"
    }
   },
   "outputs": [],
   "source": [
    "gevoel = \"negatief\"\n",
    "inhoud = \"tweede les trends in ai, weinig nieuws geleerd, veel te langdradig, te weinig oefeningen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951bc66-0a14-4970-89dc-5837db3a2b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:28:45.970222Z",
     "start_time": "2023-10-03T10:28:36.835066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste docent,\n",
      "\n",
      "Ik hoop dat deze email u goed bereikt. Ik wil graag mijn feedback delen over de tweede les trends in AI. Ten eerste wil ik benadrukken dat ik het belangrijk vind om constructieve kritiek te geven, want ik geloof dat dit kan bijdragen aan een betere leerervaring voor ons allemaal.\n",
      "\n",
      "Helaas moet ik zeggen dat ik niet erg tevreden was met de tweede les. Ik had gehoopt veel nieuwe dingen te leren, maar helaas bleek dit niet het geval te zijn. Ik vond de les te langdradig en ik merkte dat ik hierdoor mijn concentratie verloor. Daarnaast vond ik dat er te weinig oefeningen werden gegeven, waardoor ik het gevoel had dat ik de stof niet goed kon toepassen.\n",
      "\n",
      "Ik begrijp dat het onderwerp trends in AI uitgebreid is, maar ik denk dat het mogelijk is om de les meer interactief en boeiend te maken. Wellicht kunnen er meer praktijkvoorbeelden worden gegeven en kunnen we actief deelnemen aan discussies om de leerervaring te versterken.\n",
      "\n",
      "Ik wil graag benadrukken dat ik de lessen oprecht waardeer en waardeer dat u de tijd en moeite neemt om ons te onderwijzen. Mijn opmerkingen zijn bedoeld als constructieve feedback om de lessen te verbeteren. Ik kijk uit naar de volgende les en hoop op een meer boeiende en interactieve leerervaring.\n",
      "\n",
      "Met vriendelijke groet,\n",
      "[Je naam]\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Stel een beleefde email op, zowel het gevoel als de korte inhoud worden hierna tussen triple backticks gegeven.\n",
    "\n",
    "\n",
    "gevoel: ```{gevoel}```\n",
    "inhoud: ```{inhoud}```\n",
    "\"\"\"\n",
    "\n",
    "mail = get_answer(prompt, False)\n",
    "print(mail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd561f1b-cbde-4497-bc36-5be73efb54f6",
   "metadata": {},
   "source": [
    "En als docent lezen we de mail niet meer natuurlijk, maar doen we net het omgekeerde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5df91e-1aa1-4198-9719-ab66dd92c36d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T10:29:18.558435Z",
     "start_time": "2023-10-03T10:29:14.753672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Feedback over de tweede les trends in AI\n",
      "- Belang van constructieve kritiek\n",
      "- Ongenoegen over de les\n",
      "- Les was te langdradig en concentratie werd verloren\n",
      "- Gebrek aan oefeningen om de stof toe te passen\n",
      "- Suggestie om de les interactiever en boeiender te maken met praktijkvoorbeelden en discussies\n",
      "- Waardering voor de lessen en de tijd en moeite van de docent\n",
      "- Constructieve feedback om de lessen te verbeteren\n",
      "- Uitkijken naar de volgende les voor een meer boeiende en interactieve leerervaring\n",
      "\n",
      "Algemeen gevoel van de mail: negatief\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Vat de mail tussen triple backticks samen in een aantal bulletpoints, geef ook het algemene gevoel van de mail als 'positief', 'negatief' of 'neutraal'\n",
    "\n",
    "mail: ```{mail}```\n",
    "\"\"\"\n",
    "\n",
    "res = get_answer(prompt, False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490e90e-28d9-4798-a0e8-2a5c8930c30b",
   "metadata": {},
   "source": [
    "In dit geval toch al een upgrade van negatief naar neutraal, ChatGPT staat aan de zijde van de docent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43127f56f56ab44d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### oefening\n",
    "\n",
    "Creëer een prompt die op een heel positieve manier suggesties geeft over hoe de verdere cursus 'Trends in AI' kan verbeterd worden.\n",
    "(en als er iets goed uitkomt, mail gerust 😉)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trends_in_AI_development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
