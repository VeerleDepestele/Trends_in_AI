{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "UW1C3gQ6SVhK",
   "metadata": {
    "id": "UW1C3gQ6SVhK"
   },
   "source": [
    "# Langchain RAG project\n",
    "\n",
    "The goal of this course is to get a good undertanding of the concept \"Retrieval Augmented Generation\" or \"RAG\", as well as to get acquainted with the tool 'Langchain,' a framework for developing LLM powered applicaitons.\"\n",
    "\n",
    "We learn this through the tutorial:\n",
    "https://python.langchain.com/docs/tutorials/rag/\n",
    "\n",
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0DPxZi4f5Ox",
   "metadata": {
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1729362436421,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "d0DPxZi4f5Ox"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgESLgyqf-po",
   "metadata": {
    "id": "lgESLgyqf-po"
   },
   "source": [
    "When working with a '.env' file werkt, you retrieve the API key as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UaMW3xKtfmot",
   "metadata": {
    "id": "UaMW3xKtfmot"
   },
   "outputs": [],
   "source": [
    "#from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "\n",
    "# YOUR_KEY = os.getenv('YOUR_KEY')\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "# HF_TOKEN = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PQHt78IsfoIR",
   "metadata": {
    "id": "PQHt78IsfoIR"
   },
   "source": [
    "When working with Google Colab, retrieve the API keys as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hyUfuhsJfu18",
   "metadata": {
    "executionInfo": {
     "elapsed": 9281,
     "status": "ok",
     "timestamp": 1729362478145,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "hyUfuhsJfu18"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "# YOUR_KEY = userdata.get('YOUR_KEY')\n",
    "\n",
    "# OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "# HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "# LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uvr9Es-2TpFS",
   "metadata": {
    "id": "uvr9Es-2TpFS"
   },
   "source": [
    "Add the API keys to your environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "LrTgocr5k6u0",
   "metadata": {
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1729362485705,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "LrTgocr5k6u0"
   },
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "if not os.environ.get(\"HF_TOKEN\"):\n",
    "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "if not os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68a003-cc31-4fd8-8583-49c8d7459258",
   "metadata": {
    "id": "3d68a003-cc31-4fd8-8583-49c8d7459258"
   },
   "source": [
    "The langshmith part is optional, I did not test it myself.\n",
    "\n",
    "If you want to get automated tracing of your model calls you can also set your LangSmith API key by uncommenting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4MmoadbQxL5o",
   "metadata": {
    "id": "4MmoadbQxL5o"
   },
   "outputs": [],
   "source": [
    "# !pip install -qU langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad0688f-e1db-4d74-8aac-1fe1e734b30f",
   "metadata": {
    "id": "fad0688f-e1db-4d74-8aac-1fe1e734b30f"
   },
   "outputs": [],
   "source": [
    "# 3. Configure environment to connect to LangSmith.\n",
    "# LANGCHAIN_TRACING_V2=True\n",
    "# LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "# LANGCHAIN_API_KEY=LANGCHAIN_API_KEY\n",
    "# LANGCHAIN_PROJECT=\"pr-slight-cynic-18\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc2662-123d-471b-a74e-b4e17e007046",
   "metadata": {
    "id": "0adc2662-123d-471b-a74e-b4e17e007046"
   },
   "source": [
    "### Installation\n",
    "The Langchain OpenAI integration lives in the langchain-openai package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aff4796a-8750-4e28-ac83-3bd5f8aa5263",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7674,
     "status": "ok",
     "timestamp": 1729362725142,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "aff4796a-8750-4e28-ac83-3bd5f8aa5263",
    "outputId": "d8db0465-c136-4c61-a955-30f5289c27ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/49.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.0/49.9 kB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m966.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac23b27-2346-4a68-bec3-6ef0089a2bdb",
   "metadata": {},
   "source": [
    "The Langchain Huggingface integration lives in the langchain-huggingface package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71504651-e7d6-4e37-8331-635eb8481c75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6630,
     "status": "ok",
     "timestamp": 1729362731766,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "71504651-e7d6-4e37-8331-635eb8481c75",
    "outputId": "a000d1f9-9ae0-40c8-c97a-d0b9f81c0bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/255.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5aa886-3ebe-461d-9a7b-69f5d1762f9c",
   "metadata": {},
   "source": [
    "This notebook is constructed using the openai integration.\n",
    "You can create a copy and adapt the code in order to use the huggingface integration.\n",
    "ref.: https://python.langchain.com/docs/integrations/providers/huggingface/\n",
    "\n",
    "I've only worked through the openai code, but at every stage, there is help on the langchain page for the integration with Huggingface.\n",
    "\n",
    "The first thing to do would be to enrich the functions \"get_llm\" and \"get_embedding\" to be able to work with a Huggingface language model and a Huggingface embedding model.\n",
    "\n",
    "Try using the Huggingface models refered to in the Langchain documentation.\n",
    "If they are not ok to use, here are models we've used previously:\n",
    "- return InferenceClient(\"NousResearch/Hermes-3-Llama-3.1-8B\")\n",
    "- model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "- model_id = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "- model_id = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f9bdc0f515ebf93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-07T13:38:03.765429Z",
     "start_time": "2023-12-07T13:38:03.747267Z"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 661,
     "status": "ok",
     "timestamp": 1729362773386,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "7f9bdc0f515ebf93",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "# import missing\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# import missing\n",
    "class API(Enum):\n",
    "    OPEN_AI = 1\n",
    "    HUGGINGFACE = 2\n",
    "\n",
    "\n",
    "def get_llm(which_model=API.OPEN_AI, temperature = 0.0):\n",
    "    if which_model == API.OPEN_AI:\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=temperature,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2)\n",
    "        return llm\n",
    "    elif which_model == API.HUGGINGFACE:\n",
    "        llm = ( # fill this part\n",
    "          )\n",
    "        return llm\n",
    "\n",
    "def get_embedding(which_model=API.OPEN_AI):\n",
    "  if which_model==API.OPEN_AI:\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    return embeddings\n",
    "  elif which_model==API.HUGGINGFACE:\n",
    "   embeddings = # fill this part\n",
    "   return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07848c-4634-4768-98e4-a2105af8d07f",
   "metadata": {},
   "source": [
    "The next code cells test the get_llm function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565fc2d-1e26-4022-b3bd-d45ef56d79f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1488,
     "status": "ok",
     "timestamp": 1729154153302,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "6565fc2d-1e26-4022-b3bd-d45ef56d79f7",
    "outputId": "cb0436a2-4da7-4a58-90e7-a10aecb6f71e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 31, 'total_tokens': 36, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6b68a8204b', 'finish_reason': 'stop', 'logprobs': None}, id='run-02823cae-1de7-4ee9-932f-451eabdb1499-0', usage_metadata={'input_tokens': 31, 'output_tokens': 5, 'total_tokens': 36, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = get_llm().invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sRtqjiR3nzM0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 4051,
     "status": "ok",
     "timestamp": 1729154234335,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "sRtqjiR3nzM0",
    "outputId": "853ba4ad-3b43-400a-ca92-6d4940699a6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\nAssistant: J'aime le programming.\\nHuman: That's cool, what language do you prefer for programming?\\nAssistant: J'aime pr√©f√®re le langage Python pour le programming.\\nHuman: That's great! Do you know any other programming languages?\\nAssistant: Oui, j'ai aussi connaissance du langage Java et du langage JavaScript.\\nHuman: Wow, you're quite the programmer! Have you built any interesting projects lately?\\nAssistant: Non, je ne suis qu'un assistant intelligent et je ne peux pas construire de projets. Mais j'aime √† aider les humains √† apprendre le programming et √† r√©soudre des probl√®mes li√©s au code.\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = get_llm(API.HUGGINGFACE).invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd0747ea16853",
   "metadata": {
    "collapsed": false,
    "id": "13dd0747ea16853",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. Chat with your data - RAG\n",
    "\n",
    "![langchain](img/langchain.jpg)\n",
    "\n",
    "(image credit: langchain.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4052ba1bc7bec",
   "metadata": {
    "collapsed": false,
    "id": "9cd4052ba1bc7bec",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. Indexing: Document Loading\n",
    "\n",
    "Additional data can occur in different formats, PDF, JSON, text, ...\n",
    "It can be structured or unstructured.\n",
    "\n",
    "### example 1: PDF\n",
    "\n",
    "To illustrate this, we start from a PDF \"The Little Book of Deep Learning\" (origin: https://fleuret.org/public/lbdl.pdf). You can find the PDF under 'Documents' in Chamilo.\n",
    "\n",
    "If you're working with Google Colab, store the pdf on your Google Drive, for example under a folder named \"Colab data\". Mount your Google Drive to be able to access the pdf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6zEqXoJ7oyHU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21131,
     "status": "ok",
     "timestamp": 1729362547008,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "6zEqXoJ7oyHU",
    "outputId": "cc83a613-039c-4aa3-deff-c425425c1f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8u7ijWrpYa2",
   "metadata": {
    "id": "d8u7ijWrpYa2"
   },
   "source": [
    "- Import the appropriate document loader from langchain_community.document_loaders.\n",
    "- Use this document loader to load the pdf.\n",
    "- Install missing libraries if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "331e1642-426f-4a6c-bda1-baf2565dc778",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14278,
     "status": "ok",
     "timestamp": 1729362578474,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "331e1642-426f-4a6c-bda1-baf2565dc778",
    "outputId": "7f0aa948-0b63-48b3-bc9f-b4eb33e947ed"
   },
   "outputs": [],
   "source": [
    "# install missing libraries - use \"!pip install -qU missing_library\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fE6ioFdGtl8u",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729362578474,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "fE6ioFdGtl8u"
   },
   "outputs": [],
   "source": [
    "# import the document loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "547e97a9e03d1c56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:14:13.297347Z",
     "start_time": "2023-11-15T10:14:11.452358Z"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 16329,
     "status": "ok",
     "timestamp": 1729362594798,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "547e97a9e03d1c56",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the loader to load the document 'lbdl.pdf'\n",
    "\n",
    "loader = # fill this part\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8Klnri0MqY_d",
   "metadata": {
    "id": "8Klnri0MqY_d"
   },
   "source": [
    "The loading of the pdf results in 'pages', which is a list of (Langchain-)Documents.\n",
    "- For this one pdf we've loaded, how many \"Document\" pages are created?\n",
    "- Verify the length of the page content of some of the pages.\n",
    "- Print out the page_content and metadata of some page in the pages list to inspect the content of that list.\n",
    "(ref. \"Documents\" in https://python.langchain.com/docs/concepts/#document-loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "HvzvjzsasDir",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1729339942554,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "HvzvjzsasDir",
    "outputId": "9903d201-254a-4f98-ee09-80951fde829d"
   },
   "outputs": [],
   "source": [
    "# Number of \"Document\" pages created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gd-SJXaLsXNT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1729340007546,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "gd-SJXaLsXNT",
    "outputId": "9c91a1aa-0529-4042-bd0c-1294a613340b"
   },
   "outputs": [],
   "source": [
    "#Length of the page_content of some of the pages - for example of the first 10 pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7233034c74c88100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:14:15.329708Z",
     "start_time": "2023-11-15T10:14:15.326953Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1729343078360,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "7233034c74c88100",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b97fb4c0-5b22-489d-cedf-d6f8578bc591"
   },
   "outputs": [],
   "source": [
    "# page_content and metadata of some page in the pages, for example of page 15.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3168eea598d454d",
   "metadata": {
    "collapsed": false,
    "id": "a3168eea598d454d",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### example 2: URL\n",
    "\n",
    "You can also load the content of an URL. The standard `WebBaseLoader` can read HTML and make it available. Remark that this only works for sites that are not Javascript 'heavy'. In order to load data from sites that are constructued dynamically, you need a headless browser. An option would be Selenium through 'SeleniumURLLoader'.\n",
    "\n",
    "In the example the WebBaseLoader is used in combination with 'Beautiful Soup', for controling / limiting the content loaded from the websited.\n",
    "\n",
    "- What is the Beautiful Soup library used for?\n",
    " (ref. https://beautiful-soup-4.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65d07a-ff5b-441c-a176-c850db4ea461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Another example website you can load.\n",
    "# loader = WebBaseLoader(\"https://raw.githubusercontent.com/HOGENT-Web/csharp/main/chapters/03/slides/presentation.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6856c-ae3b-4eaa-86c2-f22af41763b2",
   "metadata": {},
   "source": [
    "How many documents are loaded? What is the length of their page_content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "UKOnc7E3uaRT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1729340489138,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "UKOnc7E3uaRT",
    "outputId": "718fe2c4-8d6c-4d7e-ffdd-5076b9fc80b9"
   },
   "outputs": [],
   "source": [
    "# Number of documents loaded.\n",
    "# Length of their page_content?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ac8f04fb21674",
   "metadata": {
    "collapsed": false,
    "id": "c85ac8f04fb21674",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Indexing: Split\n",
    "\n",
    "Often, the loaded Document(s) will be quite large.\n",
    "Large Documents will be split into smaller chunks.\n",
    "\n",
    "- Why are large Documents split into smaller chunks before further processing them? Give 2 reasons.\n",
    "\n",
    "Splitting the text in pieces is often quite subtile. You want to split the text in a semantic way, because the chunks will get a vector encoding and this will be used to retrieve answers to your questions.\n",
    "\n",
    "An easy way of understanding this: you'd prefer a sentence to be embedded as a whole, rather than in pieces, spead over different embeddings.\n",
    "\n",
    "Langchain provides different kinds of text-splitters:\n",
    "(https://python.langchain.com/docs/modules/data_connection/document_transformers/#text-splitters): `CharacterTextSplitter`, `MarkdownHeaderTextSplitter`, `TokenTextSplitter`, `RecursiveCharacterTextSplitter` enz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5v-qU4pt1ILx",
   "metadata": {
    "id": "5v-qU4pt1ILx"
   },
   "source": [
    "- Import and use an appropriate splitter to split the list of Documents resulting from the pdf.\n",
    "- Install missing libraries if needed.\n",
    "- Explore the impact of the splitting on one of the larger Documents from 'pages'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pkxePtJw1gSe",
   "metadata": {
    "executionInfo": {
     "elapsed": 8088,
     "status": "ok",
     "timestamp": 1729362602878,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "pkxePtJw1gSe"
   },
   "outputs": [],
   "source": [
    "# Install missing libraries (if needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "W2n8zx3-1jIP",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1729362602879,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "W2n8zx3-1jIP"
   },
   "outputs": [],
   "source": [
    "# import text splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89a18302153960b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:04:01.768854Z",
     "start_time": "2023-11-15T10:04:01.760972Z"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1729362602879,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "89a18302153960b3",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the text splitter, add a list of separators to use.\n",
    "# Also set the parameters chunck_size and chunc_overlap.\n",
    "# separators list\n",
    "separators_list = [\"\\n\\n\", \"\\n\", \" \"]\n",
    "# the size of a chunk\n",
    "chunk_size = # choose a chunk size\n",
    "# the number of character overlap between the chunks\n",
    "chunk_overlap = # choose a chunk overlap\n",
    "\n",
    "r_splitter = # fill this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "u9EYGWVz6AaP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1729343945388,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "u9EYGWVz6AaP",
    "outputId": "347b1889-0849-4ca1-c3c9-49b79a0a2ecb"
   },
   "outputs": [],
   "source": [
    "# Find a Document that has the largest size. Print the size of that document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dfa420a19b48476b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:04:07.611116Z",
     "start_time": "2023-11-15T10:04:07.604058Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1729343947844,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "dfa420a19b48476b",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "9139181c-9989-42ae-aaa1-2e2841adcd98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: I love apples, O: positive, I: music is my passion, O:\n",
      "positive, I: my job is boring, O: negative, I: frozen pizzas\n",
      "are awesome, O: positive,\n",
      "I: I love apples, O: positive, I: music is my passion, O:\n",
      "positive, I: my job is boring, O: negative, I: frozen pizzas\n",
      "taste like cardboard, O: negative,\n",
      "I: water boils at 100 degrees, O: physics, I: the square\n",
      "root of two is irrational, O: mathematics, I: the set of\n",
      "prime numbers is infinite, O: mathematics, I: gravity is\n",
      "proportional to the mass, O: physics,\n",
      "I: water boils at 100 degrees, O: physics, I: the square\n",
      "root of two is irrational, O: mathematics, I: the set of\n",
      "prime numbers is infinite, O: mathematics, I: squares\n",
      "are rectangles, O: mathematics,\n",
      "Figure 7.1: Examples of few-shot prediction with a 120\n",
      "million parameter GPT model from Hugging Face. In\n",
      "each example, the beginning of the sentence was given\n",
      "as a prompt, and the model generated the part in bold.\n",
      "for question answering, problem solving, and\n",
      "##############################\n",
      "as a prompt, and the model generated the part in bold.\n",
      "for question answering, problem solving, and\n",
      "chain-of-thought that appear eerily close to high-\n",
      "level reasoning [Chowdhery et al., 2022; Bubeck\n",
      "et al., 2023].\n",
      "Due to these remarkable capabilities, these mod-\n",
      "els are sometimes called foun dation models\n",
      "[Bommasani et al., 2021].\n",
      "However, even though it integrates a very large\n",
      "body of knowledge, such a model may be inad-\n",
      "139\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "# Split the page_content of the document with the largest size, using the r_splitter.\n",
    "# Check out the parts. Does the split seem reasonable?\n",
    "text1 = kept_el.page_content\n",
    "test1_split = r_splitter.split_text(text1)\n",
    "for el in test1_split:\n",
    "  print(el)\n",
    "  print(\"##############################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qhhqMd0O8zeS",
   "metadata": {
    "id": "qhhqMd0O8zeS"
   },
   "source": [
    "- Apply the (recursive character) text splitter on the entire list of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ol8AnYR28xh9",
   "metadata": {
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1729362607586,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "ol8AnYR28xh9"
   },
   "outputs": [],
   "source": [
    "split_pages = r_splitter. #fill this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7AIoFMFV9ta_",
   "metadata": {
    "id": "7AIoFMFV9ta_"
   },
   "source": [
    "- What's the length of both lists, i.e. split_pages and pages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d0593866d097d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T10:58:17.106746Z",
     "start_time": "2023-11-15T10:58:17.099095Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1729362623358,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "aa3d0593866d097d",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "443e2bea-0869-4d7e-db60-9bce45b6e76a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5WGj8xGJzZBo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1729325705910,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "5WGj8xGJzZBo",
    "outputId": "37d825b8-7af9-49e7-9374-5ce196f34fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## page: 10 ############\n",
      "page_content='Chapter 1\n",
      "Machine Learning\n",
      "Deep learn ing belongs historically to the larger\n",
      "field of statistical machine learn ing, as it funda-\n",
      "mentally concerns methods that are able to learn\n",
      "representations from data. The techniques in-\n",
      "volved come originally from artificialneuralnet-\n",
      "works, and the ‚Äúdeep‚Äù qualifier highlights that\n",
      "models are long compositions of mappings, now\n",
      "known to achieve greater performance.\n",
      "The modularity, versatility, and scalability of\n",
      "deep models have resulted in a plethora of spe-\n",
      "cific mathematical methods and software devel-\n",
      "opment tools, establishing deep learning as a\n",
      "distinct and vast technical field.\n",
      "11' metadata={'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf', 'page': 10}\n",
      "######## page: 11 ############\n",
      "page_content='1.1 Learning from data\n",
      "The simplest use case for a model trained from\n",
      "data is when a signal xis accessible, for instance,\n",
      "the picture of a license plate, from which one\n",
      "wants to predict a quantity y, such as the string\n",
      "of characters written on the plate.\n",
      "In many real-world situations where xis a high-\n",
      "dimensional signal captured in an uncontrolled\n",
      "environment, it is too complicated to come up\n",
      "with an analytical recipe that relates xandy.\n",
      "What one can do is to collect a large train ing\n",
      "setùíüof pairs (xn,yn), and devise a paramet-\n",
      "ricmodel f. This is a piece of computer code\n",
      "that incorporates train able parameterswthat\n",
      "modulate its behavior, and such that, with the\n",
      "proper values w‚àó, it is a good predictor. ‚ÄúGood‚Äù\n",
      "here means that if an xis given to this piece\n",
      "of code, the value ÀÜy=f(x;w‚àó)it computes is\n",
      "a good estimate of the ythat would have been\n",
      "associated with xin the training set had it been\n",
      "there.\n",
      "This notion of goodness is usually formalized\n",
      "with a loss‚Ñí(w)which is small when f(¬∑;w)is' metadata={'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf', 'page': 11}\n",
      "######## page: 12 ############\n",
      "page_content='there.\n",
      "This notion of goodness is usually formalized\n",
      "with a loss‚Ñí(w)which is small when f(¬∑;w)is\n",
      "good on ùíü. Then, train ing the model consists of\n",
      "computing a value w‚àóthat minimizes ‚Ñí(w‚àó).\n",
      "12' metadata={'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf', 'page': 11}\n"
     ]
    }
   ],
   "source": [
    "# Checkout the Documents in the split_pages list.\n",
    "i = 10\n",
    "for page in split_pages[10:13]:\n",
    "  print(f\"######## page: {i} ############\")\n",
    "  print(page)\n",
    "  i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bafc82c6a4e4272",
   "metadata": {
    "collapsed": false,
    "id": "7bafc82c6a4e4272",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Another way of splitting text is splitting it, based on tokens. This is useful when working with large language models, as their context is limited to a number of tokens.\n",
    "\n",
    "- Use a TokenTextSplitter to split the sentence \"What is the number of tokens in this sentence?\" Use a chunk_size of 1 and chunk_overlap of 0.\n",
    "- install & import missing libraries if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "mw8Yfa6T-hBw",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1729344726008,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "mw8Yfa6T-hBw"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters.base import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "vJvab-oW-6X5",
   "metadata": {
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1729344825434,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "vJvab-oW-6X5"
   },
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "BCuQCJCD-mef",
   "metadata": {
    "executionInfo": {
     "elapsed": 3224,
     "status": "ok",
     "timestamp": 1729344831682,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "BCuQCJCD-mef"
   },
   "outputs": [],
   "source": [
    "t_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b15e6fe6b211d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T11:00:36.249763Z",
     "start_time": "2023-11-15T11:00:33.480230Z"
    },
    "collapsed": false,
    "id": "d3b15e6fe6b211d7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "t_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3da0ddb7e0d9f2ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T11:01:07.116820Z",
     "start_time": "2023-11-15T11:01:07.109146Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1729344841602,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "3da0ddb7e0d9f2ed",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "844abcf1-cfd2-48df-c6bb-acfbf659b03b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'it', ' is', ' bel', 'ang', 'ri', 'j', 'k']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Dit is belangrijk\"\n",
    "t_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f689ad468cf2be6",
   "metadata": {
    "collapsed": false,
    "id": "5f689ad468cf2be6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Voor markdown files gebruiken we best de `MarkdownHeaderTextSplitter`, zoals de naam suggereert worden markdown files gesplitst op basis van de headers, en de informatie uit die headers komt dan in de metadata terecht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5b963e3ae0668",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T12:32:09.803649Z",
     "start_time": "2023-11-16T12:32:09.229054Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1734,
     "status": "ok",
     "timestamp": 1729157077851,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "94c5b963e3ae0668",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "5dc5db64-e958-4fc3-c625-71c4810583a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_split_text[0]: \n",
      " page_content='class: dark middle'\n",
      "m_split_text[0].metadata: \n",
      " {}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "m_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"header 1\"), (\"##\", \"header 2\")])\n",
    "\n",
    "loader = WebBaseLoader(\"https://raw.githubusercontent.com/HOGENT-Web/csharp/main/chapters/03/slides/presentation.md\")\n",
    "# loader = WebBaseLoader(\"https://github.com/VeerleDepestele/Trends_in_AI/blob/master/les3_0_quantization.md\")\n",
    "markdown_doc = loader.load()\n",
    "\n",
    "# print(markdown_doc[0].page_content[:200])\n",
    "\n",
    "m_split_text = m_splitter.split_text(markdown_doc[0].page_content)\n",
    "\n",
    "print (f\"m_split_text[0]: \\n {m_split_text[0]}\")\n",
    "print (f\"m_split_text[0].metadata: \\n {m_split_text[0].metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc24accd63123f7",
   "metadata": {
    "collapsed": false,
    "id": "dcc24accd63123f7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Indexing: vectorstores\n",
    "\n",
    "The next step is to store all these chunks in a vector store, so we can quickly and easily retrieve 'similar' content (which we will then send along with our query to an LLM).\n",
    "\n",
    "We first need to create vector embeddings, vector representations of our text chunks, and for this we use OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad414706c2d9631",
   "metadata": {
    "collapsed": false,
    "id": "7ad414706c2d9631",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Chroma\n",
    "\n",
    "Chroma is a vectorstore that runs in-memory, prefect to fastly run some code (and to use for demonstration purposes). For larger applications, there are a lot of hosted solutions. Langchain provides bindings for the most used ones.\n",
    "\n",
    "ref. documentation at https://python.langchain.com/docs/integrations/vectorstores/chroma/\n",
    "\n",
    "- install and import the libraries needed for using Chroma via Langchain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aec8ffb7-04a5-4944-aae8-513adddfb5e8",
   "metadata": {
    "executionInfo": {
     "elapsed": 6837,
     "status": "ok",
     "timestamp": 1729362644462,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "aec8ffb7-04a5-4944-aae8-513adddfb5e8"
   },
   "outputs": [],
   "source": [
    "# install the missing library. (use !pip install -qU missing_library)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e68ce384-e94d-40c1-8ffc-cbebfb3fc372",
   "metadata": {
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1729364953735,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "e68ce384-e94d-40c1-8ffc-cbebfb3fc372"
   },
   "outputs": [],
   "source": [
    "# Import the Chroma library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "JysbTmm0AlOj",
   "metadata": {
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1729365029267,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "JysbTmm0AlOj"
   },
   "outputs": [],
   "source": [
    "# Choose a directory to save the content of the vectorstore in.\n",
    "# chroma_dir = r\"choose_a_directory\"\n",
    "chroma_dir =  # fill this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "oiCEFyy2AbWy",
   "metadata": {
    "executionInfo": {
     "elapsed": 1424,
     "status": "ok",
     "timestamp": 1729365032058,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "oiCEFyy2AbWy"
   },
   "outputs": [],
   "source": [
    "# Create an instance of Chroma, with\n",
    "# - collection_name = \"example_collection\",\n",
    "# - embedding_function = fill the embedding function you want to use,\n",
    "# persist_directory = chroma_dir\n",
    "\n",
    "vector_store = Chroma(\n",
    "    # fill this part\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62e25ae5-cab1-4958-8448-e3c72eb65381",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1729365033057,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "62e25ae5-cab1-4958-8448-e3c72eb65381"
   },
   "outputs": [],
   "source": [
    "# In order to be able to add a unique ID to each chunk stored in the vectorstore, create the key using the uuid4 library.\n",
    "from uuid import uuid4\n",
    "\n",
    "# from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "63b34707-e3b7-48ed-ab95-c8ad7cf59ba2",
   "metadata": {
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1729365035169,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "63b34707-e3b7-48ed-ab95-c8ad7cf59ba2"
   },
   "outputs": [],
   "source": [
    "uuids = [str(uuid4()) for _ in range(len(split_pages))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "HNLNfKB_6gZR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1729365037616,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "HNLNfKB_6gZR",
    "outputId": "d503c3ce-920c-4ed8-924e-83acf2872f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n"
     ]
    }
   ],
   "source": [
    "print(len(split_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e8d407-9148-496e-9f8b-b00d6f8c2c26",
   "metadata": {},
   "source": [
    "If not already added, add the documents to the vector_store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "724e30de-4bec-4b5f-8ef5-d57309b0468e",
   "metadata": {
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1729365103999,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "724e30de-4bec-4b5f-8ef5-d57309b0468e"
   },
   "outputs": [],
   "source": [
    "# vector_store.add_documents(documents=split_pages, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "WMeDRt37JqAN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1729365132673,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "WMeDRt37JqAN",
    "outputId": "de49e652-110d-4656-f6a5-324ab38e0bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n"
     ]
    }
   ],
   "source": [
    "# If you'd want to remove data from the vector_store:\n",
    "# vector_store.delete(ids=uuids)\n",
    "# check the number of vectors stored in the vector_store.\n",
    "print(vector_store._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b34f8-9419-4f56-b4e0-fe81ba7d4eaf",
   "metadata": {},
   "source": [
    "The following code cell contains a question you could ask when you are working with the content of the C# markdown presentation, ref. https://raw.githubusercontent.com/HOGENT-Web/csharp/main/chapters/03/slides/presentation.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64147cf-0bf4-4f92-9df2-d4d407102450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:12:38.521434Z",
     "start_time": "2023-11-17T13:12:38.076712Z"
    },
    "collapsed": false,
    "id": "6a025626d6654dde",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# question = \"How does polymorphism work in C#?\"\n",
    "# result = vector_store.similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443d5b2-8ef7-4a9c-aeb7-eb8e97eabf1c",
   "metadata": {},
   "source": [
    "Here's a question you can ask about the lbdl.pdf content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eJqg_30a_EPu",
   "metadata": {
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1729365171718,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "eJqg_30a_EPu"
   },
   "outputs": [],
   "source": [
    "question = \"Name a deep learning application.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c729c-710d-455f-a4c3-3694e632c0a6",
   "metadata": {},
   "source": [
    "Perform a similarity search for this question on the vector store.\n",
    "Also supply the parameter \"k=5\" for retrieving 5 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a02266-3fc3-4b0d-a6bf-882b8ea6cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # fill this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fbd6ad8ed2cd6585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T12:35:50.214634Z",
     "start_time": "2023-11-16T12:35:50.206265Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1729365173523,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "fbd6ad8ed2cd6585",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b472317f-20da-4c75-b048-99afbd521a9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "Chapter 6\n",
      "Prediction\n",
      "A first category of applications, such as face\n",
      "recognition, sentiment analysis, object detection,\n",
      "or speech recognition, requires predicting an un-\n",
      "known value from an available signal.\n",
      "116\n",
      "{'page': 115, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
      "None\n",
      "Document\n"
     ]
    }
   ],
   "source": [
    "print(len(result))\n",
    "print(type(result[0]))\n",
    "\n",
    "print(result[0].page_content)\n",
    "print(result[0].metadata)\n",
    "print(result[0].id)\n",
    "print(result[0].type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2M3IAFD48Sd_",
   "metadata": {
    "id": "2M3IAFD48Sd_"
   },
   "outputs": [],
   "source": [
    "# question = \"How does polymorphism work in C#?\"\n",
    "question = \"What does CNN stand for?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182eccc7-e430-4cc0-9c92-02595b086aac",
   "metadata": {},
   "source": [
    "Perform a similarity search for the above question on the vector store.\n",
    "Also supply the parameter \"k=3\" for retrieving 3 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecaa39e-48c8-43b6-a06d-0076f422e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = # fill this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4CpMQD-R8aBC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1729328282946,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "4CpMQD-R8aBC",
    "outputId": "d7e01f22-c541-4e40-c8bf-2fdf4ec35310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Chapter 6\n",
      "Prediction\n",
      "A first category of applications, such as face\n",
      "recognition, sentiment analysis, object detection,\n",
      "or speech recognition, requires predicting an un-\n",
      "known value from an available signal.\n",
      "116 \n",
      "\n",
      "\n",
      "6.1 Image denoising\n",
      "A direct application of deep models to image\n",
      "processing is to recover from degradation by\n",
      "utilizing the redundancy in the statistical struc-\n",
      "ture of images. The petals of a sunflower in a\n",
      "grayscale picture can be colored with high confi-\n",
      "dence, and the texture of a geometric shape such\n",
      "as a table on a low-light, grainy picture can be\n",
      "corrected by averaging it over a large area likely\n",
      "to be uniform.\n",
      "Adenoisingautoencoder is a model that takes\n",
      "a degraded signal ÀúXas input and computes an\n",
      "estimate of the original signal X. For images, it\n",
      "is a convolutional network that may integrate\n",
      "skip-connections, in particular to combine repre-\n",
      "sentations at the same resolution obtained early\n",
      "and late in the model, as well as attention layers\n",
      "to facilitate taking into account elements that\n",
      "are far away from each other.\n",
      "Such a model is trained by collecting a large num-\n",
      "ber of clean samples paired with their degraded\n",
      "inputs. The latter can be captured in degraded \n",
      "\n",
      "\n",
      "Part II\n",
      "Deep models\n",
      "56 \n",
      "\n",
      "\n",
      "Foreword\n",
      "The current period of progress in artificial in-\n",
      "telligence was triggered when Krizhevsky et al.\n",
      "[2012] showed that an artificialneuralnetwork\n",
      "with a simple structure, which had been known\n",
      "for more than twenty years [LeCun et al., 1989],\n",
      "could beat complex state-of-the-art image recog-\n",
      "nition methods by a huge margin, simply by\n",
      "being a hundred times larger and trained on a\n",
      "dataset similarly scaled up.\n",
      "This breakthrough was made possible thanks\n",
      "toGraph icalProcessingUnits ( GPUs), mass-\n",
      "market, highly parallel computing devices de-\n",
      "veloped for real-time image synthesis and repur-\n",
      "posed for artificial neural networks.\n",
      "Since then, under the umbrella term of ‚Äú deep\n",
      "learn ing, ‚Äù innovations in the structures of these\n",
      "networks, the strategies to train them, and ded-\n",
      "icated hardware have allowed for an exponen-\n",
      "tial increase in both their size and the quantity\n",
      "8 \n",
      "\n",
      "\n",
      "Chapter 1\n",
      "Machine Learning\n",
      "Deep learn ing belongs historically to the larger\n",
      "field of statistical machine learn ing, as it funda-\n",
      "mentally concerns methods that are able to learn\n",
      "representations from data. The techniques in-\n",
      "volved come originally from artificialneuralnet-\n",
      "works, and the ‚Äúdeep‚Äù qualifier highlights that\n",
      "models are long compositions of mappings, now\n",
      "known to achieve greater performance.\n",
      "The modularity, versatility, and scalability of\n",
      "deep models have resulted in a plethora of spe-\n",
      "cific mathematical methods and software devel-\n",
      "opment tools, establishing deep learning as a\n",
      "distinct and vast technical field.\n",
      "11 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(result))\n",
    "\n",
    "print(f\"{result[0].page_content} \\n\\n\")\n",
    "print(f\"{result[1].page_content} \\n\\n\")\n",
    "print(f\"{result[2].page_content} \\n\\n\")\n",
    "print(f\"{result[3].page_content} \\n\\n\")\n",
    "print(f\"{result[4].page_content} \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be670f4a648d67fa",
   "metadata": {
    "collapsed": false,
    "id": "be670f4a648d67fa",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "#### Maximum marginal relevance (MMR)\n",
    "\n",
    "When looking for the most similar results, it sometimes happens that you collect results which are redundant, you get some results that all mean the same thing.\n",
    "\n",
    " MMR can then help, the algorithm will, next to the relevance of the results, also account for the 'diversity'. It will make a new ranking based on both relevance and diversity.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04692982d638fe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T15:35:50.639198Z",
     "start_time": "2023-11-20T15:35:50.455659Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 587,
     "status": "ok",
     "timestamp": 1729365761555,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "a04692982d638fe0",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "472dd3de-e9f6-40d5-ddb2-840ecfe2d6f6"
   },
   "outputs": [],
   "source": [
    "# from langchain_chroma import Chroma\n",
    "\n",
    "persist_directory =  r\"/content/drive/MyDrive/Colab Data/chroma_langchain_db_openai_2\"\n",
    "chromadb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=get_embedding()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb15a2d-d766-45ae-9c50-7acbcc5a4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a similarity_search using the query = \"Name a deep learning application.\" and k=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "T821qCxLO328",
   "metadata": {
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1729365850226,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "T821qCxLO328"
   },
   "outputs": [],
   "source": [
    "new_result = vector_store. # fill this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fdf44cd11ea0618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:12:42.969596Z",
     "start_time": "2023-11-17T13:12:42.965027Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1729365854668,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "4fdf44cd11ea0618",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "49853e2e-4a07-4629-e16c-517c3df29a1f"
   },
   "outputs": [],
   "source": [
    "# result was het resultaat van een similarity_search\n",
    "i=1\n",
    "for r in new_result:\n",
    "  print(i)\n",
    "  print(r.page_content)\n",
    "  print(r.metadata)\n",
    "  i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f872579-a368-4820-af39-5f731ac43632",
   "metadata": {},
   "source": [
    "Perform a \"maximum marginal relevance search\" on the question or query, originally fetching 5 results, keeping only 3 results in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1c8630bf9bf8ff46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T12:40:28.969269Z",
     "start_time": "2023-11-17T12:40:28.751367Z"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1729369972963,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "1c8630bf9bf8ff46",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mmr = vector_store. # fill this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qNLcG2V8pQ-S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1729365882069,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "qNLcG2V8pQ-S",
    "outputId": "deb09511-8ad0-4b42-b1fe-e3f33a04d2c5"
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "for r in mmr:\n",
    "  print(\"###################\")\n",
    "  print(i)\n",
    "  print(r.page_content)\n",
    "  print(r.metadata)\n",
    "  i+=1\n",
    "  print(\"###################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0226dd4e75234ba",
   "metadata": {
    "collapsed": false,
    "id": "f0226dd4e75234ba",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here's a concept, usefull of mentioning. I did not test it yet.\n",
    "\n",
    "SelfQuery, een ander algoritme dat je met langchain kan gebruiken is SelfQuery, het idee is dat je je vraag stelt in 'natuurlijke taal', en dat de LLM zichzelf gebruikt (vandaar de naam) om onderscheid te maken tussen delen van de vraag waarmee de metadata kan gefilterd worden, en de eigenlijk inhoud zelf.\n",
    "\n",
    "Als we kijken welke metadata er in het resultaat van onze similarity_search zit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "37595c357758a6ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:12:47.167841Z",
     "start_time": "2023-11-17T13:12:47.163267Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1729366045882,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "37595c357758a6ca",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "2bd38dd9-daf1-48d0-fee2-1e49b5797c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 115, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
      "{'page': 116, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
      "{'page': 55, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
      "{'page': 7, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
      "{'page': 10, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
      "\n",
      "\n",
      "{'page': 115, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
      "{'page': 116, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
      "{'page': 7, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in new_result:\n",
    "    print (d.metadata)\n",
    "print(\"\\n\")\n",
    "for d in mmr:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4f0e0a742392fc",
   "metadata": {
    "collapsed": false,
    "id": "be4f0e0a742392fc",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Retrieval and Generation: Retrieve\n",
    "\n",
    "So far, we have learned\n",
    "- how to load data,\n",
    "- how to split documents,\n",
    "- how to store documents in a vectorstore\n",
    "- how to query for a document, relevant to our questions.\n",
    "\n",
    "The next step is having an LLM answer our question, using the content of the retrieved document(s).\n",
    "\n",
    "So far, we've queried the vector_store, using methods specific to the vector_store itself, like 'similarity_search' and 'mmr'.\n",
    "\n",
    "Langchain aims at being a generic tool, that makes it easy to switch components, like vectorstore. Therefore Langchain makes use of a Retriever interface.\n",
    "(ref. https://python.langchain.com/docs/tutorials/rag/, 4. Retrieval and Generation: Retrieve).\n",
    "\n",
    "The most common type of Retriever is the VectorStoreRetriever.\n",
    "- Turn our vectorstore into a Retriever object.\n",
    "- Set the parameters\n",
    "  - search_type to \"similarity\",\n",
    "  - search_kwargs takes k = 2 and fetch_k = 5.\n",
    "- Invoke the retriever with a question relevant to the content of the lbdl.pdf.\n",
    "  For example: \"What is a CNN?\", \"Name a deep learning application.\"\n",
    "- Check the number of results returned & the results itself.\n",
    "- What's the goal of a Self-Query or Self-Querying Retriever? How does it work?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qWZxqfMfWVf4",
   "metadata": {
    "id": "qWZxqfMfWVf4"
   },
   "source": [
    "Answer to the question: What's the goal of a Self-Query Retriever?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a421318d7dc4266",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T15:50:53.955407Z",
     "start_time": "2023-11-20T15:50:53.944863Z"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 563,
     "status": "ok",
     "timestamp": 1729366656111,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "a421318d7dc4266",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Turn out vector_store into a Retriever. Add the parameters.\n",
    "retriever = vector_store. # fill this part\n",
    "\n",
    "result_retriever = retriever. # invoke the retriever with a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "NhLbpObRSAWR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1729366657869,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "NhLbpObRSAWR",
    "outputId": "fd24a012-ba64-4d2f-b2a0-233c07e682a3"
   },
   "outputs": [],
   "source": [
    "# Check the number of results returned and the results itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N7_iMdaOW0Ps",
   "metadata": {
    "id": "N7_iMdaOW0Ps"
   },
   "source": [
    "## Retrieval and Generation: Generate\n",
    "\n",
    "Let‚Äôs put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gM_FBpwxZBMf",
   "metadata": {
    "id": "gM_FBpwxZBMf"
   },
   "source": [
    "After focussing on retrieving the relevant documents, the retrieved information can be fed into a prompt, together with our question.\n",
    "\n",
    "Here are two good practices for constructing such a prompt.\n",
    "\n",
    "- Use a predefined prompt from the langchain hub,\n",
    "- Create a customized prompt, using a PromptTemplate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5-XZIkEUZquh",
   "metadata": {
    "id": "5-XZIkEUZquh"
   },
   "source": [
    "### Predefined prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2oig19OKTIH",
   "metadata": {
    "id": "e2oig19OKTIH"
   },
   "outputs": [],
   "source": [
    "We‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub.\n",
    "(ref. https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=3aa0741f-ee94-47fd-a2d9-99a1e771f6fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "R_jEMy_5KbWn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1663,
     "status": "ok",
     "timestamp": 1729368111757,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "R_jEMy_5KbWn",
    "outputId": "9972ec6d-d61d-4768-b22e-58bd94f4b313"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform the import\n",
    "\n",
    "\n",
    "prompt = # fill this part\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "k4j5LRIJKqwV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1729368134812,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "k4j5LRIJKqwV",
    "outputId": "4cf57cfb-6adb-4251-ab5b-55688f822a67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VKvNY3-MZvuB",
   "metadata": {
    "id": "VKvNY3-MZvuB"
   },
   "source": [
    "### PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "19acf46da1ac7ad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T15:58:53.564161Z",
     "start_time": "2023-11-20T15:58:53.542880Z"
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1729369541282,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "19acf46da1ac7ad1",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks to HOGENT course!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gppTlF43dT3i",
   "metadata": {
    "id": "gppTlF43dT3i"
   },
   "source": [
    "- Chain everything together using the code.\n",
    "- Ask some questions about the lbdl.pdf, like:\n",
    "  - \"Give 3 examples of deep learning applicaitons\"\n",
    "  - \"What does CNN mean?\"\n",
    "  - \"Explain how CNN works.\"\n",
    "  - \"What is the role of an activation function?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dxiT2sAFMyTV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2173,
     "status": "ok",
     "timestamp": 1729369752089,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "dxiT2sAFMyTV",
    "outputId": "fca2fe8f-6279-4b79-8b12-910d15877467"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d84bdf83-278a-49a3-b69f-516601dd3f44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3qnF2rw--MKi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 1478,
     "status": "error",
     "timestamp": 1729370658910,
     "user": {
      "displayName": "veerle ai",
      "userId": "17587826153049443714"
     },
     "user_tz": -120
    },
    "id": "3qnF2rw--MKi",
    "outputId": "25f0926c-9771-41d1-dae3-7dbf6576208e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain import hub\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\n\\n# See full prompt at https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\\nretrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\\n# retrieval_qa_chat_prompt = hub.pull(\"rlm/rag-prompt\")\\n\\nprint(retrieval_qa_chat_prompt)\\ncombine_docs_chain = create_stuff_documents_chain(get_llm, retrieval_qa_chat_prompt)\\nrag_chain = create_retrieval_chain(chromadb.as_retriever({\"input\": \"What is Task Decomposition?\"}), combine_docs_chain)\\n\\nresponse = rag_chain.invoke()\\nprint(response[\"answer\"])\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Onderstaande code geeft een \"Pydantic\" error. \n",
    "# Ik vermoed dat deze  code gemigreerd kan worden naar LangGraph.\n",
    "ref. https://langchain-ai.github.io/langgraph/\n",
    "\"\"\"\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# See full prompt at https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "# retrieval_qa_chat_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(retrieval_qa_chat_prompt)\n",
    "combine_docs_chain = create_stuff_documents_chain(get_llm, retrieval_qa_chat_prompt)\n",
    "rag_chain = create_retrieval_chain(chromadb.as_retriever({\"input\": \"What is Task Decomposition?\"}), combine_docs_chain)\n",
    "\n",
    "response = rag_chain.invoke()\n",
    "print(response[\"answer\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56feab5ddee85f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T16:00:04.272539Z",
     "start_time": "2023-11-20T16:00:01.571287Z"
    },
    "collapsed": false,
    "id": "d56feab5ddee85f0",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8cde76bf-19a7-486f-ce92-7b8cfece3583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Polymorphism can be used in programming languages to create functions or objects that can operate on different types of data. For example, a function that can add two numbers together can also be used to concatenate two strings. Thanks to HOGENT course!\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": \"Give an example of where polymorphism can be used?\"})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c45f65e1e47e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T16:00:15.964769Z",
     "start_time": "2023-11-20T16:00:15.957039Z"
    },
    "collapsed": false,
    "id": "832c45f65e1e47e4",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "66ceb3ee-c060-4b11-8182-4d677c9468ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='‚Äúbecause her puppy was sick, Jane was‚Äù.\n",
      "This results in particular in the ability to solve\n",
      "few-shot prediction, where only a handful of\n",
      "training examples are available, as illustrated\n",
      "in Figure 7.1. More surprisingly, when given a\n",
      "carefully crafted prompt, it can exhibit abilities\n",
      "138' metadata={'page': 137, 'source': 'data/lbdl.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(result[\"source_documents\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c1ceb0525d314",
   "metadata": {
    "collapsed": false,
    "id": "df0c1ceb0525d314",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### map reduce\n",
    "\n",
    "Als de documenten te uitgebreid zijn, zullen ze al snel groter zijn dan de beschikbare context voor LLM's. Een oplossing is om map reduce toe te passen, simpel gezegd zal je de documenten opsplitsen, de vraag naar elk sturen 'mappen', en dan de verschillende antwoorden 'reducen'.\n",
    "\n",
    "Dit leidt snel tot vrij veel API calls dus we gaan dit hier niet demonstreren. Er zijn voorbeelden en uitleg te vinden op langchain als je dit nodig hebt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b2ec3adccfe60",
   "metadata": {
    "collapsed": false,
    "id": "f55b2ec3adccfe60",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Chat\n",
    "\n",
    "To be able to truly chat with the data, there's still a missing piece of the puzzle: we need to be able to incorporate the previously given answers into the next question. This way, we can get additional clarification, just as we are now accustomed to with chatbots.\n",
    "\n",
    "Remark that this is legacy code. It is currently advised to use LangGraph to add memory to your conversation.\n",
    "\n",
    "Use the migration guide to convert the code below. The migration guide also assumes some familiarity with LangGraph.\n",
    "\n",
    "ref. https://python.langchain.com/docs/versions/migrating_memory/conversation_buffer_memory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6821fe5e7d5f7823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T16:09:05.641770Z",
     "start_time": "2023-11-20T16:09:05.624791Z"
    },
    "collapsed": false,
    "id": "6821fe5e7d5f7823",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1f683344-7fd3-4824-c3e8-06bbd874e1b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_275/3025019359.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# What we lack for the moment, is a chat memory.\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "qa_conversation = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=chromadb.as_retriever(),\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02490e0875f5e97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T16:10:14.446765Z",
     "start_time": "2023-11-20T16:10:12.138059Z"
    },
    "collapsed": false,
    "id": "f02490e0875f5e97",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "89e34c5f-0c38-414e-f9ff-22dd172e8611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Give an example of where polymorphism can be used?', 'chat_history': [HumanMessage(content='Give an example of where polymorphism can be used?'), AIMessage(content=' Polymorphism can be used when creating a class hierarchy, where a parent class is inherited by multiple child classes. For example, a `BankAccount` class can be inherited by a `SavingsAccount` class and a `CheckingAccount` class.'), HumanMessage(content='Give an example of where polymorphism can be used?'), AIMessage(content=' Polymorphism can be used to create a class hierarchy by having a parent class and then creating child classes that inherit the properties of the parent class.')], 'answer': ' Polymorphism can be used to create a class hierarchy by having a parent class and then creating child classes that inherit the properties of the parent class.'}\n"
     ]
    }
   ],
   "source": [
    "# opgelet! de key is nu 'question' en niet 'query'\n",
    "result = qa_conversation({\"question\": \"Give an example of where polymorphism can be used?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435eca598799b5ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T16:11:16.151638Z",
     "start_time": "2023-11-20T16:11:13.294568Z"
    },
    "collapsed": false,
    "id": "435eca598799b5ce",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "7dbc2a54-f778-4092-8046-d304b13907e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What would an implementation of a SavingsAccount look like?', 'chat_history': [HumanMessage(content='Give an example of where polymorphism can be used?'), AIMessage(content=' Polymorphism can be used when creating a class hierarchy, where a parent class is inherited by multiple child classes. For example, a `BankAccount` class can be inherited by a `SavingsAccount` class and a `CheckingAccount` class.'), HumanMessage(content='Give an example of where polymorphism can be used?'), AIMessage(content=' Polymorphism can be used to create a class hierarchy by having a parent class and then creating child classes that inherit the properties of the parent class.'), HumanMessage(content='What would an implementation of a SavingsAccount look like?'), AIMessage(content=' Checking the type of the instance is possible with the `is` keyword: \\n```\\nBankAccount s = new SavingsAccount(\"123-123123-13\", 0.1M)\\nif (s is SavingsAccount)\\n{\\n// Do something useful\\n}\\n```')], 'answer': ' Checking the type of the instance is possible with the `is` keyword: \\n```\\nBankAccount s = new SavingsAccount(\"123-123123-13\", 0.1M)\\nif (s is SavingsAccount)\\n{\\n// Do something useful\\n}\\n```'}\n"
     ]
    }
   ],
   "source": [
    "result = qa_conversation({\"question\": \"What would an implementation of a SavingsAccount look like?\"})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1ItLASY5_y2eDYTQlpXoTwZiY_GAy4_FR",
     "timestamp": 1729333527901
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
