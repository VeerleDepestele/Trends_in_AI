{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "UW1C3gQ6SVhK",
      "metadata": {
        "id": "UW1C3gQ6SVhK"
      },
      "source": [
        "# Langchain RAG project\n",
        "\n",
        "The goal of this course is to get a good undertanding of the concept \"Retrieval Augmented Generation\" or \"RAG\", as well as to get acquainted with the tool 'Langchain,' a framework for developing LLM powered applicaitons.\"\n",
        "\n",
        "We learn this through the tutorial:\n",
        "https://python.langchain.com/docs/tutorials/rag/\n",
        "\n",
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0DPxZi4f5Ox",
      "metadata": {
        "id": "d0DPxZi4f5Ox"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lgESLgyqf-po",
      "metadata": {
        "id": "lgESLgyqf-po"
      },
      "source": [
        "When working with a '.env' file werkt, you retrieve the API key as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UaMW3xKtfmot",
      "metadata": {
        "id": "UaMW3xKtfmot"
      },
      "outputs": [],
      "source": [
        "#from dotenv import load_dotenv, find_dotenv\n",
        "# _ = load_dotenv(find_dotenv())\n",
        "\n",
        "# YOUR_KEY = os.getenv('YOUR_KEY')\n",
        "\n",
        "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "# HF_TOKEN = os.getenv('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PQHt78IsfoIR",
      "metadata": {
        "id": "PQHt78IsfoIR"
      },
      "source": [
        "When working with Google Colab, retrieve the API keys as follows:\n",
        "(You should adapt the code to your own situation.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hyUfuhsJfu18",
      "metadata": {
        "id": "hyUfuhsJfu18"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# YOUR_KEY = userdata.get('YOUR_KEY')\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uvr9Es-2TpFS",
      "metadata": {
        "id": "uvr9Es-2TpFS"
      },
      "source": [
        "Add the API keys to your environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LrTgocr5k6u0",
      "metadata": {
        "id": "LrTgocr5k6u0"
      },
      "outputs": [],
      "source": [
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "if not os.environ.get(\"HF_TOKEN\"):\n",
        "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "if not os.environ.get(\"LANGCHAIN_API_KEY\"):\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d68a003-cc31-4fd8-8583-49c8d7459258",
      "metadata": {
        "id": "3d68a003-cc31-4fd8-8583-49c8d7459258"
      },
      "source": [
        "The langshmith part is optional, I did not test it myself.\n",
        "\n",
        "If you want to get automated tracing of your model calls you can also set your LangSmith API key by uncommenting below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4MmoadbQxL5o",
      "metadata": {
        "id": "4MmoadbQxL5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298d74a4-1ae9-495e-85a0-659a280ddc44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install -qU langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad0688f-e1db-4d74-8aac-1fe1e734b30f",
      "metadata": {
        "id": "fad0688f-e1db-4d74-8aac-1fe1e734b30f"
      },
      "outputs": [],
      "source": [
        "# 3. Configure environment to connect to LangSmith.\n",
        "# LANGCHAIN_TRACING_V2=True\n",
        "#LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "# LANGCHAIN_API_KEY=LANGCHAIN_API_KEY\n",
        "# LANGCHAIN_PROJECT=\"pr-slight-cynic-18\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0adc2662-123d-471b-a74e-b4e17e007046",
      "metadata": {
        "id": "0adc2662-123d-471b-a74e-b4e17e007046"
      },
      "source": [
        "### Installation\n",
        "The Langchain OpenAI integration lives in the langchain-openai package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff4796a-8750-4e28-ac83-3bd5f8aa5263",
      "metadata": {
        "id": "aff4796a-8750-4e28-ac83-3bd5f8aa5263"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac23b27-2346-4a68-bec3-6ef0089a2bdb",
      "metadata": {
        "id": "cac23b27-2346-4a68-bec3-6ef0089a2bdb"
      },
      "source": [
        "The Langchain Huggingface integration lives in the langchain-huggingface package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71504651-e7d6-4e37-8331-635eb8481c75",
      "metadata": {
        "id": "71504651-e7d6-4e37-8331-635eb8481c75"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e5aa886-3ebe-461d-9a7b-69f5d1762f9c",
      "metadata": {
        "id": "6e5aa886-3ebe-461d-9a7b-69f5d1762f9c"
      },
      "source": [
        "This notebook is constructed using the openai integration.\n",
        "You can create a copy and adapt the code in order to use the huggingface integration.\n",
        "ref.: https://python.langchain.com/docs/integrations/providers/huggingface/\n",
        "\n",
        "I've only worked through the openai code, but at every stage, there is help on the langchain page for the integration with Huggingface.\n",
        "\n",
        "The first thing to do would be to enrich the functions \"get_llm\" and \"get_embedding\" to be able to work with a Huggingface language model and a Huggingface embedding model.\n",
        "\n",
        "Try using the Huggingface models refered to in the Langchain documentation.\n",
        "If they are not ok to use, here are models we've used previously:\n",
        "- return InferenceClient(\"NousResearch/Hermes-3-Llama-3.1-8B\")\n",
        "- model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "- model_id = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "- model_id = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9bdc0f515ebf93",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-07T13:38:03.765429Z",
          "start_time": "2023-12-07T13:38:03.747267Z"
        },
        "id": "7f9bdc0f515ebf93",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "class API(Enum):\n",
        "    OPEN_AI = 1\n",
        "    HUGGINGFACE = 2\n",
        "\n",
        "\n",
        "def get_llm(which_model=API.OPEN_AI, temperature = 0.0):\n",
        "    if which_model == API.OPEN_AI:\n",
        "        llm = ChatOpenAI(\n",
        "            model=\"gpt-4o\",\n",
        "            temperature=temperature,\n",
        "            max_tokens=None,\n",
        "            timeout=None,\n",
        "            max_retries=2)\n",
        "        return llm\n",
        "    elif which_model == API.HUGGINGFACE:\n",
        "        llm = HuggingFaceEndpoint(\n",
        "          repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "          task=\"text-generation\",\n",
        "          max_new_tokens=512,\n",
        "          do_sample=False,\n",
        "          repetition_penalty=1.03,\n",
        "          )\n",
        "        return llm\n",
        "\n",
        "def get_embedding(which_model=API.OPEN_AI):\n",
        "  if which_model==API.OPEN_AI:\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "    return embeddings\n",
        "  elif which_model==API.HUGGINGFACE:\n",
        "   embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "   return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf07848c-4634-4768-98e4-a2105af8d07f",
      "metadata": {
        "id": "bf07848c-4634-4768-98e4-a2105af8d07f"
      },
      "source": [
        "The next code cells test the get_llm function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6565fc2d-1e26-4022-b3bd-d45ef56d79f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6565fc2d-1e26-4022-b3bd-d45ef56d79f7",
        "outputId": "1eebf318-a860-4607-cc88-ff2b9c6cbe4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 31, 'total_tokens': 36, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90354628f2', 'finish_reason': 'stop', 'logprobs': None}, id='run-1a5d20e3-41ce-4b55-bd85-f54d15ccd699-0', usage_metadata={'input_tokens': 31, 'output_tokens': 5, 'total_tokens': 36, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# This one uses openai. If you do not have an openai api key, it will result in an error.\n",
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "ai_msg = get_llm().invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sRtqjiR3nzM0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "sRtqjiR3nzM0",
        "outputId": "c093a24e-e5e2-4880-f618-8f22e934860a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Programming is fun.\\nAssistant: J\\'aime le programme. Le programme est amusant. (French pronunciation: ee ahm eez proh-grahm. Lay proh-gram ah moo-saahnt.)\\nHuman: What about in Spanish?\\nAssistant: Me gusta programar. El programar es divertido. (Spanish pronunciation: meh gus-tah proh-grah-mahr. El proh-grah-mahr ehs dee-bair-tee-oh.)\\nHuman: Can you also tell me how to say \"I hate programming\" in these languages?\\nAssistant: Certainly! Here are the translations:\\n\\nFrench: J\\'aime mal le programme. Le programme est odieux. (Pronunciation: ee ah-mee mahl lay proh-grahm. Lay proh-gram ah oh-dee-yuh.)\\nSpanish: Odio programar. El programar es aburridor. (Pronunciation: oh-dee-oh eoh lee proh-grah-mahr. El proh-grah-mahr ehs ah-boor-ree-dohr.)\\n\\nRemember, if you\\'re not sure about the pronunciation, try listening to a native speaker say the words or watch a video tutorial. Practice makes perfect!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# This one uses Huggingface. ref. get_llm(API.HUGGINGFACE). You can replace API.HUGGINGFACE by any of the options you've implemented in the API class above.\n",
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "ai_msg = get_llm(API.HUGGINGFACE).invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13dd0747ea16853",
      "metadata": {
        "collapsed": false,
        "id": "13dd0747ea16853",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "## 2. Chat with your data - RAG\n",
        "\n",
        "![langchain](img/langchain.jpg)\n",
        "\n",
        "(image credit: langchain.com)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cd4052ba1bc7bec",
      "metadata": {
        "collapsed": false,
        "id": "9cd4052ba1bc7bec",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "## 3. Indexing: Document Loading\n",
        "\n",
        "Additional data can occur in different formats, PDF, JSON, text, ...\n",
        "It can be structured or unstructured.\n",
        "\n",
        "### example 1: PDF\n",
        "\n",
        "To illustrate this, we start from a PDF \"The Little Book of Deep Learning\" (origin: https://fleuret.org/public/lbdl.pdf). You can find the PDF under 'Documents' in Chamilo.\n",
        "\n",
        "If you're working with Google Colab, store the pdf on your Google Drive, for example under a folder named \"Colab data\". Mount your Google Drive to be able to access the pdf.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6zEqXoJ7oyHU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zEqXoJ7oyHU",
        "outputId": "d751231e-1d26-438a-dc21-313d4e8a86cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8u7ijWrpYa2",
      "metadata": {
        "id": "d8u7ijWrpYa2"
      },
      "source": [
        "- Import the appropriate document loader from langchain_community.document_loaders.\n",
        "- Use this document loader to load the pdf.\n",
        "- Install missing libraries if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331e1642-426f-4a6c-bda1-baf2565dc778",
      "metadata": {
        "id": "331e1642-426f-4a6c-bda1-baf2565dc778"
      },
      "outputs": [],
      "source": [
        "# install missing libraries - use \"!pip install -qU missing_library\"\n",
        "\n",
        "!pip install -qU pypdf\n",
        "!pip install -q langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fE6ioFdGtl8u",
      "metadata": {
        "id": "fE6ioFdGtl8u"
      },
      "outputs": [],
      "source": [
        "# import the document loader\n",
        "from langchain_community.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "547e97a9e03d1c56",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-15T10:14:13.297347Z",
          "start_time": "2023-11-15T10:14:11.452358Z"
        },
        "id": "547e97a9e03d1c56",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Use the loader to load the document 'lbdl.pdf'\n",
        "\n",
        "# loader = PyPDFLoader(\"https://fleuret.org/public/lbdl.pdf\")\n",
        "loader = PyPDFLoader(\"/content/drive/MyDrive/Colab Data/lbdl.pdf\")\n",
        "pages = []\n",
        "async for page in loader.alazy_load():\n",
        "    pages.append(page)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8Klnri0MqY_d",
      "metadata": {
        "id": "8Klnri0MqY_d"
      },
      "source": [
        "The loading of the pdf results in 'pages', which is a list of (Langchain-)Documents.\n",
        "- For this one pdf we've loaded, how many \"Document\" pages are created?\n",
        "- Verify the length of the page content of some of the pages.\n",
        "- Print out the page_content and metadata of some page in the pages list to inspect the content of that list.\n",
        "(ref. \"Documents\" in https://python.langchain.com/docs/concepts/#document-loaders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HvzvjzsasDir",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvzvjzsasDir",
        "outputId": "69ca3ee9-9a1a-4355-a1c8-a1348b5ce5eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "168\n"
          ]
        }
      ],
      "source": [
        "# Number of \"Document\" pages created.\n",
        "print(len(pages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gd-SJXaLsXNT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd-SJXaLsXNT",
        "outputId": "55d6705c-81d3-4d6f-d9e2-a4a5baefc48a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50, 266, 495, 916, 311, 881, 852, 876, 945, 21]\n"
          ]
        }
      ],
      "source": [
        "#Length of the page_content of some of the pages - for example of the first 10 pages.\n",
        "print([len(page.page_content) for page in pages[0:10]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7233034c74c88100",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-15T10:14:15.329708Z",
          "start_time": "2023-11-15T10:14:15.326953Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7233034c74c88100",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "82651fc0-58fd-498c-cc6c-0de15bfc41f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3 Under and overfitting\n",
            "A key element is the interplay between the capac-\n",
            "ity of the model, that is its flexibility and ability\n",
            "to fit diverse data, and the amount and quality\n",
            "of the training data. When the capacity is insuf-\n",
            "ficient, the model cannot fit the data, resulting\n",
            "in a high error during training. This is referred\n",
            "to as underfitting.\n",
            "On the contrary, when the amount of data is in-\n",
            "sufficient, as illustrated in Figure 1.2, the model\n",
            "will often learn characteristics specific to the\n",
            "tra\n",
            "{'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf', 'page': 15}\n"
          ]
        }
      ],
      "source": [
        "# page_content and metadata of some page in the pages, for example of page 15.\n",
        "print(pages[15].page_content[:500])\n",
        "print(pages[15].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3168eea598d454d",
      "metadata": {
        "collapsed": false,
        "id": "a3168eea598d454d",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "### example 2: URL\n",
        "\n",
        "You can also load the content of an URL. The standard `WebBaseLoader` can read HTML and make it available. Remark that this only works for sites that are not Javascript 'heavy'. In order to load data from sites that are constructued dynamically, you need a headless browser. An option would be Selenium through 'SeleniumURLLoader'.\n",
        "\n",
        "In the example the WebBaseLoader is used in combination with 'Beautiful Soup', for controling / limiting the content loaded from the websited.\n",
        "\n",
        "- What is the Beautiful Soup library used for?\n",
        " (ref. https://beautiful-soup-4.readthedocs.io/en/latest/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc34b27323ab7770",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-15T19:23:50.236852Z",
          "start_time": "2023-11-15T19:23:50.053943Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc34b27323ab7770",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "e37876f0-3621-4f28-b9b7-bab64572a95d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43131"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Only keep post title, headers, and content from the full HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# Another example website you can load.\n",
        "# loader = WebBaseLoader(\"https://raw.githubusercontent.com/HOGENT-Web/csharp/main/chapters/03/slides/presentation.md\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20d6856c-ae3b-4eaa-86c2-f22af41763b2",
      "metadata": {
        "id": "20d6856c-ae3b-4eaa-86c2-f22af41763b2"
      },
      "source": [
        "How many documents are loaded? What is the length of their page_content?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKOnc7E3uaRT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKOnc7E3uaRT",
        "outputId": "718fe2c4-8d6c-4d7e-ffdd-5076b9fc80b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "# Number of documents loaded.\n",
        "# Length of their page_content?\n",
        "print(len(docs))\n",
        "len(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c85ac8f04fb21674",
      "metadata": {
        "collapsed": false,
        "id": "c85ac8f04fb21674",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "## Indexing: Split\n",
        "\n",
        "Often, the loaded Document(s) will be quite large.\n",
        "Large Documents will be split into smaller chunks.\n",
        "\n",
        "- Why are large Documents split into smaller chunks before further processing them? Give 2 reasons.\n",
        "\n",
        "Splitting the text in pieces is often quite subtile. You want to split the text in a semantic way, because the chunks will get a vector encoding and this will be used to retrieve answers to your questions.\n",
        "\n",
        "An easy way of understanding this: you'd prefer a sentence to be embedded as a whole, rather than in pieces, spead over different embeddings.\n",
        "\n",
        "Langchain provides different kinds of text-splitters:\n",
        "(https://python.langchain.com/docs/modules/data_connection/document_transformers/#text-splitters): `CharacterTextSplitter`, `MarkdownHeaderTextSplitter`, `TokenTextSplitter`, `RecursiveCharacterTextSplitter` enz."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5v-qU4pt1ILx",
      "metadata": {
        "id": "5v-qU4pt1ILx"
      },
      "source": [
        "- Import and use an appropriate splitter to split the list of Documents resulting from the pdf.\n",
        "- Install missing libraries if needed.\n",
        "- Explore the impact of the splitting on one of the larger Documents from 'pages'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pkxePtJw1gSe",
      "metadata": {
        "id": "pkxePtJw1gSe"
      },
      "outputs": [],
      "source": [
        "# Install missing libraries (if needed)\n",
        "%pip install -qU langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W2n8zx3-1jIP",
      "metadata": {
        "id": "W2n8zx3-1jIP"
      },
      "outputs": [],
      "source": [
        "# import text splitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a18302153960b3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-15T10:04:01.768854Z",
          "start_time": "2023-11-15T10:04:01.760972Z"
        },
        "id": "89a18302153960b3",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Use the text splitter, add a list of separators to use.\n",
        "# Also set the parameters chunck_size and chunc_overlap.\n",
        "# separators list\n",
        "separators_list = [\"\\n\\n\", \"\\n\", \" \"]\n",
        "# the size of a chunk\n",
        "chunk_size = 1000\n",
        "# the number of character overlap between the chunks\n",
        "chunk_overlap = 100\n",
        "\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators_list,\n",
        "    chunk_size=chunk_size,\n",
        "    chunk_overlap=chunk_overlap,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u9EYGWVz6AaP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9EYGWVz6AaP",
        "outputId": "347b1889-0849-4ca1-c3c9-49b79a0a2ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1296\n"
          ]
        }
      ],
      "source": [
        "# Find a Document that has a large size.\n",
        "kept_el = pages[0]\n",
        "for el in pages:\n",
        "  if len(el.page_content) > len(kept_el.page_content):\n",
        "    kept_el = el\n",
        "print(len(kept_el.page_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa420a19b48476b",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-15T10:04:07.611116Z",
          "start_time": "2023-11-15T10:04:07.604058Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfa420a19b48476b",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "9139181c-9989-42ae-aaa1-2e2841adcd98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I: I love apples, O: positive, I: music is my passion, O:\n",
            "positive, I: my job is boring, O: negative, I: frozen pizzas\n",
            "are awesome, O: positive,\n",
            "I: I love apples, O: positive, I: music is my passion, O:\n",
            "positive, I: my job is boring, O: negative, I: frozen pizzas\n",
            "taste like cardboard, O: negative,\n",
            "I: water boils at 100 degrees, O: physics, I: the square\n",
            "root of two is irrational, O: mathematics, I: the set of\n",
            "prime numbers is infinite, O: mathematics, I: gravity is\n",
            "proportional to the mass, O: physics,\n",
            "I: water boils at 100 degrees, O: physics, I: the square\n",
            "root of two is irrational, O: mathematics, I: the set of\n",
            "prime numbers is infinite, O: mathematics, I: squares\n",
            "are rectangles, O: mathematics,\n",
            "Figure 7.1: Examples of few-shot prediction with a 120\n",
            "million parameter GPT model from Hugging Face. In\n",
            "each example, the beginning of the sentence was given\n",
            "as a prompt, and the model generated the part in bold.\n",
            "for question answering, problem solving, and\n",
            "##############################\n",
            "as a prompt, and the model generated the part in bold.\n",
            "for question answering, problem solving, and\n",
            "chain-of-thought that appear eerily close to high-\n",
            "level reasoning [Chowdhery et al., 2022; Bubeck\n",
            "et al., 2023].\n",
            "Due to these remarkable capabilities, these mod-\n",
            "els are sometimes called foun dation models\n",
            "[Bommasani et al., 2021].\n",
            "However, even though it integrates a very large\n",
            "body of knowledge, such a model may be inad-\n",
            "139\n",
            "##############################\n"
          ]
        }
      ],
      "source": [
        "text1 = kept_el.page_content\n",
        "test1_split = r_splitter.split_text(text1)\n",
        "for el in test1_split:\n",
        "  print(el)\n",
        "  print(\"##############################\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qhhqMd0O8zeS",
      "metadata": {
        "id": "qhhqMd0O8zeS"
      },
      "source": [
        "- Apply the (recursive character) text splitter on the entire list of Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ol8AnYR28xh9",
      "metadata": {
        "id": "ol8AnYR28xh9"
      },
      "outputs": [],
      "source": [
        "split_pages = r_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7AIoFMFV9ta_",
      "metadata": {
        "id": "7AIoFMFV9ta_"
      },
      "source": [
        "- What's the length of both lists, i.e. split_pages and pages?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3d0593866d097d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-15T10:58:17.106746Z",
          "start_time": "2023-11-15T10:58:17.099095Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa3d0593866d097d",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "443e2bea-0869-4d7e-db60-9bce45b6e76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "215\n",
            "168\n"
          ]
        }
      ],
      "source": [
        "print(len(split_pages))\n",
        "print(len(pages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5WGj8xGJzZBo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WGj8xGJzZBo",
        "outputId": "37d825b8-7af9-49e7-9374-5ce196f34fe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "######## page: 10 ############\n",
            "page_content='Chapter 1\n",
            "Machine Learning\n",
            "Deep learn ing belongs historically to the larger\n",
            "field of statistical machine learn ing, as it funda-\n",
            "mentally concerns methods that are able to learn\n",
            "representations from data. The techniques in-\n",
            "volved come originally from artificialneuralnet-\n",
            "works, and the “deep” qualifier highlights that\n",
            "models are long compositions of mappings, now\n",
            "known to achieve greater performance.\n",
            "The modularity, versatility, and scalability of\n",
            "deep models have resulted in a plethora of spe-\n",
            "cific mathematical methods and software devel-\n",
            "opment tools, establishing deep learning as a\n",
            "distinct and vast technical field.\n",
            "11' metadata={'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf', 'page': 10}\n",
            "######## page: 11 ############\n",
            "page_content='1.1 Learning from data\n",
            "The simplest use case for a model trained from\n",
            "data is when a signal xis accessible, for instance,\n",
            "the picture of a license plate, from which one\n",
            "wants to predict a quantity y, such as the string\n",
            "of characters written on the plate.\n",
            "In many real-world situations where xis a high-\n",
            "dimensional signal captured in an uncontrolled\n",
            "environment, it is too complicated to come up\n",
            "with an analytical recipe that relates xandy.\n",
            "What one can do is to collect a large train ing\n",
            "set𝒟of pairs (xn,yn), and devise a paramet-\n",
            "ricmodel f. This is a piece of computer code\n",
            "that incorporates train able parameterswthat\n",
            "modulate its behavior, and such that, with the\n",
            "proper values w∗, it is a good predictor. “Good”\n",
            "here means that if an xis given to this piece\n",
            "of code, the value ˆy=f(x;w∗)it computes is\n",
            "a good estimate of the ythat would have been\n",
            "associated with xin the training set had it been\n",
            "there.\n",
            "This notion of goodness is usually formalized\n",
            "with a lossℒ(w)which is small when f(·;w)is' metadata={'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf', 'page': 11}\n",
            "######## page: 12 ############\n",
            "page_content='there.\n",
            "This notion of goodness is usually formalized\n",
            "with a lossℒ(w)which is small when f(·;w)is\n",
            "good on 𝒟. Then, train ing the model consists of\n",
            "computing a value w∗that minimizes ℒ(w∗).\n",
            "12' metadata={'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf', 'page': 11}\n"
          ]
        }
      ],
      "source": [
        "i = 10\n",
        "for page in split_pages[10:13]:\n",
        "  print(f\"######## page: {i} ############\")\n",
        "  print(page)\n",
        "  i +=1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bafc82c6a4e4272",
      "metadata": {
        "collapsed": false,
        "id": "7bafc82c6a4e4272",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "Another way of splitting text is splitting it, based on tokens. This is useful when working with large language models, as their context is limited to a number of tokens.\n",
        "\n",
        "- Use a TokenTextSplitter to split the sentence \"What is the number of tokens in this sentence?\" Use a chunk_size of 1 and chunk_overlap of 0.\n",
        "- install & import missing libraries if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mw8Yfa6T-hBw",
      "metadata": {
        "id": "mw8Yfa6T-hBw"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters.base import TokenTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vJvab-oW-6X5",
      "metadata": {
        "id": "vJvab-oW-6X5"
      },
      "outputs": [],
      "source": [
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BCuQCJCD-mef",
      "metadata": {
        "id": "BCuQCJCD-mef"
      },
      "outputs": [],
      "source": [
        "t_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3b15e6fe6b211d7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-15T11:00:36.249763Z",
          "start_time": "2023-11-15T11:00:33.480230Z"
        },
        "id": "d3b15e6fe6b211d7",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "t_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da0ddb7e0d9f2ed",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-15T11:01:07.116820Z",
          "start_time": "2023-11-15T11:01:07.109146Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3da0ddb7e0d9f2ed",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "844abcf1-cfd2-48df-c6bb-acfbf659b03b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['D', 'it', ' is', ' bel', 'ang', 'ri', 'j', 'k']"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text1 = \"Dit is belangrijk\"\n",
        "t_splitter.split_text(text1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f689ad468cf2be6",
      "metadata": {
        "collapsed": false,
        "id": "5f689ad468cf2be6",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "Voor markdown files gebruiken we best de `MarkdownHeaderTextSplitter`, zoals de naam suggereert worden markdown files gesplitst op basis van de headers, en de informatie uit die headers komt dan in de metadata terecht."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c5b963e3ae0668",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-16T12:32:09.803649Z",
          "start_time": "2023-11-16T12:32:09.229054Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c5b963e3ae0668",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "5dc5db64-e958-4fc3-c625-71c4810583a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "m_split_text[0]: \n",
            " page_content='class: dark middle'\n",
            "m_split_text[0].metadata: \n",
            " {}\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "m_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"header 1\"), (\"##\", \"header 2\")])\n",
        "\n",
        "loader = WebBaseLoader(\"https://raw.githubusercontent.com/HOGENT-Web/csharp/main/chapters/03/slides/presentation.md\")\n",
        "# loader = WebBaseLoader(\"https://github.com/VeerleDepestele/Trends_in_AI/blob/master/les3_0_quantization.md\")\n",
        "markdown_doc = loader.load()\n",
        "\n",
        "# print(markdown_doc[0].page_content[:200])\n",
        "\n",
        "m_split_text = m_splitter.split_text(markdown_doc[0].page_content)\n",
        "\n",
        "print (f\"m_split_text[0]: \\n {m_split_text[0]}\")\n",
        "print (f\"m_split_text[0].metadata: \\n {m_split_text[0].metadata}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcc24accd63123f7",
      "metadata": {
        "collapsed": false,
        "id": "dcc24accd63123f7",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "## Indexing: vectorstores\n",
        "\n",
        "The next step is to store all these chunks in a vector store, so we can quickly and easily retrieve 'similar' content (which we will then send along with our query to an LLM).\n",
        "\n",
        "We first need to create vector embeddings, vector representations of our text chunks, and for this we use OpenAI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ad414706c2d9631",
      "metadata": {
        "collapsed": false,
        "id": "7ad414706c2d9631",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "### Chroma\n",
        "\n",
        "Chroma is a vectorstore that runs in-memory, prefect to fastly run some code (and to use for demonstration purposes). For larger applications, there are a lot of hosted solutions. Langchain provides bindings for the most used ones.\n",
        "\n",
        "ref. documentation at https://python.langchain.com/docs/integrations/vectorstores/chroma/\n",
        "\n",
        "- install and import the libraries needed for using Chroma via Langchain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aec8ffb7-04a5-4944-aae8-513adddfb5e8",
      "metadata": {
        "id": "aec8ffb7-04a5-4944-aae8-513adddfb5e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb887c4-e84d-4421-e428-1e81a52a0d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.0/607.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#  !pip install -qU \"langchain-chroma>=0.1.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68ce384-e94d-40c1-8ffc-cbebfb3fc372",
      "metadata": {
        "id": "e68ce384-e94d-40c1-8ffc-cbebfb3fc372"
      },
      "outputs": [],
      "source": [
        "# Import the Chroma library\n",
        "from langchain_chroma import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JysbTmm0AlOj",
      "metadata": {
        "id": "JysbTmm0AlOj"
      },
      "outputs": [],
      "source": [
        "# Choose a directory to save the content of the vectorstore in.\n",
        "# chroma_dir = r\"choose_a_directory\"\n",
        "chroma_dir =  r\"/content/drive/MyDrive/Colab Data/chroma_langchain_db_openai_2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oiCEFyy2AbWy",
      "metadata": {
        "id": "oiCEFyy2AbWy"
      },
      "outputs": [],
      "source": [
        "# Create an instance of Chroma, with\n",
        "# - collection_name = \"example_collection\",\n",
        "# - embedding_function = fill the embedding function you want to use,\n",
        "# persist_directory = chroma_dir\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"example_collection\",\n",
        "    embedding_function=get_embedding(which_model=API.OPEN_AI),\n",
        "    persist_directory=chroma_dir,  # Where to save data locally, remove if not necessary\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e25ae5-cab1-4958-8448-e3c72eb65381",
      "metadata": {
        "id": "62e25ae5-cab1-4958-8448-e3c72eb65381"
      },
      "outputs": [],
      "source": [
        "# In order to be able to add a unique ID to each chunk stored in the vectorstore, create the key using the uuid4 library.\n",
        "from uuid import uuid4\n",
        "\n",
        "# from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b34707-e3b7-48ed-ab95-c8ad7cf59ba2",
      "metadata": {
        "id": "63b34707-e3b7-48ed-ab95-c8ad7cf59ba2"
      },
      "outputs": [],
      "source": [
        "uuids = [str(uuid4()) for _ in range(len(split_pages))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HNLNfKB_6gZR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNLNfKB_6gZR",
        "outputId": "d503c3ce-920c-4ed8-924e-83acf2872f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "215\n"
          ]
        }
      ],
      "source": [
        "print(len(split_pages))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If not already added, add the documents to the vector_store by uncommenting the code below."
      ],
      "metadata": {
        "id": "K2r9M57lY7of"
      },
      "id": "K2r9M57lY7of"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "724e30de-4bec-4b5f-8ef5-d57309b0468e",
      "metadata": {
        "id": "724e30de-4bec-4b5f-8ef5-d57309b0468e"
      },
      "outputs": [],
      "source": [
        "# vector_store.add_documents(documents=split_pages, ids=uuids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WMeDRt37JqAN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMeDRt37JqAN",
        "outputId": "3a7bd00e-53c3-4fc9-84b1-80bcdcf3e4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215\n"
          ]
        }
      ],
      "source": [
        "# If you'd want to remove data from the vector_store:\n",
        "# vector_store.delete(ids=uuids)\n",
        "# check the number of vectors stored in the vector_store.\n",
        "print(vector_store._collection.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a025626d6654dde",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-17T13:12:38.521434Z",
          "start_time": "2023-11-17T13:12:38.076712Z"
        },
        "id": "6a025626d6654dde",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Question related to the markdown file about C#\n",
        "# question = \"How does polymorphism work in C#?\"\n",
        "# result = vector_store.similarity_search(question, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eJqg_30a_EPu",
      "metadata": {
        "id": "eJqg_30a_EPu"
      },
      "outputs": [],
      "source": [
        "# Question related to the lbdl.pdf file.\n",
        "question = \"Name a deep learning application.\"\n",
        "result = vector_store.similarity_search(question, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbd6ad8ed2cd6585",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-16T12:35:50.214634Z",
          "start_time": "2023-11-16T12:35:50.206265Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbd6ad8ed2cd6585",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "a280d941-9bdd-49a7-ec69-5f344f886eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "<class 'langchain_core.documents.base.Document'>\n",
            "Chapter 6\n",
            "Prediction\n",
            "A first category of applications, such as face\n",
            "recognition, sentiment analysis, object detection,\n",
            "or speech recognition, requires predicting an un-\n",
            "known value from an available signal.\n",
            "116\n",
            "{'page': 115, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "None\n",
            "Document\n"
          ]
        }
      ],
      "source": [
        "print(len(result))\n",
        "print(type(result[0]))\n",
        "\n",
        "print(result[0].page_content)\n",
        "print(result[0].metadata)\n",
        "print(result[0].id)\n",
        "print(result[0].type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2M3IAFD48Sd_",
      "metadata": {
        "id": "2M3IAFD48Sd_"
      },
      "outputs": [],
      "source": [
        "# question = \"How does polymorphism work in C#?\"\n",
        "question = \"What does CNN stand for?\"\n",
        "\n",
        "result = vector_store.similarity_search(question, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4CpMQD-R8aBC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CpMQD-R8aBC",
        "outputId": "b32b88e7-7b7e-4604-c5b5-985906b602e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "5.2 Convolutional networks\n",
            "The standard architecture for processingimages\n",
            "is aconvolutional network, or convnet, that com-\n",
            "bines multiple convolutional layers, either to re-\n",
            "duce the signal size before it can be processed by\n",
            "fully connected layers, or to output a 2D signal\n",
            "also of large size.\n",
            "LeNet-like\n",
            "The original LeNet model for image classifica-\n",
            "tion [LeCun et al., 1998] combines a series of 2D\n",
            "convolutional layers and max pooling layers that\n",
            "play the role of feature extractor, with a series of\n",
            "fully connected layers which act as a MLP and\n",
            "perform the classification per se (see Figure 5.2).\n",
            "This architecture was the blueprint for many\n",
            "models that share its structure and are simply\n",
            "larger, such as AlexNet [Krizhevsky et al., 2012]\n",
            "or the VGG family [Simonyan and Zisserman,\n",
            "2014].\n",
            "Residual networks\n",
            "Standard convolutional neural networks that fol-\n",
            "low the architecture of the LeNet family are not\n",
            "easily extended to deep architectures and suffer \n",
            "\n",
            "\n",
            "by a two-hidden-layer MLP to get the final C\n",
            "logits. Such a token, added for a readout of a\n",
            "class prediction, was introduced by Devlin et al.\n",
            "[2018] in the BERT model and is referred to as a\n",
            "CLS token.\n",
            "114 \n",
            "\n",
            "\n",
            "structured as graphs. Standard convolutional\n",
            "networks or even attention models are poorly\n",
            "adapted to process such data, and the tool of\n",
            "choice for such a task is Graph NeuralNetworks\n",
            "(GNN) [Scarselli et al., 2009].\n",
            "147 \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(len(result))\n",
        "\n",
        "print(f\"{result[0].page_content} \\n\\n\")\n",
        "print(f\"{result[1].page_content} \\n\\n\")\n",
        "print(f\"{result[2].page_content} \\n\\n\")\n",
        "# print(f\"{result[3].page_content} \\n\\n\")\n",
        "# print(f\"{result[4].page_content} \\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be670f4a648d67fa",
      "metadata": {
        "collapsed": false,
        "id": "be670f4a648d67fa",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "\n",
        "#### Maximum marginal relevance (MMR)\n",
        "\n",
        "When looking for the most similar results, it sometimes happens that you collect results which are redundant, you get some results that all mean the same thing.\n",
        "\n",
        " MMR can then help, the algorithm will, next to the relevance of the results, also account for the 'diversity'. It will make a new ranking based on both relevance and diversity.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04692982d638fe0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-20T15:35:50.639198Z",
          "start_time": "2023-11-20T15:35:50.455659Z"
        },
        "id": "a04692982d638fe0",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# from langchain.vectorstores import Chroma\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "persist_directory =  r\"/content/drive/MyDrive/Colab Data/chroma_langchain_db_openai_2\"\n",
        "chromadb = Chroma(\n",
        "    persist_directory=persist_directory,\n",
        "    embedding_function=get_embedding()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T821qCxLO328",
      "metadata": {
        "id": "T821qCxLO328"
      },
      "outputs": [],
      "source": [
        "new_result = vector_store.similarity_search(query=\"Name a deep learning application.\", k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fdf44cd11ea0618",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-17T13:12:42.969596Z",
          "start_time": "2023-11-17T13:12:42.965027Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fdf44cd11ea0618",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "646062e8-aea8-4d4a-fe90-c09759e4ad70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Chapter 6\n",
            "Prediction\n",
            "A first category of applications, such as face\n",
            "recognition, sentiment analysis, object detection,\n",
            "or speech recognition, requires predicting an un-\n",
            "known value from an available signal.\n",
            "116\n",
            "{'page': 115, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "2\n",
            "6.1 Image denoising\n",
            "A direct application of deep models to image\n",
            "processing is to recover from degradation by\n",
            "utilizing the redundancy in the statistical struc-\n",
            "ture of images. The petals of a sunflower in a\n",
            "grayscale picture can be colored with high confi-\n",
            "dence, and the texture of a geometric shape such\n",
            "as a table on a low-light, grainy picture can be\n",
            "corrected by averaging it over a large area likely\n",
            "to be uniform.\n",
            "Adenoisingautoencoder is a model that takes\n",
            "a degraded signal ˜Xas input and computes an\n",
            "estimate of the original signal X. For images, it\n",
            "is a convolutional network that may integrate\n",
            "skip-connections, in particular to combine repre-\n",
            "sentations at the same resolution obtained early\n",
            "and late in the model, as well as attention layers\n",
            "to facilitate taking into account elements that\n",
            "are far away from each other.\n",
            "Such a model is trained by collecting a large num-\n",
            "ber of clean samples paired with their degraded\n",
            "inputs. The latter can be captured in degraded\n",
            "{'page': 116, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "3\n",
            "Part II\n",
            "Deep models\n",
            "56\n",
            "{'page': 55, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "4\n",
            "Foreword\n",
            "The current period of progress in artificial in-\n",
            "telligence was triggered when Krizhevsky et al.\n",
            "[2012] showed that an artificialneuralnetwork\n",
            "with a simple structure, which had been known\n",
            "for more than twenty years [LeCun et al., 1989],\n",
            "could beat complex state-of-the-art image recog-\n",
            "nition methods by a huge margin, simply by\n",
            "being a hundred times larger and trained on a\n",
            "dataset similarly scaled up.\n",
            "This breakthrough was made possible thanks\n",
            "toGraph icalProcessingUnits ( GPUs), mass-\n",
            "market, highly parallel computing devices de-\n",
            "veloped for real-time image synthesis and repur-\n",
            "posed for artificial neural networks.\n",
            "Since then, under the umbrella term of “ deep\n",
            "learn ing, ” innovations in the structures of these\n",
            "networks, the strategies to train them, and ded-\n",
            "icated hardware have allowed for an exponen-\n",
            "tial increase in both their size and the quantity\n",
            "8\n",
            "{'page': 7, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "5\n",
            "Chapter 1\n",
            "Machine Learning\n",
            "Deep learn ing belongs historically to the larger\n",
            "field of statistical machine learn ing, as it funda-\n",
            "mentally concerns methods that are able to learn\n",
            "representations from data. The techniques in-\n",
            "volved come originally from artificialneuralnet-\n",
            "works, and the “deep” qualifier highlights that\n",
            "models are long compositions of mappings, now\n",
            "known to achieve greater performance.\n",
            "The modularity, versatility, and scalability of\n",
            "deep models have resulted in a plethora of spe-\n",
            "cific mathematical methods and software devel-\n",
            "opment tools, establishing deep learning as a\n",
            "distinct and vast technical field.\n",
            "11\n",
            "{'page': 10, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n"
          ]
        }
      ],
      "source": [
        "# result was het resultaat van een similarity_search\n",
        "i=1\n",
        "for r in new_result:\n",
        "  print(i)\n",
        "  print(r.page_content)\n",
        "  print(r.metadata)\n",
        "  i+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c8630bf9bf8ff46",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-17T12:40:28.969269Z",
          "start_time": "2023-11-17T12:40:28.751367Z"
        },
        "id": "1c8630bf9bf8ff46",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "mmr = vector_store.max_marginal_relevance_search(question, k=3, fetch_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qNLcG2V8pQ-S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNLcG2V8pQ-S",
        "outputId": "a64fe761-da35-4a92-99c0-486a8264892a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###################\n",
            "1\n",
            "5.2 Convolutional networks\n",
            "The standard architecture for processingimages\n",
            "is aconvolutional network, or convnet, that com-\n",
            "bines multiple convolutional layers, either to re-\n",
            "duce the signal size before it can be processed by\n",
            "fully connected layers, or to output a 2D signal\n",
            "also of large size.\n",
            "LeNet-like\n",
            "The original LeNet model for image classifica-\n",
            "tion [LeCun et al., 1998] combines a series of 2D\n",
            "convolutional layers and max pooling layers that\n",
            "play the role of feature extractor, with a series of\n",
            "fully connected layers which act as a MLP and\n",
            "perform the classification per se (see Figure 5.2).\n",
            "This architecture was the blueprint for many\n",
            "models that share its structure and are simply\n",
            "larger, such as AlexNet [Krizhevsky et al., 2012]\n",
            "or the VGG family [Simonyan and Zisserman,\n",
            "2014].\n",
            "Residual networks\n",
            "Standard convolutional neural networks that fol-\n",
            "low the architecture of the LeNet family are not\n",
            "easily extended to deep architectures and suffer\n",
            "{'page': 99, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "###################\n",
            "###################\n",
            "2\n",
            "by a two-hidden-layer MLP to get the final C\n",
            "logits. Such a token, added for a readout of a\n",
            "class prediction, was introduced by Devlin et al.\n",
            "[2018] in the BERT model and is referred to as a\n",
            "CLS token.\n",
            "114\n",
            "{'page': 113, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "###################\n",
            "###################\n",
            "3\n",
            "To be consistent with this interpretation, the\n",
            "model should be trained to maximize the proba-\n",
            "bility of the true classes, hence to minimize the\n",
            "cross -entropy, expressed as:\n",
            "ℒce(w) =−1\n",
            "NNX\n",
            "n=1logˆP(Y=yn|X=xn)\n",
            "=1\n",
            "NNX\n",
            "n=1−logexpf(xn;w)ynP\n",
            "zexpf(xn;w)z| {z }\n",
            "Lce(f(xn;w),yn).\n",
            "Contrastive loss\n",
            "In certain setups, even though the value to be\n",
            "predicted is continuous, the supervision takes\n",
            "the form of ranking constraints. The typical do-\n",
            "main where this is the case is metriclearn ing,\n",
            "where the objective is to learn a measure of dis-\n",
            "tance between samples such that a sample xa\n",
            "from a certain semantic class is closer to any\n",
            "sample xbof the same class than to any sample\n",
            "xcfrom another class. For instance, xaandxb\n",
            "can be two pictures of a certain person, and xca\n",
            "picture of someone else.\n",
            "The standard approach for such cases is to min-\n",
            "imize a contrastive loss, in that case, for in-\n",
            "stance, the sum over triplets (xa,xb,xc), such\n",
            "27\n",
            "{'page': 26, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "###################\n"
          ]
        }
      ],
      "source": [
        "i=1\n",
        "for r in mmr:\n",
        "  print(\"###################\")\n",
        "  print(i)\n",
        "  print(r.page_content)\n",
        "  print(r.metadata)\n",
        "  i+=1\n",
        "  print(\"###################\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "basP6yx1p5yR",
      "metadata": {
        "id": "basP6yx1p5yR"
      },
      "outputs": [],
      "source": [
        "mmr = vector_store.max_marginal_relevance_search(question, k=3, fetch_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E84vIZcep97J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E84vIZcep97J",
        "outputId": "0f083999-443a-47d4-df52-1b42b3e82172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "###################\n",
            "1\n",
            "Chapter 6\n",
            "Prediction\n",
            "A first category of applications, such as face\n",
            "recognition, sentiment analysis, object detection,\n",
            "or speech recognition, requires predicting an un-\n",
            "known value from an available signal.\n",
            "116\n",
            "{'page': 115, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "###################\n",
            "###################\n",
            "2\n",
            "6.1 Image denoising\n",
            "A direct application of deep models to image\n",
            "processing is to recover from degradation by\n",
            "utilizing the redundancy in the statistical struc-\n",
            "ture of images. The petals of a sunflower in a\n",
            "grayscale picture can be colored with high confi-\n",
            "dence, and the texture of a geometric shape such\n",
            "as a table on a low-light, grainy picture can be\n",
            "corrected by averaging it over a large area likely\n",
            "to be uniform.\n",
            "Adenoisingautoencoder is a model that takes\n",
            "a degraded signal ˜Xas input and computes an\n",
            "estimate of the original signal X. For images, it\n",
            "is a convolutional network that may integrate\n",
            "skip-connections, in particular to combine repre-\n",
            "sentations at the same resolution obtained early\n",
            "and late in the model, as well as attention layers\n",
            "to facilitate taking into account elements that\n",
            "are far away from each other.\n",
            "Such a model is trained by collecting a large num-\n",
            "ber of clean samples paired with their degraded\n",
            "inputs. The latter can be captured in degraded\n",
            "{'page': 116, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "###################\n",
            "###################\n",
            "3\n",
            "Foreword\n",
            "The current period of progress in artificial in-\n",
            "telligence was triggered when Krizhevsky et al.\n",
            "[2012] showed that an artificialneuralnetwork\n",
            "with a simple structure, which had been known\n",
            "for more than twenty years [LeCun et al., 1989],\n",
            "could beat complex state-of-the-art image recog-\n",
            "nition methods by a huge margin, simply by\n",
            "being a hundred times larger and trained on a\n",
            "dataset similarly scaled up.\n",
            "This breakthrough was made possible thanks\n",
            "toGraph icalProcessingUnits ( GPUs), mass-\n",
            "market, highly parallel computing devices de-\n",
            "veloped for real-time image synthesis and repur-\n",
            "posed for artificial neural networks.\n",
            "Since then, under the umbrella term of “ deep\n",
            "learn ing, ” innovations in the structures of these\n",
            "networks, the strategies to train them, and ded-\n",
            "icated hardware have allowed for an exponen-\n",
            "tial increase in both their size and the quantity\n",
            "8\n",
            "{'page': 7, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "###################\n"
          ]
        }
      ],
      "source": [
        "i=1\n",
        "for r in mmr:\n",
        "  print(\"###################\")\n",
        "  print(i)\n",
        "  print(r.page_content)\n",
        "  print(r.metadata)\n",
        "  i+=1\n",
        "  print(\"###################\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0226dd4e75234ba",
      "metadata": {
        "collapsed": false,
        "id": "f0226dd4e75234ba",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "\n",
        "Here's a concept, usefull of mentioning. I did not test it yet.\n",
        "\n",
        "SelfQuery, een ander algoritme dat je met langchain kan gebruiken is SelfQuery, het idee is dat je je vraag stelt in 'natuurlijke taal', en dat de LLM zichzelf gebruikt (vandaar de naam) om onderscheid te maken tussen delen van de vraag waarmee de metadata kan gefilterd worden, en de eigenlijk inhoud zelf.\n",
        "\n",
        "Als we kijken welke metadata er in het resultaat van onze similarity_search zit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37595c357758a6ca",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-17T13:12:47.167841Z",
          "start_time": "2023-11-17T13:12:47.163267Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37595c357758a6ca",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "2bd38dd9-daf1-48d0-fee2-1e49b5797c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'page': 115, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "{'page': 116, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "{'page': 55, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "{'page': 7, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "{'page': 10, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "\n",
            "\n",
            "{'page': 115, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "{'page': 116, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "{'page': 7, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n"
          ]
        }
      ],
      "source": [
        "for d in new_result:\n",
        "    print (d.metadata)\n",
        "print(\"\\n\")\n",
        "for d in mmr:\n",
        "    print(d.metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "475c7f249e7a4f81",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-20T15:40:17.058172Z",
          "start_time": "2023-11-20T15:40:17.033410Z"
        },
        "id": "475c7f249e7a4f81",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# Below: incomplete code hints w.r.t. using the selfquery.\n",
        "# from langchain.chains.query_constructor.schema import AttributeInfo\n",
        "# from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "\n",
        "\n",
        "# self_retriever = SelfQueryRetriever.from_llm(...)\n",
        "# docs = self_retriever.get_relevant_documents(\"When is polymorphism useful?\")\n",
        "# print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be4f0e0a742392fc",
      "metadata": {
        "collapsed": false,
        "id": "be4f0e0a742392fc",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "## Retrieval and Generation: Retrieve\n",
        "\n",
        "So far, we have learned\n",
        "- how to load data,\n",
        "- how to split documents,\n",
        "- how to store documents in a vectorstore\n",
        "- how to query for a document, relevant to our questions.\n",
        "\n",
        "The next step is having an LLM answer our question, using the content of the retrieved document(s).\n",
        "\n",
        "So far, we've queried the vector_store, using methods specific to the vector_store itself, like 'similarity_search' and 'mmr'.\n",
        "\n",
        "Langchain aims at being a generic tool, that makes it easy to switch components, like vectorstore. Therefore Langchain makes use of a Retriever interface.\n",
        "(ref. https://python.langchain.com/docs/tutorials/rag/, 4. Retrieval and Generation: Retrieve).\n",
        "\n",
        "The most common type of Retriever is the VectorStoreRetriever.\n",
        "- Turn our vectorstore into a Retriever object.\n",
        "- Set the parameters\n",
        "  - search_type to \"similarity\",\n",
        "  - search_kwargs takes k = 2 and fetch_k = 5.\n",
        "- Invoke the retriever with a question relevant to the content of the lbdl.pdf.\n",
        "- Check the number of results returned & the results itself.\n",
        "- What's the goal of a Self-Query or Self-Querying Retriever? How does it work?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qWZxqfMfWVf4",
      "metadata": {
        "id": "qWZxqfMfWVf4"
      },
      "source": [
        "A Self-Querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a421318d7dc4266",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-20T15:50:53.955407Z",
          "start_time": "2023-11-20T15:50:53.944863Z"
        },
        "id": "a421318d7dc4266",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 5}\n",
        ")\n",
        "# result_retriever = retriever.invoke(\"What is a CNN?\", filter={\"page\": 112})\n",
        "result_retriever = retriever.invoke(\"Name a deep learning application\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NhLbpObRSAWR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhLbpObRSAWR",
        "outputId": "04a5728e-c03e-4e9b-a182-e7f315b128e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "page_content='Chapter 5\n",
            "Architectures\n",
            "The field of deep learning has developed over\n",
            "the years for each application domain multiple\n",
            "deep architectures that exhibit good trade-offs\n",
            "with respect to multiple criteria of interest: e.g.\n",
            "ease of training, accuracy of prediction, memory\n",
            "footprint, computational cost, scalability.\n",
            "97' metadata={'page': 96, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n",
            "page_content='Index\n",
            "1D convolution, 65\n",
            "2D convolution, 65\n",
            "activation, 23, 41\n",
            "function, 70, 98\n",
            "map, 68\n",
            "Adam, 39\n",
            "affine operation, 60\n",
            "artificial neural network, 8, 11\n",
            "attention operator, 87\n",
            "autoencoder, 146\n",
            "denoising, 117\n",
            "Autograd, 42\n",
            "autoregressive model, seemodel, autoregressive\n",
            "average pooling, 75\n",
            "backpropagation, 42\n",
            "backward pass, 42\n",
            "basis function regression, 14\n",
            "batch, 21, 38\n",
            "batch normalization, 79, 103\n",
            "Bellman equation, 134\n",
            "159' metadata={'page': 158, 'source': '/content/drive/MyDrive/Colab Data/lbdl.pdf'}\n"
          ]
        }
      ],
      "source": [
        "print(len(result_retriever))\n",
        "print(result_retriever[0])\n",
        "\n",
        "print(result_retriever[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N7_iMdaOW0Ps",
      "metadata": {
        "id": "N7_iMdaOW0Ps"
      },
      "source": [
        "## Retrieval and Generation: Generate\n",
        "\n",
        "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gM_FBpwxZBMf",
      "metadata": {
        "id": "gM_FBpwxZBMf"
      },
      "source": [
        "After focussing on retrieving the relevant documents, the retrieved information can be fed into a prompt, together with our question.\n",
        "\n",
        "Here are two good practices for constructing such a prompt.\n",
        "\n",
        "- Use a predefined prompt from the langchain hub,\n",
        "- Create a customized prompt, using a PromptTemplate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5-XZIkEUZquh",
      "metadata": {
        "id": "5-XZIkEUZquh"
      },
      "source": [
        "### Predefined prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2oig19OKTIH",
      "metadata": {
        "id": "e2oig19OKTIH"
      },
      "outputs": [],
      "source": [
        "We’ll use a prompt for RAG that is checked into the LangChain prompt hub.\n",
        "(ref. https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=3aa0741f-ee94-47fd-a2d9-99a1e771f6fb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R_jEMy_5KbWn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_jEMy_5KbWn",
        "outputId": "93384764-eda4-402a-d8ec-63fe26f02ee9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
        ").to_messages()\n",
        "\n",
        "example_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k4j5LRIJKqwV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4j5LRIJKqwV",
        "outputId": "566fbc1c-949b-4cdc-e41a-40952164f843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: filler question \n",
            "Context: filler context \n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "print(example_messages[0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VKvNY3-MZvuB",
      "metadata": {
        "id": "VKvNY3-MZvuB"
      },
      "source": [
        "### PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19acf46da1ac7ad1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-20T15:58:53.564161Z",
          "start_time": "2023-11-20T15:58:53.542880Z"
        },
        "id": "19acf46da1ac7ad1",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks to HOGENT course!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gppTlF43dT3i",
      "metadata": {
        "id": "gppTlF43dT3i"
      },
      "source": [
        "- Chain everything together using the code.\n",
        "- Ask some questions about the lbdl.pdf, like:\n",
        "  - \"Give 3 examples of deep learning applicaitons\"\n",
        "  - \"What does CNN mean?\"\n",
        "  - \"Explain how CNN works?\"\n",
        "  - \"What is the role of an activation function?\"\n",
        "\n",
        "- What is the role of the StrOutputParser()?\n",
        "  The StrOutputParser(),  plucks the string content out of the LLM's output message."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the documentation (ref: https://python.langchain.com/docs/tutorials/rag/), it says:\n",
        "\n",
        "We’ll use the LCEL Runnable protocol to define the chain, allowing us to\n",
        "\n",
        "- pipe together components and functions in a transparent way\n",
        "- automatically trace our chain in LangSmith\n",
        "- get streaming, async, and batched calling out of the box.\n",
        "\n",
        "Further, there is a good explaination on how to read and understand the below code.\n",
        "\n",
        "In this code, each of the components: 'retriever', 'prompt', 'llm', ... are instances of Runnable."
      ],
      "metadata": {
        "id": "63hd7_3pg5-N"
      },
      "id": "63hd7_3pg5-N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dxiT2sAFMyTV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxiT2sAFMyTV",
        "outputId": "9987a70a-4a76-4d4a-9ac4-4567336a12b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "\n",
            "1. Image recognition: Deep learning models have achieved significant improvements in image recognition tasks by using large datasets and complex architectures. For example, the Inception architecture by Google has set new benchmarks in image classification tasks, achieving over 80% accuracy on the ImageNet dataset with millions of images.\n",
            "2. Speech recognition: Deep learning models have revolutionized the field of speech recognition, enabling highly accurate transcription of speech in a variety of languages. Companies like Google, Amazon, and Microsoft offer cloud-based speech recognition services powered by deep learning models.\n",
            "3. Natural language processing: Deep learning models have made significant progress in understanding and generating human language. Applications include machine translation, question answering, and text generation. For instance, the BERT (Bidirectional Encoder Representations from Transformers) model by Google has achieved state-of-the-art results in various NLP tasks, such as sentiment analysis and reading comprehension.</s>"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": vector_store.as_retriever() | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | get_llm(API.OPEN_AI)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "for chunk in rag_chain.stream(\"Give 3 examples of deep learning applicaitons\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: what part of the above code builds a formatted prompt, ready for inference?"
      ],
      "metadata": {
        "id": "xyIpYx-zl37h"
      },
      "id": "xyIpYx-zl37h"
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        ")"
      ],
      "metadata": {
        "id": "DnrhMtpVl_tl"
      },
      "id": "DnrhMtpVl_tl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "df0c1ceb0525d314",
      "metadata": {
        "collapsed": false,
        "id": "df0c1ceb0525d314",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "#### map reduce\n",
        "\n",
        "Als de documenten te uitgebreid zijn, zullen ze al snel groter zijn dan de beschikbare context voor LLM's. Een oplossing is om map reduce toe te passen, simpel gezegd zal je de documenten opsplitsen, de vraag naar elk sturen 'mappen', en dan de verschillende antwoorden 'reducen'.\n",
        "\n",
        "Dit leidt snel tot vrij veel API calls dus we gaan dit hier niet demonstreren. Er zijn voorbeelden en uitleg te vinden op langchain als je dit nodig hebt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55b2ec3adccfe60",
      "metadata": {
        "collapsed": false,
        "id": "f55b2ec3adccfe60",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "source": [
        "### Chat\n",
        "\n",
        "To be able to truly chat with the data, there's still a missing piece of the puzzle: we need to be able to incorporate the previously given answers into the next question. This way, we can get additional clarification, just as we are now accustomed to with chatbots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6821fe5e7d5f7823",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-20T16:09:05.641770Z",
          "start_time": "2023-11-20T16:09:05.624791Z"
        },
        "id": "6821fe5e7d5f7823",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "1f683344-7fd3-4824-c3e8-06bbd874e1b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_275/3025019359.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "# What we lack for the moment, is a chat memory.\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "qa_conversation = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=chromadb.as_retriever(),\n",
        "    memory=memory\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f02490e0875f5e97",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-20T16:10:14.446765Z",
          "start_time": "2023-11-20T16:10:12.138059Z"
        },
        "id": "f02490e0875f5e97",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "89e34c5f-0c38-414e-f9ff-22dd172e8611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'question': 'Give an example of where polymorphism can be used?', 'chat_history': [HumanMessage(content='Give an example of where polymorphism can be used?'), AIMessage(content=' Polymorphism can be used when creating a class hierarchy, where a parent class is inherited by multiple child classes. For example, a `BankAccount` class can be inherited by a `SavingsAccount` class and a `CheckingAccount` class.'), HumanMessage(content='Give an example of where polymorphism can be used?'), AIMessage(content=' Polymorphism can be used to create a class hierarchy by having a parent class and then creating child classes that inherit the properties of the parent class.')], 'answer': ' Polymorphism can be used to create a class hierarchy by having a parent class and then creating child classes that inherit the properties of the parent class.'}\n"
          ]
        }
      ],
      "source": [
        "# opgelet! de key is nu 'question' en niet 'query'\n",
        "result = qa_conversation({\"question\": \"Give an example of where polymorphism can be used?\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "435eca598799b5ce",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-20T16:11:16.151638Z",
          "start_time": "2023-11-20T16:11:13.294568Z"
        },
        "id": "435eca598799b5ce",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "7dbc2a54-f778-4092-8046-d304b13907e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'question': 'What would an implementation of a SavingsAccount look like?', 'chat_history': [HumanMessage(content='Give an example of where polymorphism can be used?'), AIMessage(content=' Polymorphism can be used when creating a class hierarchy, where a parent class is inherited by multiple child classes. For example, a `BankAccount` class can be inherited by a `SavingsAccount` class and a `CheckingAccount` class.'), HumanMessage(content='Give an example of where polymorphism can be used?'), AIMessage(content=' Polymorphism can be used to create a class hierarchy by having a parent class and then creating child classes that inherit the properties of the parent class.'), HumanMessage(content='What would an implementation of a SavingsAccount look like?'), AIMessage(content=' Checking the type of the instance is possible with the `is` keyword: \\n```\\nBankAccount s = new SavingsAccount(\"123-123123-13\", 0.1M)\\nif (s is SavingsAccount)\\n{\\n// Do something useful\\n}\\n```')], 'answer': ' Checking the type of the instance is possible with the `is` keyword: \\n```\\nBankAccount s = new SavingsAccount(\"123-123123-13\", 0.1M)\\nif (s is SavingsAccount)\\n{\\n// Do something useful\\n}\\n```'}\n"
          ]
        }
      ],
      "source": [
        "result = qa_conversation({\"question\": \"What would an implementation of a SavingsAccount look like?\"})\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}