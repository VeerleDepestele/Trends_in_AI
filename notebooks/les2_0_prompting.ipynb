{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3365e369",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "- Prompt Basics\n",
    "- Prompt Patterns\n",
    "- Prompt Caching\n",
    "\n",
    "ref. https://console.groq.com/docs/prompting\n",
    "\n",
    "Most of the time, the way to use an LLM is well documented. Often, prompt engineering is described within the documentation. \\\n",
    "\n",
    "https://platform.openai.com/docs/guides/prompt-engineering \\\n",
    "https://www.llama.com/docs/how-to-guides/prompting/ \\\n",
    "https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview \\\n",
    "https://console.groq.com/docs/prompting\n",
    "\n",
    "In this notebook, mainly some basic prompt engineering techniques are described. \\\n",
    "Please, also go through the les2_2 and les2_3 notebooks, where the theoretical concepts were further developed and examples are shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897684dd",
   "metadata": {},
   "source": [
    "## Prompt Basics\n",
    "Prompting is the methodology through which we communicate instructions, parameters, and expectations to large language models. Consider a prompt as a detailed specification document provided to the model: the more precise and comprehensive the specifications, the higher the quality of the output. This guide establishes the fundamental principles for crafting effective prompts for open-source instruction-tuned models, including Llama, Deepseek, and Gemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908079bc",
   "metadata": {},
   "source": [
    "### Why Prompts Matter\n",
    "Large language models require clear direction to produce optimal results. Without precise instructions, they may produce inconsistent outputs. Well-structured prompts provide several benefits:\n",
    "\n",
    "**Reduce development time** by minimizing iterations needed for acceptable results.\n",
    "**Enhance output consistency** to ensure responses meet validation requirements without modification.\n",
    "**Optimize resource usage** by maintaining efficient context window utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1fc4d7",
   "metadata": {},
   "source": [
    "### Prompt building blocks\n",
    "Most high-quality prompts contain five elements: \n",
    "- role, \n",
    "- input, \n",
    "- instructions, \n",
    "- expected output and examples, \n",
    "- constraints.\n",
    "\n",
    "| termElement                  | What it does                                                | \n",
    "|:-----------------------------|:-----------------    ---------------------------------------| \n",
    "| Role (+ context)             | Sets persona or expertise (\"You are a data analyst...\")     |                 \n",
    "| Input data (+ context)       | The data or question to transform                           | \n",
    "| Instructions                 | List of required actions                                    |\n",
    "| Expected output/examples     | Schema or miniature example to lock formatting              |\n",
    "| Constraints                  | Constraints                                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86e287",
   "metadata": {},
   "source": [
    "#### 1. Role \n",
    "The role sets persona or expertise:\n",
    "- \"You are a data analystâ€¦\",\n",
    "- \"You are an expert JavaScript developer\",\n",
    "\n",
    "Some contextcan be provided with the role:\n",
    "- \"You are a helpful assistant that recommends books based on user preferences\"    \n",
    "- \"You are a professors who evaluates bachelor's thesis proposals\", \n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6917a65",
   "metadata": {},
   "source": [
    "#### 2. Input data\n",
    "\n",
    "Input data is the data that describes what the prompt is about: \n",
    "- the text that needs to be summarized, \n",
    "- the bachelor's thesis that needs to be evaluated, \n",
    "- the piece of code that needs to be explained,\n",
    "- ...\n",
    "\n",
    "Some context - background knowledge or reference material for example - can be added. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f19f32",
   "metadata": {},
   "source": [
    "#### 3. Instructions\n",
    "\n",
    "The instructions describe what the LLM should do:\n",
    "- \"Give 5 recommandations for new books\",\n",
    "- \"Describe all spelling and grammatical errors, provide two suggestions for improvement to the student\",\n",
    "- \"Explain what this piece of code does\",\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b9f7e",
   "metadata": {},
   "source": [
    "#### 4. Expected output - examples\n",
    "Sometimes it is useful to provide examples of input and output, especially when a certain structure is expected (for example, JSON data with a specific format).\n",
    "\n",
    "In this context, we speak of 'zero-shot prompting' (when no example is provided) and 'few-shot prompting' (when examples are included in the prompt).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ecc17",
   "metadata": {},
   "source": [
    "#### 5. Constraints\n",
    "\n",
    "Some constraints can be added:\n",
    "- \"Only output a table\",\n",
    "- \"Generate max. 500 tokens\",\n",
    "- \"If you don't know the answer, do not make anything up, just say you don't know.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0d8fb",
   "metadata": {},
   "source": [
    "**Example 1**\n",
    "\n",
    "![prompt_engineering_example_nl](/notebooks/img/prompt_engineering_example_nl.png)\n",
    "\n",
    "**Example 2** \n",
    "\n",
    "You are an AI assistant that creates personalized book recommendations based on user preferences.\n",
    "\n",
    "User: Man, 46 years old, enjoys science fiction and non-fiction, and reads in Dutch and English.\n",
    "Favorite books: Predictably Irrational by Dan Ariely, Do Androids Dream of Electric Sheep by Philip K. Dick, and A Random Walk Down Wall Street by Burton Malkiel.\n",
    "\n",
    "Can you come up with five recommendations for this user?\n",
    "\n",
    "Example output: The Martian by Andy Weir, because you enjoy science fiction.\n",
    "\n",
    "Make sure the recommendations align with the genres the user likes and are similar to his favorite books. Do not recommend books from the example. Be friendly and make the user eager to start reading the books you suggest.\n",
    "\n",
    "**Exercise**\n",
    "In the example above, identify the prompt building blocks / prompt techniques that have been used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b955dfb",
   "metadata": {},
   "source": [
    "### Role channels\n",
    "\n",
    "Most chat-style APIs expose three channels:\n",
    "\n",
    "| Channel\t    | Typical Use                                                                           |\n",
    "|:----------|:--------------------------------------------------------------------------------------|\n",
    "| system\t    | High-level persona & non-negotiable rules (\"You are a helpful financial assistant.\"). |\n",
    "| user\t        | The actual request or data, such as a user's message in a chat.                       |\n",
    "| assistant\t    | The model's response. In multi-turn conversations, the assistant role can be used     |\n",
    "|               | to track the conversation history.                                                    | \n",
    "\n",
    "\n",
    "The following example demonstrates how to implement a customer service chatbot using role channels. Role channels provide a structured way for the model to maintain context and generate contextually appropriate responses throughout the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4aa12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next, I'll check the connection to the computer. Is the monitor cable (VGA, HDMI, or DisplayPort) securely connected to both the monitor and the computer?\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful IT support chatbot for 'Tech Solutions'.\n",
    "Your role is to assist employees with common IT issues, provide guidance on using company software, and help troubleshoot basic technical problems.\n",
    "Respond clearly and patiently. If an issue is complex, explain that you will create a support ticket for a human technician.\n",
    "Keep responses brief and ask a maximum of one question at a time.\n",
    "\"\"\"\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"My monitor isn't turning on.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Let's try to troubleshoot. Is the monitor properly plugged into a power source?\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Yes, it's plugged in.\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aacb3a",
   "metadata": {},
   "source": [
    "## Prompt patterns\n",
    "\n",
    "- in context learning: zero shot inference, ref. les2_2_prompt_engineering_openai.ipynb, \"3. examples\"\n",
    "- in context learning: one or few shot inference, , ref. les2_2_prompt_engineering_openai.ipynb, \"3. examples\"\n",
    "- chain of thought reasoning, ref. les2_3_prompt_engineering_llama.ipynb, \"Chain of thought prompting\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b2c6f",
   "metadata": {},
   "source": [
    "### Zero shot inference\n",
    "\n",
    "Zero shot provides instructions without examples, relying on the model's existing knowledge.\n",
    "\n",
    "The model leans on the general-purpose knowledge it absorbed during pre-training to infer the right output. You provide instructions but no examples, allowing the model to apply its existing understanding to the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d194d8",
   "metadata": {},
   "source": [
    "## Prompt caching\n",
    "\n",
    "(Sustainability related?)\n",
    "\n",
    "Prompt caching is a technique used for optimizing both performance and cost.\n",
    "\n",
    "The idea behind it, is: if you send a large prompt (for example, a long system message, context, or background document), you shouldnâ€™t have to pay for it repeatedly or retransmit it fully on every API call.\n",
    "\n",
    "How the concept of prompt caching is implemented in groq:\n",
    "\n",
    "Model prompts often contain repetitive content, such as system prompts and tool definitions. Prompt caching automatically reuses computation from recent requests when they share a common prefix, delivering significant cost savings and improved response times while maintaining data privacy through volatile-only storage that expires automatically.\n",
    "\n",
    "Prompt caching works automatically on all your API requests with no code changes required and no additional fees.\n",
    "\n",
    "How It Works\n",
    "Prefix Matching: When you send a request, the system examines and identifies matching prefixes from recently processed requests stored temporarily in volatile memory. Prefixes can include system prompts, tool definitions, few-shot examples, and more.\n",
    "\n",
    "Cache Hit: If a matching prefix is found, cached computation is reused, dramatically reducing latency and token costs by 50% for cached portions.\n",
    "\n",
    "Cache Miss: If no match exists, your prompt is processed normally, with the prefix temporarily cached for potential future matches.\n",
    "\n",
    "Automatic Expiration: All cached data automatically expires within a few hours, which helps ensure privacy while maintaining the benefits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trends_in_AI_development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
