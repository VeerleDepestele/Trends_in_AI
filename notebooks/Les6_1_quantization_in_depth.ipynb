{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d550d3b9",
   "metadata": {},
   "source": [
    "\n",
    "# Quantization in depth\n",
    "\n",
    "Quantization refers to the process of mapping a large set to a smaller set of values. There are many quantization techniques. In this lesson, we will dive deep into the theory of linear quantization. \n",
    "\n",
    "_ You will implement from scratch the asymmetric variant of linear quantization. \n",
    "_ You will also learn about the scaling factor and the zero point.\n",
    "\n",
    "## Quantize and dequantize a tensor\n",
    "\n",
    "Let’s have a look at an example. On your left you can see the original tensor in FP32. There is the quantized tensor on the right. The quantized tensor is quantized in `torch.int8`, and we use linear quantization to get this tensor. We will see in this lesson how we get this quantized tensor, but also how do we get back to the original tensor.\n",
    "\n",
    "![Les3_s1_quantization_example](img/Les3_s1_quantization_example.png)\n",
    "\n",
    "Let’s have a quick recap on what we can quantize in a neural network. In a neural network \n",
    "- you can quantize the weights, i.e. the neural network parameters,\n",
    "- you can quantize the activations. The activations are values that propagate through the layers of the neural network.\n",
    "\n",
    "If you quantize a neural network after it has been trained, you are doing something called **post-training quantization**.\n",
    "\n",
    "Advantages of quantization:\n",
    "- you get a smaller model, \n",
    "- speed gains from \n",
    "    - the memory bandwidth and \n",
    "    - faster operations, such as the matrix multiplication and the matrix-to-vector multiplication.\n",
    "\n",
    "We will see why it is the case in the next part when we talk about how to perform inference with a quantized model.\n",
    "\n",
    "There are many challenges to quantization. We will deep dive into these challenges in the last part of this lesson. Here's a preview of these challenges:\n",
    "\n",
    "- quantization error,\n",
    "- retraining (quantization aware training),\n",
    "- limited hardware support,\n",
    "- calibration dataset needed,\n",
    "- packing / unpacking.\n",
    "\n",
    "Linear quantization uses a linear mapping to map the higher precision range, for example floating point 32, to a lower precision range, for example int8.\n",
    "\n",
    "![Les3_s2_linear_quantization_0](img/Les3_s2_linear_quantization_0.png)\n",
    "\n",
    "There are two parameters in linear quantization:\n",
    "- the scale S\n",
    "- the zero point Z. \n",
    "\n",
    "The scale is stored in the same data type as the original tensor, and the zero point is stored in the same data type as the quantized tensor.\n",
    "Why this is the case will become clear in the next slides.\n",
    "\n",
    "Here is an example.\n",
    "\n",
    "Let’s say the scale is equal to two and the zero point is equal to zero.\n",
    "\n",
    "If we have a quantized value of ten (q=10), the dequantized value - r - would be equal to 2(q-0), which will be equal to 20.\n",
    "\n",
    "If we look at the example we presented in the first few slides, we would have something like this:\n",
    "\n",
    "![Les3_s2_linear_quantization_1](img/Les3_s3_linear_quantization_1.png)\n",
    "\n",
    "In the example, there is:\n",
    "- the original tensor (at the left),\n",
    "- the quantized tensor (at the right),\n",
    "- the zero point (z = -77),\n",
    "- the scale (s = 3.58).\n",
    "\n",
    "We will see how we get the zero point and the scale in the next few slides.\n",
    "\n",
    "![Les3_s4_quantization_example_1](img/Les3_s4_quantization_example_1.png)\n",
    "\n",
    "But first, we have the original tensor and we need to quantize this tensor.\n",
    "So, how do we get the quantized tensor? \n",
    "\n",
    "The relationship is r=s(q-z).\n",
    "\n",
    "Remark that the quantized tensor (q) is on the specific d-type, which is eight-bit integers in our example.\n",
    "Therefore, the number is rounded. The last step would be to cast this value to the correct d-type such as int8.\n",
    "\n",
    "![Les3_s5_isolate_q](img/Les3_s5_isolate_q.png)\n",
    "\n",
    "Now, let’s code the function that will give us the quantized tensor. \\\n",
    "ref. notebooks/Les3_quantization_in_depth/L2_linear_I_quantize_dequantize_tensor.ipynb, function \"linear_q_for_quantization_with_scale_and_zero_point\".\n",
    "\n",
    "Knowing the scale and the zero points, it will give us the quantized tensor.\n",
    "\n",
    "After quantizing the tensor, it is dequantized. When the quantization worked wel, the dequantized tensor is close to the original tensor, but they are not exactly the same.\n",
    "\n",
    "To get the quantization error tensor we subtract the original tensor and the dequantized tensor and we take the absolute value of the entire matrix.\n",
    "ref. notebooks/Les3_quantization_in_depth/L2_linear_I_quantize_dequantize_tensor.ipynb\n",
    "\n",
    "The quantization error can be measured by calculating the mean squared error between the original tensor and the dequantized tensor: \\\n",
    "(dequantized_tensor - test_tensor).square().mean()\n",
    "\n",
    "In the example, the quantization error is about 170. The error is quite high because in this example we assign a random value to scale and zero points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e27be3",
   "metadata": {},
   "source": [
    "## Get the scale and zero point\n",
    "\n",
    "How to determine the optimal s and z?\n",
    "\n",
    "To obtain the scale and the zero point, we need to look at the extreme values:\n",
    "-  r_min should map to q_min and \n",
    "-  r_max should map to q_max\n",
    "\n",
    "We get the following two equations:\n",
    "\n",
    "![Les3_s6_calculate_s_and_z](/img/Les3_s6_calculate_s_and_z.png)\n",
    "\n",
    "Since we have two unknowns s and z, we can solve this equation.\n",
    "\n",
    "If we subtract the first equation from the second one, we can get the scale.\n",
    "\n",
    "![Les3_s11_scale_derivation](img/Les3_s11_scale_derivation.png)\n",
    "\n",
    "For the zero point, since we've already determined s, we just need - for example - to use the first equation and replace s by the value we got before to get the zero point.\n",
    "\n",
    "We make z as the same d-type as the quantized tensor.\n",
    "In the example, z must be an integer. This is not the same d-type as the scale.\n",
    "\n",
    "The goal behind this choice is to represent zero in the original range as an integer in the quantized range.\n",
    "Thanks to that, when you quantize the value zero, it will take the value zero in the quantized range.\n",
    "And what is great is that if you’d dequantize the value z, it will become zero again.\n",
    "\n",
    "![Les3_s7_zero_point_derivation](img/Les3_s7_zero_point_derivation.png)\n",
    "\n",
    "![Les3_s8_why_make_z_an_integer](img/Les3_s8_why_make_z_an_integer.png)\n",
    "\n",
    "Explanation of how we calculate the scale and the zero point on the example:\n",
    "1. Get the range of the original tensor, i.e. we need to know r_min and r_max\n",
    "2. r_min = -184, r_max = 728.6\n",
    "3. Get the range of the quantized tensor. We quantize the tensor in torch.int8.\n",
    "4. Thus: q_min = -128 and q_max = 127.\n",
    "\n",
    "Knowing that \n",
    "- r_min = s (q_min - z) and\n",
    "- r_max = s (q_max - z),\n",
    "you get that the scale is equal to 3.58 and the zero point is equal to -77.\n",
    "\n",
    "We also need to round the value and to cast it to the correct d-type since we saw that z has the same d-type as the quantized value.\n",
    "\n",
    "![Les3_s9_calculate_s_and_z_example](img/Les3_s9_calculate_s_and_z_example.png)\n",
    "\n",
    "The last edge case, we need to threat is that we need to figure out is what happens when the zero point is out of range.\n",
    "\n",
    "For example, since we need to cast z to the quantized datatype, such as int8, what should we do when z is out of range?\n",
    "What if - when calculated - z_min is less than q_min? Or z_max is larger than q_max?\n",
    "\n",
    "- If z < q_min, we set z to be equal to q_min, \n",
    "- if z > q_max, we set z to be equal to q_max.\n",
    "\n",
    "In this way we don’t have overflow and underflow.\n",
    "\n",
    "![Les3_s10_zero_point_out_of_range](img/Les3_s10_zero_point_out_of_range.png)\n",
    "\n",
    "Now, consult the notebook:\n",
    "- notebooks/Les3_quantization_in_depth_2/L2_linear_I_get_scale_and_zero_point.ipynb\n",
    "\n",
    "In this notebook, a linear quantisation function is defined.\n",
    "\n",
    "The linear quantization function will only take a tensor and will return to you the quantized tensor, the scale and the zero point.\n",
    "\n",
    "In this function, two previously defined functions are used:\n",
    "- \"get_q_scales_and_zero_point\": pass the tensor and d-type, returns: scale and zero_point,\n",
    "- \"linear_q_scale_and_zero_point\": pass the tensor and the scale, the zero point, and the d-type. It returns the quantized tensor.\n",
    "\n",
    "The \"linear_quantization\" function returns the quantized tensor, the scale and the zero point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce048e",
   "metadata": {},
   "source": [
    "## Symmetric vs assymetric mode\n",
    "\n",
    "In this part you will learn about the symmetric mode of linear quantization.\n",
    "\n",
    "You will also implement quantization at different granularity, such as \n",
    "- per tensor, \n",
    "- per channel, and \n",
    "- per group \\ \n",
    "quantization.\n",
    "\n",
    "Finally, you will check how to perform inference on the quantized linear layer.\n",
    "\n",
    "There are two modes in linear quantization.\n",
    "\n",
    "- The first one is the **asymmetric mode**. \\\n",
    "    This is when you map the r_min, r_max to q_min and q_max and we just did that in the previous lesson.\n",
    "\n",
    "- The second one is the **symmetric mode**. \\\n",
    "    This is when we map negative r_max, r_max to negative q_max, q_max and r_max can be defined as the maximum of the absolute value of the tensor.\n",
    "\n",
    "![Les3_s12_linear_quantization_mode](img/Les3_s12_linear_quantization_mode.png)\n",
    "\n",
    "**In the symmetric mode, we do not need to store the zero point, since it is equal to zero.**\n",
    "This happens because the floating point range and the quantized range are symmetric with respect to zero.\n",
    "\n",
    "![Les3_s13_linear_quantization_mode_1](img/Les3_s13_linear_quantization_mode_1.png)\n",
    "\n",
    "Hence, we can simplify the equation, in the previous lesson, to get the following equation.\n",
    "\n",
    "- q = int(round(r/s))\n",
    "- s = r_max / q_max\n",
    "\n",
    "The quantized tensor (q) is simply the original tensor (r) divided by the scale (s) that we run and cast to the data type of the quantized tensor. \\ \n",
    "The scale S is simply r_max/q_max.\n",
    "\n",
    "For calculating the quantized tensor in the symmetric mode, we now only need to calculate the scale, since the zero point is known to be zero.\n",
    "\n",
    "Notebook:\n",
    "notebooks/quantization_in_depth_3/L3_linear_II_symmetric_vs_asymmetric.ipynb\n",
    "\n",
    "### Tradeoffs between symmetric and asymetric quantization\n",
    "\n",
    "The trade offs between these two linear quantization modes are:\n",
    "\n",
    "1. The utilization of the quantization range.\n",
    "  \n",
    "When you’re using the asymmetric quantization, the quantization range is fully used. \\\n",
    "When you’re using the symmetric mode, if the floating point range is biased toward one side, for example, you can think about the RELU layers where the output is positive. \\\n",
    "This will result in the quantization range, where a part of the range is dedicated to values that we will never see.\n",
    "\n",
    "2. The second tradeoff is the simplicity.\n",
    "\n",
    "Symmetric mode is much simpler compared to asymmetric mode.\n",
    "\n",
    "3. The thrid tradeoff is the memory.\n",
    "For symmetric quantization, we don’t need to store the zero points.\n",
    "\n",
    "In practice, we use symmetric quantization when we are trying to quantize to eight-bits, but when we quantized to low bits such as two, three, or four bits, we often use asymmetric quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec84df",
   "metadata": {},
   "source": [
    "## Finer granularity for more precision\n",
    "\n",
    "The more granular the quantization is, the more accurate it will be. \\\n",
    "However, note that it requires more memory since we need to store more quantization parameters.\n",
    "\n",
    "There are different levels of granularity when it comes to quantization.\n",
    "There is:\n",
    "\n",
    "1. per tensor quantization, \n",
    "2. per channel quantization,\n",
    "3. per group quantization.\n",
    "\n",
    "We don’t have to use the same scale and zero point for a whole tensor.\n",
    "**Per tensor quantization** is what we did so far. \\\n",
    "We can for instance calculate a scale and the zero point for each axis. This is called **per channel quantization**. \\\n",
    "We could also choose a group of n elements to get the scale and zero point, and quantize each group with its own scale and zero points, which is called **per group quantization**.\n",
    "\n",
    "![Les3_s14_granularity](img/Les3_s14_granularity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c221cf",
   "metadata": {},
   "source": [
    "### Per Tensor Quantization\n",
    "\n",
    "ref. notebooks/Les3_quantization_in_depth_4/L2_linear_II_per_channel.ipynb.\n",
    "\n",
    "In this notebook, the test tesnsor from the previous labs is used. \\\n",
    "Symmetric symmetric quantization is performed on this tensor.\n",
    "\n",
    "Afterwards, the dequantized tensor and the quantization error are calculated.\n",
    "\n",
    "The quantization error, with linear quantization on the test tensor is about 2.5.\n",
    "When we used asymmetric quantization, we had a quantization error of about 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298c15b",
   "metadata": {},
   "source": [
    "### Per Channel Quantization\n",
    "\n",
    "We need to store the scales and the zero point \n",
    "- for each row if we decide to quantize along the rows and \n",
    "- we need to store them along each column, if we decide to quantize along the columns.\n",
    "\n",
    "The memory needed to store all these linear parameters is pretty small.\n",
    "We usually use per channel quantization when quantizing models in eight-bit.\n",
    "\n",
    "![Les3_s15_per_channel_quantization](img/Les3_s15_per_channel_quantization.png)\n",
    "\n",
    "Let’s code the per channel quantization.\n",
    "To simplify the work we will restrict ourselves to the symmetric mode of the linear quantization.\n",
    "\n",
    "ref. notebooks/Les3_quantization_in_depth_5/L3_linear_II_per_channel.ipynb.\n",
    "\n",
    "\n",
    "For the test_tensor we've used, we get a lower quantization error in both cases (per row quantization, per column quantization) compared to tensor quantization.\n",
    "This is because outlier values will only impact one channel it was in, instead of the entire tensor.\n",
    "\n",
    "| qunatization type        | symmetric / assymetric    | quantization error on test tensor | \n",
    "|:-------------------------|:--------------------------|:----------------------------------| \n",
    "| per tensor               | asymmetric                | 1.5                               |                            \n",
    "| per tensor               | symetric                  | +- 2.5                            |\n",
    "| per channel - per row    | symmetric                 | 1.8                               |\n",
    "| per channel - per column | symmetric                 | 1.078                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcde1d8",
   "metadata": {},
   "source": [
    "### Per Group Quantization\n",
    "\n",
    "Let's go even smaller and do per group quantization. In per group quantization we perform quantization on groups of n elements.\n",
    "Common values for n are 32, 64, or 128. \n",
    "\n",
    "![per_group_quantization](img/Les3_s16_per_group_quantization.png)\n",
    "\n",
    "Per group quantization can require a lot of memory. Let's say, we want to quantize a tensor in four-bit, and we choose a group size equal to 32.\n",
    "We use symmetric mode. That means that the zero point is equal to zero. we store the scales in floating point 16.\n",
    "\n",
    "It means that we are actually quantizing the tensor in 4.5 bits:\n",
    "- each element is stored in 4 bit,\n",
    "- we store the scale in FP16, so we need 16 bit, for every group of 32 elements, therefore 16/32\n",
    "\n",
    "| qunatization type        | symmetric / assymetric    | quantization error on test tensor | \n",
    "|:-------------------------|:--------------------------|:----------------------------------| \n",
    "| per tensor               | asymmetric                | 1.5                               |                            \n",
    "| per tensor               | symetric                  | +- 2.5                            |\n",
    "| per channel - per row    | symmetric                 | 1.8                               |\n",
    "| per channel - per column | symmetric                 | 1.078                             |\n",
    "| per group -   of 3       | symmetric                 | 2.15                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59c109",
   "metadata": {},
   "source": [
    "## Quantizing Weights & Activations for Inference\n",
    "\n",
    "How to perform inference with linear quantization?\n",
    "\n",
    "In a neural network, you can quantize the weights, but you can also quantize the activation.\n",
    "Depending on what we quantized, the storage and the computation are not the same.\n",
    "\n",
    "- If you only quantize the weights, the computation will be using floating point arithmetic, i.e. floating point 32, floating point 16 or bfloat16.\n",
    "- If you also quantize the activation you will be using integer based arithmetics.\n",
    "\n",
    "For the first case where you only quantize the weights (for example to int8).\n",
    "Note that you need to dequantize the weights to perform the floating point computation.\n",
    "\n",
    "If you quantize also the activations, you will be using integer based arithmetics. Remark that this is not supported by all hardware.\n",
    "\n",
    "![Les3_s17_quantizing_weights_and_activations_for_inference](img/Les3_s17_quantizing_weights_and_activations_for_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38483fb",
   "metadata": {},
   "source": [
    "## Custom build an 8 bit-quantizer\n",
    "\n",
    "In this part, you will leverage the tools that you have just built in order to create your own quantizer to quantize any model in eight-bit precision.\n",
    "This quantizer is modality agnostic, meaning you can apply it on vision, audio texts, and even multimodal models.\n",
    "\n",
    "we will learn about how to make our own quantizer to quantize any model in eight-bit precision using the per channel linear\n",
    "quantization scheme.\n",
    "\n",
    "For that, we'll break down the project into multiple sub steps.\n",
    "\n",
    "1. We will deep dive into creating a W8A16 linear layer class, where \"W8\" stands for eight-bit weights and \"A16\" stands for 16 bits activations.\n",
    "2. We will use this class to store eight-bit weights and scales,\n",
    "3. We will see how we can replace all instances of torch.nn.layers with that new class.\n",
    "4. We will build a quantizer to quantize our model end to end.\n",
    "5. We will test our quantizer on many scenarios.\n",
    "6. We will study the impact of the eight bit quantization on different models.\n",
    "\n",
    "ref. Les3_quantization_in_depth_8/L4_building_quantizer_custom_quantizer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca12a49",
   "metadata": {},
   "source": [
    "## Replace Pytorch layers with Quantized layers\n",
    "\n",
    "ref. Les3_quantization_in_depth_9/L4_building_quantizer_replace_layers.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fceac4",
   "metadata": {},
   "source": [
    "## Quantize any Open Source PyTorch Model\n",
    "\n",
    "Let's test our implementation on models that you can find on Hugging Face Transformers.\n",
    "\n",
    "ref. Les3_quantization_in_depth_10/L4_building_quantizer_quantize_models.ipynb"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
